import os
import urllib2

from BeautifulSoup import BeautifulSoup
from dateutil import parser
import mistune

from data.acoustics import speech_recognition
from scales import error_rate # or error_percent?
from taxonomy import offline

# needs to be manually updated when more metrics are added
h2_name_map = {
        "LibriSpeech" : "librispeech",
        "WSJ" : "wsj",
        "Switchboard Hub5'00" : "swb_hub_500",
        "Fisher" : "fisher",
        "CHiME (noisy speech)" : "chime",
        "TIMIT" : "timit"
        }

def add_metric(h2_name, file_output, scale = "error_percent"):
    name = h2_name_map[h2_name]
    if name == "":
        print "Error! Need to add h2_name to h2_name_map to be able to parse!"
        return file_output
    if name == "swb_hub_500":
        file_output += "\n"
        return file_output
    s = "\n{0} = speech_recognition.metric(name=\"{1}\", scale={2}, attributes=['language'])\n".format(name, h2_name, scale)
    file_output += s
    return file_output

def add_measure(metric_name, data, file_output):
    notes = data['name'].encode('ascii', 'ignore')
    if notes == "Humans":
        return file_output
    value = data['value'].strip('%')
    if value == "":
        return file_output
    s = "{0}.measure({1}, {2}, '{3}', '{4}')\n".format(h2_name_map[metric_name], data['date'], value, notes, data['url'])
    file_output += s
    return file_output

def row_data(row):
    columns = row.findAll('td')
    l = len(columns)
    date = columns[l-2].getText()
    date = str(parser.parse(date).date()).split("-")
    date = "date({0}, {1}, {2})".format(int(date[0]), int(date[1]), int(date[2]))
    r = {
            'name': columns[l-1].getText(),
            'date': date,
            'url': columns[l-3].find('a', href=True)['href'], 
            'value': columns[0].getText() # for now, always just take the first column
        }
    return r

def main():
    md = urllib2.urlopen('https://raw.githubusercontent.com/syhw/wer_are_we/master/README.md')
    md = md.read()
    html = mistune.markdown(md)
    bs = BeautifulSoup(html)
    wer_data_file = os.path.abspath(os.path.join(os.path.dirname(__file__),  "../data/wer.py"))
    file_output = "# The file was autogenerated by ../scrapers/wer.py\n\nfrom datetime import date\n\nfrom data.acoustics import speech_recognition, swb_hub_500\nfrom scales import *\n\n"
    for table, header in zip(bs.findAll('table'), bs.findAll('h3')):
        header = header.getText()
        file_output = add_metric(header, file_output)

        rows = table.findAll('tr')
        for row in rows:
            if row.findAll('td') == []:
                continue
            data = row_data(row)
            file_output = add_measure(header, data, file_output)
        
    with open(wer_data_file, 'wb') as f:
        f.write(file_output)

if not offline:
    main()
