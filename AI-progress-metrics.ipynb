{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring the progress of AI research\n",
    "\n",
    "This notebook collects problems and metrics / datasets from the AI research literature, and tracks progress on them. You can use it to see how things are progressing in specific subfields or AI/ML as a whole, as a place to report new results you've obtained, and as place to look for problems that might new datasets/metrics designed for them.\n",
    "\n",
    "### Taxonomy\n",
    "\n",
    "It collates data with the following structure:\n",
    "\n",
    "```\n",
    "problem \n",
    "    \\   \\\n",
    "     \\   metrics  -  measures \n",
    "      \\\n",
    "       - subproblems\n",
    "            \\\n",
    "          metrics\n",
    "             \\\n",
    "            measure[ment]s\n",
    "```\n",
    "\n",
    "Problems describe the ability to learn an important category of task.\n",
    "\n",
    "Metrics should ideally be formulated in the form \"software is able to learn to do X given training data of type Y\". In some cases X is the interesting part, but sometimes also Y.\n",
    "\n",
    "Measurements are the score that a specific instance of a specific algorithm was able to get on a Metric.\n",
    "\n",
    "problems are tagged with attributes:\n",
    "eg, vision, abstract-games, language, world-modelling, safety\n",
    "\n",
    "Some of these are about performance relative to humans (which is of course a very arbitrary standard, but one we're familiar with)\n",
    "* agi -- most capable humans can do this, so AGIs can do this (note it's conceivable that an agent might pass the Turing test before all of these are won)\n",
    "* super -- the very best humans can do this, or human organisations can do this\n",
    "* verysuper -- neither humans nor human orgs can presently do this\n",
    "\n",
    "problems can have \"subproblems\", including simpler cases and preconditions for solving the problem in general\n",
    "\n",
    "a \"metric\" is one way of measuring progress on a problem, commonly associated with a test dataset. There will often be several metrics\n",
    "for a given problem, but in some cases we'll start out with zero metrics and will need to start proposing some...\n",
    "\n",
    "a measure[ment] is a score on a given metric, by a particular codebase/team/project, at a particular time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML ; HTML('''<script>code_show=false; function code_toggle() {if (code_show){$('div.input').hide();} else { $('div.input').show();} code_show = !code_show} $( document ).ready(code_toggle); </script> <form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to hide/show source code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2166,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Housekeeping code\n",
    "%matplotlib inline  \n",
    "import matplotlib as mpl\n",
    "#mpl.use(\"nbAgg\")\n",
    "\n",
    "from collections import defaultdict\n",
    "from lxml.cssselect import CSSSelector\n",
    "from math import log\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import datetime\n",
    "import lxml.html\n",
    "import json\n",
    "import numpy\n",
    "import re\n",
    "import requests\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except ImportError:\n",
    "    print \"Seaborn style not installed\"\n",
    "\n",
    "date = datetime.date\n",
    "problems = {}\n",
    "metrics = {}\n",
    "measurements = set() # we don't try to guarantee unique names for these, so use a set\n",
    "all_attributes = set()\n",
    "\n",
    "class Problem:\n",
    "    def __init__(self, name, attributes=[], solved=False, url=None):\n",
    "        self.name = name\n",
    "        self.attributes = attributes\n",
    "        for a in attributes:\n",
    "            global all_attributes\n",
    "            all_attributes.add(a)\n",
    "        self.subproblems = []\n",
    "        self.superproblems = []\n",
    "        self.metrics = []\n",
    "        self.solved = solved\n",
    "        self.url = url\n",
    "        global problems, metrics\n",
    "        problems[name] = self\n",
    "        \n",
    "    def add_subproblem(self, other_problem):\n",
    "        # add this other problem as a subproblem of us\n",
    "        other_problem.superproblems.append(self)\n",
    "        self.subproblems.append(other_problem)\n",
    "        \n",
    "    def metric(self, *args, **kwargs):\n",
    "        m = Metric(*args, **kwargs)\n",
    "        m.parent = self\n",
    "        self.metrics.append(m)\n",
    "        return m\n",
    "    \n",
    "    def check_solved(self):\n",
    "        if all(m.solved for m in self.metrics + self.subproblems):\n",
    "            self.solved = True\n",
    "            for p in self.superproblems:\n",
    "                p.check_solved()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Problem({0})\".format(self.name)\n",
    "    \n",
    "    def print_structure(self, indent=0):\n",
    "        print indent * \" \" + str(self)\n",
    "        for m in self.metrics:\n",
    "            print (indent + 4) * \" \" + str(m)\n",
    "        for p in self.subproblems:\n",
    "            p.print_structure(indent + 4)\n",
    "\n",
    "            \n",
    "# Different metrics and measurements for progress are made on very different types of scales\n",
    "# we have some helper functions to regularise these a little bit, so we can tell (for instance)\n",
    "# whether progress on some metric appears to be accelerating or decelerating.\n",
    "\n",
    "# Interface:\n",
    "#    improvement(score1, score2): retrns a consistent measure of how much better score2 is than score1\n",
    "#    pseudolinear(score): returns a modified version of score where we would expect vaguely linear progress\n",
    "\n",
    "class Linear:\n",
    "    offset = (2,-2)\n",
    "    axis_label = \"Score\"\n",
    "    def improvement(self, score1, score2):\n",
    "        return score2 - score1\n",
    "    def pseudolinear(self, score):\n",
    "        return score\n",
    "linear = Linear()\n",
    "score = Linear()\n",
    "\n",
    "class ELO:\n",
    "    offset = (2,-2)\n",
    "    axis_label = \"ELO rating\"\n",
    "    def improvement(self, score1, score2):\n",
    "        \"\"\"\n",
    "        Normalise an ELO score\n",
    "        \n",
    "        An ELO increase of 400 improves your odds by 10x, so we could justify something like\n",
    "        return 10.0 ** ((score2 - score1)/400.)\n",
    "        However, it seems that at least for chess ELO progress has been roughly linear over\n",
    "        time, both for humans and computers (though with different coefficients). Perhaps this\n",
    "        tracks exponential increases in ability to search the game's state space, driven directly\n",
    "        by Moore's law on the computer side, and indirectly for humans by access to better training\n",
    "        tools and more profound libraries of past play.\n",
    "        \n",
    "        So for now let's treat this as linear? But ELO is not a chess-specific measure, and in other\n",
    "        contexts we may want to do exponentiation as documented above?\n",
    "        \"\"\"\n",
    "        return score2 - score1\n",
    "    def pseudolinear(self, score):\n",
    "        return score\n",
    "    \n",
    "elo = ELO()\n",
    "\n",
    "class ErrorRate:\n",
    "    \"\"\"Many labelling contests use these measures\"\"\"\n",
    "    offset = (2,2)\n",
    "    axis_label = \"Error rate\"\n",
    "    def improvement(self, score1, score2):\n",
    "        # improvement is measured as a negative log of the error rate\n",
    "        return log(score1) - log(score2)\n",
    "    def pseudolinear(self, score):\n",
    "        # error rate 1 => 0\n",
    "        # error rate 0 => infinity\n",
    "        return -log(score)\n",
    "error_rate = ErrorRate()\n",
    "\n",
    "# some problems have performance measured in bits per X (bits per character, bits per pixel, etc), \n",
    "# reflecting the amount of information necessary for a model to accurately encode something from a corpus.\n",
    "# Lower is better and zero is infinitely good, so we can re-use the error rate math for now (though\n",
    "# scores above 1 are possible)\n",
    "bits_per_x = ErrorRate()\n",
    "bits_per_x.axis_label = \"Model Entropy\"\n",
    "# perplexity is 2 to the bits_per_x\n",
    "perplexity = ErrorRate()\n",
    "perplexity.axis_label = \"Perplexity\"\n",
    "\n",
    "class CorrectPercent:\n",
    "    \"100 - error rate\"\n",
    "    offset = (3,-6)\n",
    "    axis_label = \"Percentage correct\"\n",
    "    def erate(self, score):\n",
    "        return (100. - score)/100.\n",
    "\n",
    "    def improvement(self, score1, score2):\n",
    "        return score2 - score1\n",
    "    \n",
    "    def pseudolinear(self, score):\n",
    "        from math import log\n",
    "        return log(self.erate(score))\n",
    "\n",
    "correct_percent = CorrectPercent()\n",
    "\n",
    "class ErrorPercent:\n",
    "    \"100 * error rate\"\n",
    "    offset = (3,-6)\n",
    "    axis_label = \"Percentage error\"\n",
    "    def erate(self, score):\n",
    "        return score/100.\n",
    "\n",
    "    def improvement(self, score1, score2):\n",
    "        return score1 - score2\n",
    "    \n",
    "    def pseudolinear(self, score):\n",
    "        from math import log\n",
    "        return log(self.erate(score))\n",
    "    \n",
    "error_percent = ErrorPercent()\n",
    "\n",
    "mpl.rcParams[\"legend.fontsize\"] = u\"x-small\"\n",
    "mpl.rcParams[\"xtick.labelsize\"] = u\"xx-small\"\n",
    "mpl.rcParams[\"ytick.labelsize\"] = u\"x-small\"\n",
    "\n",
    "class Metric:\n",
    "    def __init__(self, name, url=None, solved=False, notes=\"\", scale=linear, target=None, target_source=None,\n",
    "                 parent=None, changeable=False, axis_label=None, target_label=None):\n",
    "        self.name = name\n",
    "        self.measures = []\n",
    "        self.solved = solved\n",
    "        self.url = url\n",
    "        self.notes = notes\n",
    "        self.scale = scale\n",
    "        self.target = target\n",
    "        self.target_source = target_source # Source for human-level performance number\n",
    "        self.changeable = changeable # True if a metric changes over time\n",
    "        global metrics\n",
    "        metrics[name] = self\n",
    "        self.parent = parent\n",
    "        self.target_label = target_label\n",
    "        self.axis_label = (     axis_label            if axis_label \n",
    "                           else self.scale.axis_label if hasattr(self.scale, \"axis_label\") \n",
    "                           else self.name)\n",
    "\n",
    "        \n",
    "    def __str__(self):\n",
    "        solved = \"SOLVED\" if self.solved else \"\"\n",
    "        return \"Metric({0})      {1}\".format(self.name, solved)\n",
    "        \n",
    "    def measure(self, *args, **kwargs):\n",
    "        m = Measurement(*args, **kwargs)\n",
    "        m.metric = self\n",
    "        if self.target:\n",
    "            if self.target_source == m.url and self.target == m.value:\n",
    "                print \"Skipping aparent source paper\", m.url\n",
    "                return None\n",
    "            if self.scale.improvement(self.target, m.value) >= 0:\n",
    "                self.solved = True\n",
    "                self.parent.check_solved()\n",
    "        self.measures.append(m)\n",
    "        return m\n",
    "    \n",
    "    def graph(self):\n",
    "        fig = plt.figure(dpi=300)\n",
    "        fig.set_size_inches(7, 5)\n",
    "        #fig.add_subplot(111).set_ylabel(self.name)\n",
    "        subplot = fig.add_subplot(111)\n",
    "        subplot.set_ylabel(self.axis_label)\n",
    "        subplot.set_title(self.name)\n",
    "        #fig.add_subplot(111).set_ylabel(self.name)\n",
    "        \n",
    "        self.measures.sort(key=lambda m: (m.date, m.metric.scale.pseudolinear(m.value)))\n",
    "        \n",
    "        # scatter plot of results in the literature\n",
    "        xs = [m.date for m in self.measures]\n",
    "        ys = [m.value for m in self.measures]\n",
    "        \n",
    "        x1s = [m.date  for m in self.measures if      self.target and self.scale.improvement(self.target, m.value) >= 0]\n",
    "        y1s = [m.value for m in self.measures if      self.target and self.scale.improvement(self.target, m.value) >= 0]\n",
    "        x2s = [m.date  for m in self.measures if not (self.target and self.scale.improvement(self.target, m.value) >= 0)]\n",
    "        y2s = [m.value for m in self.measures if not (self.target and self.scale.improvement(self.target, m.value) >= 0)]\n",
    "        \n",
    "        cs = [(0,0,0) if not self.target \n",
    "              else \"r\" if self.scale.improvement(self.target, m.value) >= 0 \n",
    "              else \"b\" ]\n",
    "        \n",
    "        plt.plot_date(x1s, y1s, c=\"b\")\n",
    "        plt.plot_date(x2s, y2s, c=\"r\")\n",
    "        for n, m in enumerate(self.measures):\n",
    "            label = m.name if len(m.name) < 25 else m.name[:25] + \"...\"\n",
    "            plt.annotate('%s' % label, xy=(xs[n],ys[n]), xytext=m.metric.scale.offset, fontsize=6, textcoords='offset points')\n",
    "            # cases where either results or dates of publication are uncertain\n",
    "            kwargs = {\"c\": \"#80cf80\", \"linewidth\": 1.0, \"capsize\": 1.5, \"capthick\": 0.5, \"dash_capstyle\": 'projecting'}\n",
    "            if m.min_date or m.max_date:\n",
    "                before = (xs[n] - m.min_date) if m.min_date else datetime.timedelta(0)\n",
    "                after = (m.max_date - xs[n]) if m.max_date else datetime.timedelta(0)\n",
    "                kwargs[\"xerr\"] = numpy.array([[before], [after]])\n",
    "            if self.measures[n].value != self.measures[n].minval:\n",
    "                kwargs[\"yerr\"] = numpy.array([[ys[n] - self.measures[n].minval], [self.measures[n].maxval - ys[n]]])\n",
    "            if \"xerr\" in kwargs or \"yerr\" in kwargs:\n",
    "                subplot.errorbar(xs[n], ys[n], **kwargs)\n",
    "        \n",
    "        # line graph of the frontier of best results\n",
    "        if not self.changeable:\n",
    "            best = self.measures[0].value\n",
    "            frontier_x, frontier_y = [], []\n",
    "            for m in self.measures:\n",
    "                if self.scale.improvement(best, m.value) >= 0:\n",
    "                    frontier_x.append(m.date)\n",
    "                    frontier_y.append(m.value)\n",
    "                    xy = (m.date, m.value)       \n",
    "                    best = m.value\n",
    "            plt.plot_date(frontier_x, frontier_y, \"g-\")\n",
    "        \n",
    "        # dashed line for \"solved\" / strong human performance\n",
    "        if self.target:\n",
    "            target_label = (       self.target_label  if self.target_label\n",
    "                             else \"Human performance\" if self.parent and \"agi\" in self.parent.attributes\n",
    "                             else \"Target\")\n",
    "            plt.plot_date([self.measures[0].date, self.measures[-1].date], 2 * [self.target], \"r--\", label=target_label)\n",
    "            \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2200,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def canonicalise(url):\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    if url.startswith(\"http://arxiv.org\"):\n",
    "        url = url.replace(\"http\", \"https\")\n",
    "    if url.startswith(\"https://arxiv.org/pdf/\"):\n",
    "        url = url.replace(\"pdf\", \"abs\", 1)\n",
    "        url = url.replace(\".pdf\", \"\", 1)\n",
    "    return url\n",
    "# dates of conferences help us date papers from the \"Are We There Yet\" dataset\n",
    "conference_dates = {\"ICML 2016\": date(2016, 6, 19),\n",
    "                    \"NIPS 2015\": date(2015, 12, 7),\n",
    "                    \"ICLR 2014\": date(2014, 4, 14),\n",
    "                    \"ICML 2013\": date(2013, 6, 16)}\n",
    "conferences_wanted = defaultdict(lambda: 0)\n",
    "\n",
    "class Measurement:\n",
    "    def __init__(self, d, value, name, url, algorithms=[], uncertainty=0, minval=None, maxval=None, opensource=False, replicated=\"\",\n",
    "                 papername=None, venue=None, min_date=None, max_date=None, algorithm_src_url=None):\n",
    "        self.date = d\n",
    "        self.value = value\n",
    "        assert isinstance(value, float) or isinstance(value, int), \"Measurements on metrics need to be numbers\"\n",
    "        self.name = name\n",
    "        \n",
    "        # For papers on arxiv, always use the abstract link rather than the PDF link\n",
    "        self.url = canonicalise(url)\n",
    "        assert self.url or papername, \"Measurements must have a URL or a paper name\"\n",
    "        self.min_date = min_date\n",
    "        self.max_date = max_date\n",
    "        self.aglorithm_src_url = algorithm_src_url\n",
    "        self.minval = minval if minval else value - uncertainty\n",
    "        self.maxval = maxval if maxval else value + uncertainty\n",
    "        self.opensource = opensource\n",
    "        self.replicated_url = replicated\n",
    "        self.algorithms = []\n",
    "        arxiv_papername, arxiv_dates = ade.get_paper_data(self.url)\n",
    "        if \"arxiv.org\" in self.url:\n",
    "            assert arxiv_dates, \"Failed to extract arxiv dates for \"+ self.url\n",
    "        self.papername = papername if papername else arxiv_papername\n",
    "        self.determine_paper_dates(d, arxiv_dates, venue)\n",
    "            \n",
    "        global measurements\n",
    "        measurements.add(self)\n",
    "    \n",
    "\n",
    "    \n",
    "    year_re=re.compile(r\"([0-9][0-9][0-9][0-9])\")\n",
    "    def determine_paper_dates(self, d, arxiv_dates, venue):\n",
    "        \"\"\"\n",
    "        Try to figure out when a result was obtained, and our uncertainty on that.\n",
    "        \n",
    "        :param datetime.date d: date supplied at paper entry time. We may not be able to trust this if the paper had multiple versions\n",
    "                                   and the person doing the entry didn't specify which version they got their result numbers from :/\n",
    "        :param dict arxiv_dates:   either None or a dict like {\"1\": date(2017,1,13), \"2\": date(2017, 3, 4)...}\n",
    "        :param venue:              for Rodriguo Benenson's data, a publication venue like \"ICML 2016\" or \"arXiv 2014\"\n",
    "        \"\"\"\n",
    "        # begin by trusting whoever entered the data\n",
    "        self.date = d\n",
    "\n",
    "        # but warn if it doesn't match arXiv dates\n",
    "        adates = sorted(arxiv_dates.values()) if arxiv_dates else []\n",
    "\n",
    "        if arxiv_dates and d:\n",
    "            if d < min(adates) and d > max(adates):\n",
    "                print \"WARNING, date\", self.date, \"for\", self.url, \"does not match any of the arXiv versions (%s)\" % \" \".join(str(s) for s in arxiv_dates.values())\n",
    "        if arxiv_dates:\n",
    "            if len(arxiv_dates) == 1:\n",
    "                if not self.date:\n",
    "                    self.date = adates[0]\n",
    "            else:\n",
    "                # multiple arxiv dates means the url wasn't versioned, and we might not have gotten the date exactly right\n",
    "                self.min_date = self.min_date if self.min_date else min(adates)\n",
    "                self.max_date = self.max_date if self.max_date else max(adates)\n",
    "                if not self.date:\n",
    "                    midrange = datetime.timedelta(days=0.5 * (self.max_date - self.min_date).days)\n",
    "                    self.date = self.min_date + midrange\n",
    "        elif venue and not self.date:\n",
    "            # if all we have is a conference / journal, we might be able to still figure something out..\n",
    "            if venue.upper() in conference_dates:\n",
    "                self.date = conference_dates[venue]\n",
    "            else:\n",
    "                conferences_wanted[venue] += 1\n",
    "                year = int(self.year_re.search(venue).groups(0)[0])\n",
    "                self.date = date(year, 7, 1)\n",
    "                self.min_date = date(year, 1, 1)\n",
    "                self.max_date = date(year, 12, 31)\n",
    "        if not self.date:\n",
    "            print d, arxiv_dates, venue\n",
    "        assert self.date, \"Need a date for paper {0} {1}\".format(self.url, self.papername)\n",
    "\n",
    "print canonicalise('http://arxiv.org/pdf/1412.6806.pdf')\n",
    "#cifar10.measure(None, 96.53, 'Fractional Max-Pooling', 'http://arxiv.org/abs/1412.6071', papername='Fractional Max-Pooling', venue='arXiv 2015')\n",
    "cifar10.measure(None, 95.59, 'Striving for Simplicity: The All Convolutional Net', 'http://arxiv.org/pdf/1412.6806.pdf', papername='Striving for Simplicity: The All Convolutional Net', venue='ICLR 2015')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2168,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# simple hooks for letting us save & restore datetime.date objects in a JSON cache\n",
    "class DateEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, datetime.date):\n",
    "            return str(obj)\n",
    "        return super(MyEncoder, self).default(obj)\n",
    "\n",
    "def parse_date(json_dict):\n",
    "    if \"dates\" in json_dict:\n",
    "        for v, date_str in json_dict[\"dates\"].items():\n",
    "            json_dict[\"dates\"][v] = datetime.datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
    "    return json_dict\n",
    "\n",
    "\n",
    "class ArxivDataExtractor:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            with open(\".paper_cache.json\") as f:\n",
    "                self.cache = json.load(f, object_hook=parse_date)\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "            print \"(Continuing with an empty cache)\"\n",
    "            self.cache = {}\n",
    "        self.arxiv_re = re.compile(r\"\\[[0-9v.]+\\] (.*)\")\n",
    "        \n",
    "    def save_cache(self):\n",
    "        try:\n",
    "            with open(\".paper_cache.json\", \"w\") as f:\n",
    "                json.dump(self.cache, f, indent=4, sort_keys=True, cls=DateEncoder)\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "            print \"Not able to save cache\"\n",
    "    \n",
    "    ends_with_version = re.compile(r\".*v([0-9])+$\")\n",
    "    def arxiv_link_version(self, url):\n",
    "        m = self.ends_with_version.match(url)\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_paper_data(self, url):\n",
    "        \"Ask arxiv for a (papername, {version:date}) if we don't know it\"\n",
    "        if not url:\n",
    "            return (None, None)    \n",
    "        if url in self.cache:\n",
    "            c = self.cache[url]\n",
    "            return (c[\"name\"], c.get(\"dates\"))\n",
    "        \n",
    "        req = requests.get(url)\n",
    "        tree = lxml.html.fromstring(req.content)\n",
    "        papername = tree.findtext('.//title')\n",
    "        record = {}\n",
    "        dates = None\n",
    "        if papername:\n",
    "            match = self.arxiv_re.match(papername)\n",
    "            if match:\n",
    "                papername = match.groups(0)[0]\n",
    "                v = self.arxiv_link_version(url)\n",
    "                dates = self.get_submission_dates(tree, v)\n",
    "                record[\"dates\"] = dates\n",
    "        record[\"name\"] = papername\n",
    "        self.cache[url] = record\n",
    "        print \"Caching paper name:\", papername\n",
    "        self.save_cache()\n",
    "        return papername, dates\n",
    "    \n",
    "    version_re = re.compile(r\"\\[v([0-9]+)\\] (.*[0-9][0-9][0-9][0-9]) \")\n",
    "\n",
    "    def get_submission_dates(self, arxiv_tree, queried_version):\n",
    "        links = CSSSelector(\"div.submission-history\")(arxiv_tree)[0]\n",
    "        versions = {}\n",
    "        #print \"Parsing\", links.text_content()\n",
    "        for line in links.text_content().split(\"\\n\"):\n",
    "            match = self.version_re.match(line)\n",
    "            if match:\n",
    "                version, d = match.group(1), match.group(2)\n",
    "                d = datetime.datetime.strptime(d,'%a, %d %b %Y').date()\n",
    "                versions[version] = d\n",
    "                if queried_version == version:\n",
    "                    return {version: d}\n",
    "                #print version, date\n",
    "\n",
    "        return versions\n",
    "\n",
    "ade = ArxivDataExtractor()\n",
    "\n",
    "ade.get_paper_data('http://arxiv.org/pdf/1412.6806.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources incorporated / to incorporate\n",
    "\n",
    "* \u2611 Jack Clark's collection of progress measurements (elsewhere in this repo!)\n",
    "* \u2610 Sarah Constantin's [Performance Trends in AI](https://srconstantin.wordpress.com/2017/01/28/performance-trends-in-ai/)\n",
    "* \u2610 Katja Grace's [Algorithmic Progress in Six Domains](https://intelligence.org/files/AlgorithmicProgress.pdf)\n",
    "* \u2611 Eric Yuan's [Comparison of Machine Reading Comprehension Datasets](http://eric-yuan.me/compare-popular-mrc-datasets/)\n",
    "* \u2610 Rodrigo Benenson's [\"Who is the Best at X / Are we there yet?\"](https://rodrigob.github.io/are_we_there_yet/build/#about) collating machine vision datasets & progress\n",
    "\n",
    "#### Notes on importing data\n",
    "\n",
    "* Each `.measure()` call is a data point of a specific algorithm on a specific metric/dataset. Thus one paper will often produce multiple measurements on multiple metrics. It's most important to enter results that were at or near the frontier of best performance on the date they were published, though this isn't a strict requirement and it's nice to have a sense of the performance of the field, or of algorithms that are otherwise notable even if they aren't the frontier for a sepcific problem.\n",
    "* When multiple revisions of a paper (typically on arXiv) have the same results on some metric, use the date of the first version (the CBTest results in [this paper](https://arxiv.org/abs/1606.02245v4) are an example)\n",
    "* When subsequent revisions of a paper improve on the original results ([example](https://arxiv.org/abs/1606.01549v3)), use the date and scores of the first results, or if each revision is interesting / on the frontier of best performance, include each paper\n",
    "  * We didn't check this carefully for our first ~100 measurement data points :(. In order to denote when we've checked which revision of an arXiv preprint first published a result, cite the specific version (https://arxiv.org/abs/1606.01549v3 rather than https://arxiv.org/abs/1606.01549); that way we can see which previous entries should be double-checked for this form of inaccuracy.\n",
    "* Where possible, use a clear short name or acronym for each algorithm. The full paper name can go in the `papername` field (and is auto-populated for some papers). When matplotlib 2.1 ships\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2169,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BEGIN ACTUALLY CLASSIFYING PROBLEMS\n",
    "\n",
    "scene_description = Problem(\"Scene description\", [\"agi\", \"vision\", \"language\", \"world-modelling\"])\n",
    "image_classification = Problem(\"Image classification\", [\"vision\", \"agi\"])\n",
    "image_classification.add_subproblem(scene_description)\n",
    "\n",
    "imagenet = image_classification.metric(\"imagenet\", \"http://image-net.org\", scale=error_rate, target=0.051)\n",
    "imagenet.notes = \"\"\"\n",
    "Correctly label images from the Imagenet dataset. As of 2016, this includes:\n",
    " - Object localization for 1000 categories.\n",
    " - Object detection for 200 fully labeled categories.\n",
    " - Object detection from video for 30 fully labeled categories.\n",
    " - Scene classification for 365 scene categories (Joint with MIT Places team) on Places2 Database http://places2.csail.mit.edu.\n",
    " - Scene parsing for 150 stuff and discrete object categories (Joint with MIT Places team).\n",
    "WARNING: these subchallenges were added in successive years of the Imagenet challenge, so results from years are not directly\n",
    "comparable; however progress should probably be understated by comparing them?\n",
    "Note about the warning (Yomna): I think Jack said this wasn't a problem.\n",
    "\"\"\"\n",
    "imagenet.measure(date(2010,8,31), 0.28191, \"NEC UIUC\", \"http://image-net.org/challenges/LSVRC/2010/results\")\n",
    "imagenet.measure(date(2011,10,26), 0.2577, \"XRCE\",\"http://image-net.org/challenges/LSVRC/2011/results\")\n",
    "imagenet.measure(date(2012,10,13), 0.16422, \"SuperVision\", \"http://image-net.org/challenges/LSVRC/2012/results.html\")\n",
    "imagenet.measure(date(2013,11,14), 0.11743, \"Clarifai\",\"http://www.image-net.org/challenges/LSVRC/2013/results.php\")\n",
    "imagenet.measure(date(2014,8,18), 0.07405, \"VGG\", \"http://image-net.org/challenges/LSVRC/2014/index\")\n",
    "imagenet.measure(date(2015,12,10), 0.03567, \"MSRA\", \"http://image-net.org/challenges/LSVRC/2015/results\", algorithms=[\"residual-networks\"])\n",
    "imagenet.measure(date(2016,9,26), 0.02991, \"Trimps-Soushen\", \"http://image-net.org/challenges/LSVRC/2016/results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vqa1_oe = scene_description.metric(\"COCO VQA v1.0 open ended\", url=\"http://visualqa.org/\", scale=correct_percent)\n",
    "vqa1_mc = scene_description.metric(\"COCO VQA v1.0 multiple choice\", url=\"http://visualqa.org/\", scale=correct_percent)\n",
    "vqa2_oe = scene_description.metric(\"COCO VQA v2.0 open ended\", url=\"http://visualqa.org/\", scale=correct_percent)\n",
    "vqa2_mc = scene_description.metric(\"COCO VQA v2.0 multiple choice\", url=\"http://visualqa.org/\", scale=correct_percent)\n",
    "\n",
    "vqa1_oe.measure(date(2015,12,15), 55.89, \"iBOWIMG baseline\", url=\"https://arxiv.org/abs/1512.02167\")\n",
    "vqa1_mc.measure(date(2015,12,15), 61.97, \"iBOWIMG baseline\", url=\"https://arxiv.org/abs/1512.02167\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Game Playing\n",
    "\n",
    "In principle, games are a sufficiently open-ended framework that all of intelligence could be captured within them.\n",
    "\n",
    "But abstract games like chess, go, checkers etc can be played with no knowldege of the human world or physics. Although this domain has largely been solved to super-human performance levels, there are a few ends that need to be tied up, especially in terms of having agents learn rules for arbitrary abstract games effectively given various plausible starting points (eg, textual descriptions of the rules or examples of correct play).\n",
    "\n",
    "## Abstract Strategy Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2171,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abstract_strategy_games = Problem(\"Abstract strategy games\", [\"agi\", \"abstract-games\"])\n",
    "\n",
    "playing_with_hints = Problem(\"Playing abstract games with extensive hints\", [\"abstract-games\"], solved=True)\n",
    "abstract_strategy_games.add_subproblem(playing_with_hints)\n",
    "playing_with_hints.notes = \"\"\"\n",
    "  Complex abstract strategy games have been solved to super-human levels\n",
    "  by computer systems with extensive rule-hinting and heuristics,\n",
    "  in some cases combined with machine learning techniques.\n",
    "\"\"\"\n",
    "computer_chess = playing_with_hints.metric(\"computer chess\", scale=elo, target=2882, target_label=\"Best human play\", target_source=\"https://en.wikipedia.org/w/index.php?title=Comparison_of_top_chess_players_throughout_history&oldid=777500496#Elo_system\")\n",
    "computer_go = playing_with_hints.metric(\"computer go\", scale=elo, target=3632, target_label=\"Best human play\", target_source=\"https://www.goratings.org/en/history/\")\n",
    "computer_go.solved = True # until we get proper data\n",
    "\n",
    "# For some caveats, see https://en.wikipedia.org/w/index.php?title=Chess_engine&oldid=764341963#Ratings\n",
    "# We could script ingestion of data from CCRL, or get data from Katja\n",
    "computer_chess.measure(date(1997,05,11), 2725, \"Deep Blue\", uncertainty=25, url=\"https://www.quora.com/What-was-Deep-Blues-Elo-rating\")\n",
    "computer_chess.measure(date(2006,05,27), 2995, \"Rybka 1.1 64bit\", uncertainty=25, url=\"https://web.archive.org/web/20060531091049/http://www.computerchess.org.uk/ccrl/4040/rating_list_all.html\")\n",
    "computer_chess.measure(date(2010,8,7), 3269, \"Rybka 4 64bit\", uncertainty=22, url=\"https://web.archive.org/web/20100923131123/http://www.computerchess.org.uk/ccrl/4040/rating_list_all.html\")\n",
    "computer_chess.measure(date(2013,7,20), 3248, \"Houdini 3 64bit\", uncertainty=16, url=\"https://web.archive.org/web/20130415000000*/http://www.computerchess.org.uk/ccrl/4040/rating_list_all.html\")\n",
    "computer_chess.measure(date(2015,7,4), 3332, \"Komodo 9\", uncertainty=24, url=\"https://web.archive.org/web/20150708104805/http://www.computerchess.org.uk/ccrl/4040/rating_list_all.html\")\n",
    "computer_chess.measure(date(2017,02,27), 3393, \"Stockfish\", uncertainty=50, url=\"https://web.archive.org/web/20170227044521/http://www.computerchess.org.uk/ccrl/4040/\")\n",
    "# Wikipedia has some nice data here:\n",
    "computer_chess.measure(date(1984,12,31), 1631, \"Novag Super Constellation 6502 4 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1985,12,31), 1827, \"Mephisto Amsterdam 68000 12 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1986,12,31), 1827, \"Mephisto Amsterdam 68000 12 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1987,12,31), 1923, \"Mephisto Dallas 68020 14 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1988,12,31), 1993, \"Mephisto MM 4 Turbo Kit 6502 16 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1989,12,31), 2027, \"Mephisto Portorose 68020 12 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1990,12,31), 2138, \"Mephisto Portorose 68030 36 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1991,12,31), 2127, \"Mephisto Vancouver 68030 36 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1992,12,31), 2174, \"Chess Machine Schroder 3.0 ARM2 30 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1993,12,31), 2235, \"Mephisto Genius 2.0 486/50-66 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1995,12,31), 2306, \"MChess Pro 5.0 Pentium 90 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1996,12,31), 2337, \"Rebel 8.0 Pentium 90 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1997,12,31), 2418, \"HIARCS 6.0 49MB P200 MMX\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1998,12,31), 2460, \"Fritz 5.0 PB29% 67MB P200 MMX\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1999,12,31), 2594, \"Chess Tiger 12.0 DOS 128MB K6-2 450 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2000,12,31), 2607, \"Fritz 6.0 128MB K6-2 450 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2001,12,31), 2709, \"Chess Tiger 14.0 CB 256MB Athlon 1200\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2002,12,31), 2759, \"Deep Fritz 7.0 256MB Athlon 1200 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2003,12,31), 2791, \"Shredder 7.04 UCI 256MB Athlon 1200 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2004,12,31), 2800, \"Shredder 8.0 CB 256MB Athlon 1200 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2005,12,31), 2808, \"Shredder 9.0 UCI 256MB Athlon 1200 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2006,12,31), 2902, \"Rybka 1.2 256MB Athlon 1200 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2007,12,31), 2935, \"Rybka 2.3.1 Arena 256MB Athlon 1200 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2008,12,31), 3238, \"Deep Rybka 3 2GB Q6600 2.4 GHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2009,12,31), 3232, \"Deep Rybka 3 2GB Q6600 2.4 GHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2010,12,31), 3227, \"Deep Rybka 3 2GB Q6600 2.4 GHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2011,12,31), 3216, \"Deep Rybka 4 2GB Q6600 2.4 GHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2012,12,31), 3221, \"Deep Rybka 4 x64 2GB Q6600 2.4 GHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2013,12,31), 3241, \"Komodo 5.1 MP x64 2GB Q6600 2.4 GHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2014,12,31), 3295, \"Komodo 7.0 MP x64 2GB Q6600 2.4 GHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2015,12,31), 3334, \"Stockfish 6 MP x64 2GB Q6600 2.4 GHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2016,12,31), 3366, \"Komodo 9.1 MP x64 2GB Q6600 2.4 GHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "\n",
    "\n",
    "mastering_historical_games = Problem(\"Superhuman mastery of arbitrary abstract strategy games\", [\"super\", \"abstract-games\"])\n",
    "abstract_strategy_games.add_subproblem(mastering_historical_games)\n",
    "mastering_chess = mastering_historical_games.metric(\"mastering chess\")\n",
    "mastering_chess.notes = \"\"\"\n",
    "  Beating all humans at chess, given a corpus of past play amongst masters,\n",
    "  but no human-crafted policy constraints and heuristics. This will probably fall out\n",
    "  immediately once learning_abstract_game_rules is solved, since playing_with_hints\n",
    "  has been solved.\n",
    "\"\"\"\n",
    "\n",
    "# Are there any published metrics for these yet?\n",
    "learning_abstract_game_rules = Problem(\"Learning the rules of complex strategy games from examples\", [\"agi\", \"abstract-games\"])\n",
    "abstract_strategy_games.add_subproblem(learning_abstract_game_rules)\n",
    "learning_chess = learning_abstract_game_rules.metric(\"learning chess\")\n",
    "learning_chess.notes = \"\"\"\n",
    "  Chess software contains hard-coded policy constraints for valid play; this metric is whether RL\n",
    "  or other agents can correctly build those policy constraints from examples or oracles\"\"\"\n",
    "learning_go = learning_abstract_game_rules.metric(\"learning go\")\n",
    "learning_go.notes = \"\"\"\n",
    "  Go software contains policy constraints for valid play and evaluating the number of\n",
    "  liberties for groups. This metric is whether RL or other agents can correctly build those \n",
    "  policy constraints from examples or oracles\"\"\"\n",
    "\n",
    "learning_arbitrary_abstract_games = Problem(\"Play an arbitrary abstract game, first learning the rules\", [\"agi\", \"abstract-games\"])\n",
    "abstract_strategy_games.add_subproblem(learning_arbitrary_abstract_games)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time video games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2172,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Atari 2600 Games: Breakout, Enduro, Pong, Q*Bert, Seaquest, S. Invaders. Each game has its own metric.\n",
    "\n",
    "simple_games = Problem(\"Simple video games\", [\"world-modelling\", \"realtime-games\", \"agi\"]) \n",
    "\n",
    "# Alien\n",
    "alien_metric = simple_games.metric(\"Atari 2600 Alien\", scale=linear, target=6875, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "alien_metric.measure(date(2015, 2, 26), 3069, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "alien_metric.measure(date(2015,11,20), 1620, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "alien_metric.measure(date(2015,11,20), 3747.7, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "alien_metric.measure(date(2015,11,20), 4461.4, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Amidar\n",
    "amidar_metric = simple_games.metric(\"Atari 2600 Amidar\", scale=linear, target=1676, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "amidar_metric.measure(date(2015, 2, 26), 739.5, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "amidar_metric.measure(date(2015,11,20), 978, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "amidar_metric.measure(date(2015,11,20), 1793.3, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "amidar_metric.measure(date(2015,11,20), 2354.5, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Assault\n",
    "assault_metric = simple_games.metric(\"Atari 2600 Assault\", scale=linear, target=1496, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "assault_metric.measure(date(2015, 2, 26), 3359, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "assault_metric.measure(date(2015,11,20), 4280.4, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "assault_metric.measure(date(2015,11,20), 5393.2, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "assault_metric.measure(date(2015,11,20), 4621.0, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Asterix\n",
    "asterix_metric = simple_games.metric(\"Atari 2600 Asterix\", scale=linear, target=8503, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "asterix_metric.measure(date(2015, 2, 26), 6012, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "asterix_metric.measure(date(2015,11,20), 4359, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "asterix_metric.measure(date(2015,11,20), 17356, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "asterix_metric.measure(date(2015,11,20), 28188, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Asteroids\n",
    "asteroids_metric = simple_games.metric(\"Atari 2600 Asteroids\", scale=linear, target=13157, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "asteroids_metric.measure(date(2015, 2, 26), 1629, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "asteroids_metric.measure(date(2015,11,20), 1364.5, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "asteroids_metric.measure(date(2015,11,20), 734.7, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "asteroids_metric.measure(date(2015,11,20), 2837.7, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Atlantis\n",
    "atlantis_metric = simple_games.metric(\"Atari 2600 Atlantis\", scale=linear, target=29028, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "atlantis_metric.measure(date(2015, 2, 26), 85641, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "atlantis_metric.measure(date(2015,11,20), 279987, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "atlantis_metric.measure(date(2015,11,20), 106056, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "atlantis_metric.measure(date(2015,11,20), 382572, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Bank Heist\n",
    "bank_heist_metric = simple_games.metric(\"Atari 2600 Bank Heist\", scale=linear, target=734.4, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "bank_heist_metric.measure(date(2015, 2, 26), 429.7, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "bank_heist_metric.measure(date(2015,11,20), 455, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "bank_heist_metric.measure(date(2015,11,20), 1030.6, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "bank_heist_metric.measure(date(2015,11,20), 1611.9, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Battle Zone\n",
    "battle_zone_metric = simple_games.metric(\"Atari 2600 Battle Zone\", scale=linear, target=37800, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "battle_zone_metric.measure(date(2015, 2, 26), 26300, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "battle_zone_metric.measure(date(2015,11,20), 29900, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "battle_zone_metric.measure(date(2015,11,20), 31700, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "battle_zone_metric.measure(date(2015,11,20), 37150, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Beam Rider\n",
    "beam_rider_metric = simple_games.metric(\"Atari 2600 Beam Rider\", scale=linear, target=7456, target_source=\"https://arxiv.org/pdf/1312.5602.pdf\")\n",
    "beam_rider_metric.measure(date(2013,12,19), 4092, \"DQN\", \"https://arxiv.org/pdf/1312.5602.pdf\")\n",
    "beam_rider_metric.measure(date(2015,11,20), 8627, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "beam_rider_metric.measure(date(2015,11,20), 13772, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "beam_rider_metric.measure(date(2015,11,20), 12164, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Berzerk \n",
    "berzerk_metric = simple_games.metric(\"Atari 2600 Berzerk\", scale=linear, target=2630.4, target_source=\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "berzerk_metric.measure(date(2015,11,20), 585, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "berzerk_metric.measure(date(2015,11,20), 1225, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "berzerk_metric.measure(date(2015,11,20), 1472, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Bowling\n",
    "bowling_metric = simple_games.metric(\"Atari 2600 Bowling\", scale=linear, target=154.8, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "bowling_metric.measure(date(2015, 2, 26), 42.8, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "bowling_metric.measure(date(2015,11,20), 50.4, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "bowling_metric.measure(date(2015,11,20), 68.1, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "bowling_metric.measure(date(2015,11,20), 65.5, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Boxing\n",
    "boxing_metric = simple_games.metric(\"Atari 2600 Boxing\", scale=linear, target=4.3, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "boxing_metric.measure(date(2015, 2, 26), 71.8, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "boxing_metric.measure(date(2015,11,20), 88, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "boxing_metric.measure(date(2015,11,20), 91.6, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "boxing_metric.measure(date(2015,11,20), 99.4, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Breakout\n",
    "breakout_metric = simple_games.metric(\"Atari 2600 Breakout\", scale=linear, target=31.8, target_source=\"https://pdfs.semanticscholar.org/340f/48901f72278f6bf78a04ee5b01df208cc508.pdf\")\n",
    "breakout_metric.measure(date(2013,12,19), 225, \"DQN\", \"https://arxiv.org/pdf/1312.5602.pdf\")\n",
    "breakout_metric.measure(date(2015,2,26), 401.2, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "breakout_metric.measure(date(2015,10,22), 375, \"DoubleDQN\", \"https://pdfs.semanticscholar.org/3b97/32bb07dc99bde5e1f9f75251c6ea5039373e.pdf\")\n",
    "\n",
    "breakout_metric.measure(date(2015,11,20), 385.5, \"DQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "breakout_metric.measure(date(2015,11,20), 418.5, \"DDQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "breakout_metric.measure(date(2015,11,20), 345.3, \"Duel\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "breakout_metric.measure(date(2016,6,16), 766.8, \"A3C LSTM\", \"https://arxiv.org/pdf/1602.01783.pdf\")\n",
    "\n",
    "# Centipede\n",
    "centipede_metric = simple_games.metric(\"Atari 2600 Centipede\", scale=linear, target=11963, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "centipede_metric.measure(date(2015, 2, 26), 8309, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "centipede_metric.measure(date(2015,11,20), 4657, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "centipede_metric.measure(date(2015,11,20), 5409, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "centipede_metric.measure(date(2015,11,20), 7561, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Chopper Command\n",
    "chopper_command_metric = simple_games.metric(\"Atari 2600 Chopper Command\", scale=linear, target=9882, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "chopper_command_metric.measure(date(2015, 2, 26), 6687, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "chopper_command_metric.measure(date(2015,11,20), 6126, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "chopper_command_metric.measure(date(2015,11,20), 5809, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "chopper_command_metric.measure(date(2015,11,20), 11215, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Crazy Climber\n",
    "crazy_climber_metric = simple_games.metric(\"Atari 2600 Crazy Climber\", scale=linear, target=35411, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "crazy_climber_metric.measure(date(2015, 2, 26), 114103, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "crazy_climber_metric.measure(date(2015,11,20), 110763, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "crazy_climber_metric.measure(date(2015,11,20), 117282, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "crazy_climber_metric.measure(date(2015,11,20), 143570, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Demon Attack\n",
    "demon_attack_metric = simple_games.metric(\"Atari 2600 Demon Attack\", scale=linear, target=3401, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "demon_attack_metric.measure(date(2015, 2, 26), 9711, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "demon_attack_metric.measure(date(2015,11,20), 12149, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "demon_attack_metric.measure(date(2015,11,20), 58044, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "demon_attack_metric.measure(date(2015,11,20), 60813, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Double Dunk\n",
    "# TODO: investigate alternate scale\n",
    "double_dunk_metric = simple_games.metric(\"Atari 2600 Double Dunk\", scale=linear, target=-15.5, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "double_dunk_metric.measure(date(2015, 2, 26), -18.1, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "double_dunk_metric.measure(date(2015,11,20), -6.6, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "double_dunk_metric.measure(date(2015,11,20), -5.5, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "double_dunk_metric.measure(date(2015,11,20), 0.1, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Enduro\n",
    "enduro_metric = simple_games.metric(\"Atari 2600 Enduro\", scale=linear, target=309.6, target_source=\"https://pdfs.semanticscholar.org/340f/48901f72278f6bf78a04ee5b01df208cc508.pdf\")\n",
    "enduro_metric.measure(date(2013,12,19), 661, \"DQN\", \"https://arxiv.org/pdf/1312.5602.pdf\")\n",
    "enduro_metric.measure(date(2015,2,26), 301, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "enduro_metric.measure(date(2015,10,22), 319, \"DoubleDQN\", \"https://pdfs.semanticscholar.org/3b97/32bb07dc99bde5e1f9f75251c6ea5039373e.pdf\")\n",
    "\n",
    "enduro_metric.measure(date(2015,11,20), 729.0, \"DQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "enduro_metric.measure(date(2015,11,20), 1211.8, \"DDQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "enduro_metric.measure(date(2015,11,20), 2258.2, \"Duel\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "enduro_metric.measure(date(2016,6,16), 82.5, \"A3C LSTM\", \"https://arxiv.org/pdf/1602.01783.pdf\")\n",
    "\n",
    "# Fishing Derby\n",
    "fishing_derby_metric = simple_games.metric(\"Atari 2600 Fishing Derby\", scale=linear, target=5.5, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "fishing_derby_metric.measure(date(2015, 2, 26), -0.8, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "fishing_derby_metric.measure(date(2015,11,20), -4.9, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "fishing_derby_metric.measure(date(2015,11,20), 15.5, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "fishing_derby_metric.measure(date(2015,11,20), 46.4, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Freeway\n",
    "freeway_metric = simple_games.metric(\"Atari 2600 Freeway\", scale=linear, target=29.6, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "freeway_metric.measure(date(2015, 2, 26), 30.3, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "freeway_metric.measure(date(2015,11,20), 30.8, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "freeway_metric.measure(date(2015,11,20), 33.3, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "freeway_metric.measure(date(2015,11,20), 0, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Frostbite\n",
    "frostbite_metric = simple_games.metric(\"Atari 2600 Frostbite\", scale=linear, target=4355, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "frostbite_metric.measure(date(2015, 2, 26), 328.3, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "frostbite_metric.measure(date(2015,11,20), 797.7, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "frostbite_metric.measure(date(2015,11,20), 1683.3, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "frostbite_metric.measure(date(2015,11,20), 4672, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Gopher\n",
    "gopher_metric = simple_games.metric(\"Atari 2600 Gopher\", scale=linear, target=2321, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "gopher_metric.measure(date(2015, 2, 26), 8520, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "gopher_metric.measure(date(2015,11,20), 8777, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "gopher_metric.measure(date(2015,11,20), 14840, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "gopher_metric.measure(date(2015,11,20), 15718, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Gravitar\n",
    "gravitar_metric = simple_games.metric(\"Atari 2600 Gravitar\", scale=linear, target=2672, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "gravitar_metric.measure(date(2015, 2, 26), 306.7, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "gravitar_metric.measure(date(2015,11,20), 473, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "gravitar_metric.measure(date(2015,11,20), 412, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "gravitar_metric.measure(date(2015,11,20), 588, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# H.E.R.O.\n",
    "hero_metric = simple_games.metric(\"Atari 2600 HERO\", scale=linear, target=25763, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "hero_metric.measure(date(2015, 2, 26), 19950, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "hero_metric.measure(date(2015,11,20), 20437, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "hero_metric.measure(date(2015,11,20), 20130, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "hero_metric.measure(date(2015,11,20), 20818, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Ice Hockey\n",
    "ice_hockey_metric = simple_games.metric(\"Atari 2600 Ice Hockey\", scale=linear, target=0.9, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "ice_hockey_metric.measure(date(2015, 2, 26), -1.6, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# James Bond\n",
    "james_bond_metric = simple_games.metric(\"Atari 2600 James Bond\", scale=linear, target=406.7, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "james_bond_metric.measure(date(2015, 2, 26), 576.7, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Kangaroo\n",
    "kangaroo_metric = simple_games.metric(\"Atari 2600 Kangaroo\", scale=linear, target=3035, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "kangaroo_metric.measure(date(2015, 2, 26), 6740, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Krull\n",
    "krull_metric = simple_games.metric(\"Atari 2600 Krull\", scale=linear, target=2395, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "krull_metric.measure(date(2015, 2, 26), 3805, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Kung-Fu Master\n",
    "kung_fu_master_metric = simple_games.metric(\"Atari 2600 Kung-Fu Master\", scale=linear, target=22736, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "kung_fu_master_metric.measure(date(2015, 2, 26), 23270, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Montezuma's Revenge\n",
    "montezumas_revenge_metric = simple_games.metric(\"Atari 2600 Montezuma's Revenge\", scale=linear, target=4367, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "montezumas_revenge_metric.measure(date(2015, 2, 26), 0, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Ms. Pacman\n",
    "ms_pacman_metric = simple_games.metric(\"Atari 2600 Ms. Pacman\", scale=linear, target=15693, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "ms_pacman_metric.measure(date(2015, 2, 26), 2311, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Name This Game\n",
    "name_this_game_metric = simple_games.metric(\"Atari 2600 Name This Game\", scale=linear, target=4076, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "name_this_game_metric.measure(date(2015, 2, 26), 7257, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Pong\n",
    "pong_metric = simple_games.metric(\"Atari 2600 Pong\", scale=linear, target=9.3, target_source=\"https://pdfs.semanticscholar.org/340f/48901f72278f6bf78a04ee5b01df208cc508.pdf\")\n",
    "pong_metric.measure(date(2013,12,19), 21, \"DQN\", \"https://arxiv.org/pdf/1312.5602.pdf\")\n",
    "pong_metric.measure(date(2015,2,26), 18.9, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "pong_metric.measure(date(2015,10,22), 21, \"DoubleDQN\", \"https://pdfs.semanticscholar.org/3b97/32bb07dc99bde5e1f9f75251c6ea5039373e.pdf\")\n",
    "\n",
    "pong_metric.measure(date(2015,11,20), 21, \"DQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf?_ga=1.123524811.1334652001.1475539859\")\n",
    "pong_metric.measure(date(2015,11,20), 20.9, \"DDQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf?_ga=1.123524811.1334652001.1475539859\")\n",
    "pong_metric.measure(date(2015,11,20), 19.5, \"Duel\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf?_ga=1.123524811.1334652001.1475539859\")\n",
    "\n",
    "pong_metric.measure(date(2016,6,16), 10.7, \"A3C LSTM\", \"https://arxiv.org/pdf/1602.01783.pdf\")\n",
    "\n",
    "# Private Eye\n",
    "private_eye_metric = simple_games.metric(\"Atari 2600 Private Eye\", scale=linear, target=69571, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "private_eye_metric.measure(date(2015, 2, 26), 1788, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Q*Bert\n",
    "q_bert_metric = simple_games.metric(\"Atari 2600 Q*Bert\", scale=linear, target=13455, target_source=\"https://pdfs.semanticscholar.org/340f/48901f72278f6bf78a04ee5b01df208cc508.pdf\")\n",
    "q_bert_metric.measure(date(2013,12,19), 4500, \"DQN\", \"https://arxiv.org/pdf/1312.5602.pdf\")\n",
    "q_bert_metric.measure(date(2015,2,26), 10596, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "q_bert_metric.measure(date(2015,10,22), 14875, \"DoubleDQN\", \"https://pdfs.semanticscholar.org/3b97/32bb07dc99bde5e1f9f75251c6ea5039373e.pdf\")\n",
    "\n",
    "q_bert_metric.measure(date(2015,11,20), 13117.3, \"DQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "q_bert_metric.measure(date(2015,11,20), 15088.5, \"DDQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "q_bert_metric.measure(date(2015,11,20), 19220, \"Duel\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "q_bert_metric.measure(date(2016,6,16), 21307, \"A3C LSTM\", \"https://arxiv.org/pdf/1602.01783.pdf\")\n",
    "\n",
    "# River Raid\n",
    "river_raid_metric = simple_games.metric(\"Atari 2600 River Raid\", scale=linear, target=13513, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "river_raid_metric.measure(date(2015, 2, 26), 8316, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Road Runner\n",
    "road_runner_metric = simple_games.metric(\"Atari 2600 Road Runner\", scale=linear, target=7845, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "road_runner_metric.measure(date(2015, 2, 26), 18257, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Robotank\n",
    "robotank_metric = simple_games.metric(\"Atari 2600 Robotank\", scale=linear, target=11.9, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "robotank_metric.measure(date(2015, 2, 26), 51.6, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Seaquest\n",
    "seaquest_metric = simple_games.metric(\"Atari 2600 Seaquest\", scale=linear, target=20182, target_source=\"https://pdfs.semanticscholar.org/340f/48901f72278f6bf78a04ee5b01df208cc508.pdf\")\n",
    "seaquest_metric.measure(date(2013,12,19), 1740, \"DQN\", \"https://arxiv.org/pdf/1312.5602.pdf\")\n",
    "seaquest_metric.measure(date(2015,2,26), 5286, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "seaquest_metric.measure(date(2015,10,22), 7995, \"DoubleDQN\", \"https://pdfs.semanticscholar.org/3b97/32bb07dc99bde5e1f9f75251c6ea5039373e.pdf?_ga=1.165640319.1334652001.1475539859\")\n",
    "\n",
    "seaquest_metric.measure(date(2015,11,20), 5860.6, \"DQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf?_ga=1.123524811.1334652001.1475539859\")\n",
    "seaquest_metric.measure(date(2015,11,20), 16452.7, \"DDQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf?_ga=1.123524811.1334652001.1475539859\")\n",
    "seaquest_metric.measure(date(2015,11,20), 50254.2, \"Duel\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf?_ga=1.123524811.1334652001.1475539859\")\n",
    "\n",
    "seaquest_metric.measure(date(2016,6,16), 1326.1, \"A3C LSTM\", \"https://arxiv.org/pdf/1602.01783.pdf\")\n",
    "\n",
    "# Space Invaders\n",
    "space_invaders_metric = simple_games.metric(\"Atari 2600 Space Invaders\", scale=linear, target=1652, target_source=\"https://pdfs.semanticscholar.org/340f/48901f72278f6bf78a04ee5b01df208cc508.pdf\")\n",
    "space_invaders_metric.measure(date(2013,12,19), 1075, \"DQN\", \"https://arxiv.org/pdf/1312.5602.pdf\")\n",
    "space_invaders_metric.measure(date(2015,2,26), 1976, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "space_invaders_metric.measure(date(2015,10,22), 3154, \"DoubleDQN\", \"https://pdfs.semanticscholar.org/3b97/32bb07dc99bde5e1f9f75251c6ea5039373e.pdf?_ga=1.165640319.1334652001.1475539859\")\n",
    "\n",
    "space_invaders_metric.measure(date(2015,11,20), 1692.3, \"DQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf?_ga=1.123524811.1334652001.1475539859\")\n",
    "space_invaders_metric.measure(date(2015,11,20), 2525.5, \"DDQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf?_ga=1.123524811.1334652001.1475539859\")\n",
    "space_invaders_metric.measure(date(2015,11,20), 6427.3, \"Duel\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf?_ga=1.123524811.1334652001.1475539859\")\n",
    "\n",
    "space_invaders_metric.measure(date(2016,6,16), 23846, \"A3C LSTM\", \"https://arxiv.org/pdf/1602.01783.pdf\")\n",
    "\n",
    "# Star Gunner\n",
    "star_gunner_metric = simple_games.metric(\"Atari 2600 Star Gunner\", scale=linear, target=10250, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "star_gunner_metric.measure(date(2015, 2, 26), 57997, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Tennis\n",
    "# TODO: negative linear scale?\n",
    "tennis_metric = simple_games.metric(\"Atari 2600 Tennis\", scale=linear, target=-8.9, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "tennis_metric.measure(date(2015, 2, 26), -2.5, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Time Pilot\n",
    "time_pilot_metric = simple_games.metric(\"Atari 2600 Time Pilot\", scale=linear, target=5925, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "time_pilot_metric.measure(date(2015, 2, 26), 5947, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Tutankham\n",
    "tutankham_metric = simple_games.metric(\"Atari 2600 Tutankham\", scale=linear, target=167.6, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "tutankham_metric.measure(date(2015, 2, 26), 186.7, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Up and Down\n",
    "up_and_down_metric = simple_games.metric(\"Atari 2600 Up and Down\", scale=linear, target=9082, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "up_and_down_metric.measure(date(2015, 2, 26), 8456, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Venture\n",
    "venture_metric = simple_games.metric(\"Atari 2600 Venture\", scale=linear, target=1188, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "venture_metric.measure(date(2015, 2, 26), 3800, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Video Pinball\n",
    "video_pinball_metric = simple_games.metric(\"Atari 2600 Video Pinball\", scale=linear, target=17298, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "video_pinball_metric.measure(date(2015, 2, 26), 42684, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Wizard of Wor\n",
    "wizard_of_wor_metric = simple_games.metric(\"Atari 2600 Wizard of Wor\", scale=linear, target=4757, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "wizard_of_wor_metric.measure(date(2015, 2, 26), 3393, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Zaxxon\n",
    "zaxxon_metric = simple_games.metric(\"Atari 2600 Zaxxon\", scale=linear, target=9173, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "zaxxon_metric.measure(date(2015, 2, 26), 4977, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2173,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Speech recognition\n",
    "\n",
    "\"\"\"\n",
    "        http://melodi.ee.washington.edu/s3tp/\n",
    "\n",
    "* * *\n",
    "**_Word error rate on Switchboard (specify details): [Month, Year: Score [SWB]: Team].  Compiled by Jack Clark._**\n",
    "\n",
    "A note about measurement: We're measuring Switchboard (SWB) and Call Home (CH) performance (mostly) from the Hub5'00 dataset, with main scores assesses in terms of word error rate on SWB. We also create \n",
    "\n",
    "Why do we care: Reflects the improvement of audio processing systems on speech over time.\n",
    "\n",
    "\"\"\"\n",
    "speech_recognition = Problem(name=\"Speech Recognition\", attributes=[\"language\", \"agi\"])\n",
    "swb_hub_500 = speech_recognition.metric(name=\"Word error rate on Switchboard trained against the Hub5'00 dataset\",\n",
    "                                               scale=error_percent, target=5.9)\n",
    "swb_hub_500.measure(date(2011,8,31), 16.1, \"CD-DNN\", \"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/CD-DNN-HMM-SWB-Interspeech2011-Pub.pdf\")\n",
    "swb_hub_500.measure(date(2012,4,27), 18.5, \"DNN-HMM\", \"https://pdfs.semanticscholar.org/ce25/00257fda92338ec0a117bea1dbc0381d7c73.pdf?_ga=1.195375081.452266805.1483390947\")\n",
    "\n",
    "swb_hub_500.measure(date(2013,8,25), 12.9, \"DNN MMI\", \"http://www.danielpovey.com/files/2013_interspeech_dnn.pdf\")\n",
    "swb_hub_500.measure(date(2013,8,25), 12.6, \"DNN sMBR\", \"http://www.danielpovey.com/files/2013_interspeech_dnn.pdf\")\n",
    "swb_hub_500.measure(date(2013,8,25), 12.9, \"DNN MPE\", \"http://www.danielpovey.com/files/2013_interspeech_dnn.pdf\")\n",
    "swb_hub_500.measure(date(2013,8,25), 12.9, \"DNN BMMI\", \"http://www.danielpovey.com/files/2013_interspeech_dnn.pdf\")\n",
    "\n",
    "swb_hub_500.measure(date(2014,6,30), 16, \"DNN\", \"https://arxiv.org/abs/1406.7806v1\")\n",
    "\n",
    "swb_hub_500.measure(date(2014,12,7), 20, \"Deep Speech\", \"https://arxiv.org/abs/1412.5567\")\n",
    "swb_hub_500.measure(date(2014,12,7), 12.6, \"Deep Speech + FSH\", url=\"https://arxiv.org/abs/1412.5567\") # TODO: why is this also included?\n",
    "\n",
    "swb_hub_500.measure(date(2015,5,21), 8.0, \"IBM 2015\", \"https://arxiv.org/abs/1505.05899\") # TODO: (name check)\n",
    "swb_hub_500.measure(date(2016,4,27), 6.9, \"IBM 2016\", \"https://arxiv.org/abs/1604.08242v1\") # TODO: (name check)\n",
    "\n",
    "swb_hub_500.measure(date(2017,2,17), 6.9, \"RNNLM\", \"https://arxiv.org/abs/1609.03528\") # TODO: (name check)\n",
    "swb_hub_500.measure(date(2017,2,17), 6.2, \"Microsoft 2016\", \"https://arxiv.org/abs/1609.03528\") # TODO: (name check)\n",
    "\n",
    "swb_hub_500.measure(date(2016,10,17), 6.6, \"CNN-LSTM\", \"https://arxiv.org/abs/1610.05256\") # TODO: (name check)\n",
    "swb_hub_500.measure(date(2016,10,17), 5.9, \"CNN-LSTM\",\"https://arxiv.org/abs/1610.05256\") # TODO: (name check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2174,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Image Generation\n",
    "\n",
    "\"\"\" \n",
    "* * *\n",
    "**_Generative models of CIFAR-10 Natural Images _****[Year: bits-per-subpixel, method]. Compiled by Durk Kingma.**\n",
    "\n",
    "**Why we care:**\n",
    "(1) The compression=prediction=understanding=intelligence view (see Hutter prize, etc.). (Note that perplexity, log-likelihood, and #bits are all equivalent measurements.)\n",
    "(2) Learning a generative model is a prominent auxiliary task towards semi-supervised learning. Current SOTA semi-supervised classification results utilize generative models.\n",
    "3) You're finding patterns in the data that let you compress it more efficiently. Ultimate pattern recognition benchmark because you're trying to find the patterns in all the data. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "image_generation = Problem(\"Generative models of CIFAR-10 Natural Images\", [\"vision\", \"agi\"])\n",
    "# note: this section is not on scene generation, but making the distinction seemed like a good idea.\n",
    "scene_generation = Problem(\"Be able to generate complex scene e.g. a baboon receiving their degree at convocatoin.\", [\"vision\", \"world-modelling\", \"agi\"])\n",
    "scene_generation.add_subproblem(image_generation)\n",
    "\n",
    "# NOTE: scale, and target need to be checked\n",
    "image_generation_metric = image_generation.metric(name=\"bits-per-subpixel\", scale=bits_per_x)\n",
    "\n",
    "image_generation_metric.measure(date(2014,10,30), 4.48, \"NICE\", \"https://arxiv.org/abs/1410.8516\")\n",
    "image_generation_metric.measure(date(2015,2,16), 4.13, \"DRAW\", \"https://arxiv.org/abs/1502.04623\")\n",
    "image_generation_metric.measure(date(2016,5,27), 3.49, \"Real NVP\", \"https://arxiv.org/abs/1605.08803\")\n",
    "image_generation_metric.measure(date(2016,6,15), 3.11, \"VAE with IAF\", \"https://papers.nips.cc/paper/6581-improved-variational-inference-with-inverse-autoregressive-flow\")\n",
    "image_generation_metric.measure(date(2016,5,27), 3.0, \"PixelRNN\", \"https://arxiv.org/abs/1605.08803\")\n",
    "image_generation_metric.measure(date(2016,11,4), 2.92, \"PixelCNN++\", \"https://openreview.net/forum?id=BJrFC6ceg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modelling and Comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text compression is one way to see how well machine learning systems are able to model human language. Shannon's [classic 1951 paper]() obtained an expiermental measure of human text compression performance at 0.6 - 1.3 bits per character: humans know, better than classic algorithms, what word is likely to come next in a piece of writing. More recent work ([Moradi 1998](https://pdfs.semanticscholar.org/48bc/ce35ceb72068723d5f360f388a073aadadca.pdf), Cover 1978) provides estimates that are text-relative and in the 1.3 bits per character (and for some texts, much higher) range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "modelling_english = Problem(\"Accurate modelling of human language.\", [\"language\", \"agi\"])\n",
    "ptperplexity = modelling_english.metric(name=\"Penn Treebank Perplexity\", scale=perplexity)\n",
    "ptperplexity.measure(date(2016,10,27), 66, \"Recurrent Highway Networks\", \"https://arxiv.org/pdf/1607.03474v3\")\n",
    "ptperplexity.measure(date(2016,9,26), 70.9, \"Pointer Sentinel-LSTM\", \"https://arxiv.org/pdf/1609.07843v1.pdf\")\n",
    "ptperplexity.measure(date(2016,10,5), 73.4, \"Variational LSTM\", \"https://arxiv.org/pdf/1512.05287v5.pdf\")\n",
    "ptperplexity.measure(date(2014,10,8), 82.2, \"RNN Dropout Regularization\", \"https://arxiv.org/abs/1409.2329\")\n",
    "ptperplexity.measure(date(2013,12,20), 107.5, \"Deep RNN\", \"https://arxiv.org/abs/1312.6026\")\n",
    "ptperplexity.measure(date(2012,12,2), 124.7, \"RNNLM\", \"https://pdfs.semanticscholar.org/04e0/fefb859f4b02b017818915a2645427bfbdb2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hp_compression = modelling_english.metric(name=\"Hutter Prize (Bits per Character)\", scale=bits_per_x, target=1.3)\n",
    "hp_compression.measure(date(2016,10,31), 1.313, \"Surprisal-Driven Zoneout\",\n",
    "                   \"https://pdfs.semanticscholar.org/e9bc/83f9ff502bec9cffb750468f76fdfcf5dd05.pdf\")\n",
    "hp_compression.measure(date(2016,10,19), 1.37, \"Surprisal-Driven Feedback RNN\",\n",
    "                   \"https://arxiv.org/pdf/1608.06027.pdf\")\n",
    "hp_compression.measure(date(2016,9,27), 1.39, \"Hypernetworks\", \"https://arxiv.org/abs/1609.09106\")\n",
    "hp_compression.measure(date(2016,9,6), 1.32, \" Hierarchical Multiscale RNN\", \"https://arxiv.org/abs/1609.01704\")\n",
    "hp_compression.measure(date(2016,7,12), 1.32, \"Recurrent Highway Networks\", \"https://arxiv.org/abs/1607.03474\")\n",
    "hp_compression.measure(date(2015,7,6), 1.47, \"Grid LSTM\", \"https://arxiv.org/abs/1507.01526\")\n",
    "hp_compression.measure(date(2015,2,15), 1.58, \"Gated Feedback RNN\", \"https://arxiv.org/abs/1502.02367\")\n",
    "# we need to match/double check the release date of the specific version of cmix that got this performance?\n",
    "# hp_compression.measure(date(2014,4,13), 1.245, \"cmix\", \"http://www.byronknoll.com/cmix.html\")\n",
    "hp_compression.measure(date(2013,8,4), 1.67, \"RNN, LSTM\", \"https://arxiv.org/abs/1308.0850\")\n",
    "hp_compression.measure(date(2011,6,28), 1.60, \"RNN\", \"http://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Also consider adding the Microsoft Sentence Completion Challenge; see eg http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf table 7.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "translation = Problem(\"Translation between human langauges\", [\"agi\", \"language\"])\n",
    "# XXX BLEU scores may or may not need their own scale\n",
    "en_fr_bleu = translation.metric(\"news-test-2014 En-Fr BLEU\", url=\"http://aclweb.org/anthology/P/P02/P02-1040.pdf\", scale=correct_percent, target_label=\"Identical to professional human translations\", target=50)\n",
    "en_fr_bleu.measure(date(2014, 9, 1), 36.15, \"RNN-search50*\", url=\"https://arxiv.org/abs/1409.0473\")\n",
    "en_fr_bleu.measure(date(2014, 10, 30), 37.5, \"LSTM6 + PosUnk\", url=\"https://arxiv.org/abs/1410.8206\")\n",
    "en_fr_bleu.measure(date(2016, 9, 26), 39.92, \"GNMT+RL\", url=\"https://arxiv.org/abs/1609.08144\")\n",
    "en_fr_bleu.measure(date(2016, 7, 23), 39.2, \"Deep-Att + PosUnk\", url=\"https://arxiv.org/abs/1606.04199\")\n",
    "en_fr_bleu.measure(date(2017, 1, 23), 40.56, \"MoE 2048\", url=\"https://arxiv.org/pdf/1701.06538\")\n",
    "\n",
    "en_de_bleu = translation.metric(\"news-test-2014 En-De BLEU\", url=\"http://aclweb.org/anthology/P/P02/P02-1040.pdf\", scale=correct_percent, target=50)\n",
    "\n",
    "# XXX add more languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation\n",
    "\n",
    "Conversation is the classic AI progress measure! There is the Turing test, and also easier variants of the Turing test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "turing_test = Problem(\"Conduct arbitrary sustained, probing conversation\", [\"agi\", \"language\", \"world-modelling\", \"communication\"])\n",
    "easy_turing_test = Problem(\"Turing test for casual conversation\", [\"agi\", \"language\", \"world-modelling\", \"communication\"])\n",
    "turing_test.add_subproblem(easy_turing_test)\n",
    "\n",
    "loebner = easy_turing_test.metric(\"The Loebner Prize scored selection answers\", url=\"http://www.aisb.org.uk/events/loebner-prize\", \n",
    "                                  scale=correct_percent, changeable=True, target=100)\n",
    "# XXX humans probably don't get 100% on the Loebner Prize selection questions; we should ask the organizers to score\n",
    "# some humans\n",
    "\n",
    "loebner.notes = \"\"\"\n",
    "The Loebner Prize is an actual enactment of the Turing Test. Importantly, judges are instructed to engage in casual, natural\n",
    "conversation rather than deliberately probing to determine if participants are \"intelligent\" (Brian Christian, The Most Human Human).\n",
    "This makes it considerably easier than a probing Turing Test, and it is close to being solved. \n",
    "\n",
    "However these aren't scores for the full Loebner Turing Test; since 2014 the Loebner prize has scored its entrants by\n",
    "giving them a corpus of conversation and scoring their answers. We use these numbers because they remove variability\n",
    "in the behaviour of the judges. Unfortunately, these questions change from year to year (and have to, since \n",
    "entrants will test with last year's data).\n",
    "\"\"\"\n",
    "loebner.measure(date(2016,9,17), 90, \"Mitsuku 2016\", url=\"http://www.aisb.org.uk/events/loebner-prize#Results16\")\n",
    "loebner.measure(date(2016,9,17), 78.3, \"Tutor 2016\", url=\"http://www.aisb.org.uk/events/loebner-prize#Results16\")\n",
    "loebner.measure(date(2016,9,17), 77.5, \"Rose 2016\", url=\"http://www.aisb.org.uk/events/loebner-prize#Results16\")\n",
    "loebner.measure(date(2016,9,17), 77.5, \"Arckon 2016\", url=\"http://www.aisb.org.uk/events/loebner-prize#Results16\")\n",
    "loebner.measure(date(2016,9,17), 76.7, \"Katie 2016\", url=\"http://www.aisb.org.uk/events/loebner-prize#Results16\")\n",
    "\n",
    "loebner.measure(date(2015,9,19), 83.3, \"Mitsuku 2015\", url=\"http://www.aisb.org.uk/events/loebner-prize#Results15\")\n",
    "loebner.measure(date(2015,9,19), 80, \"Lisa 2015\", url=\"http://www.aisb.org.uk/events/loebner-prize#Results15\")\n",
    "loebner.measure(date(2015,9,19), 76.7, \"Izar 2015\", url=\"http://www.aisb.org.uk/events/loebner-prize#Results15\")\n",
    "loebner.measure(date(2015,9,19), 75, \"Rose 2015\",url=\"http://www.aisb.org.uk/events/loebner-prize#Results15\")\n",
    "\n",
    "loebner.measure(date(2014,11,15), 89.2, \"Rose 2014\", url=\"http://www.aisb.org.uk/events/loebner-prize#contest2014\")\n",
    "loebner.measure(date(2014,11,15), 88.3, \"Izar 2014\", url=\"http://www.aisb.org.uk/events/loebner-prize#contest2014\")\n",
    "loebner.measure(date(2014,11,15), 88.3, \"Misuku 2014\", url=\"http://www.aisb.org.uk/events/loebner-prize#contest2014\")\n",
    "loebner.measure(date(2014,11,15), 81.67, \"Uberbot 2014\", url=\"http://www.aisb.org.uk/events/loebner-prize#contest2014\")\n",
    "loebner.measure(date(2014,11,15), 80.83, \"Tutor 2014\", url=\"http://www.aisb.org.uk/events/loebner-prize#contest2014\")\n",
    "#loebner.measure(date(2014,11,15), 76.7, \"The Professor 2014\", url=\"http://www.aisb.org.uk/events/loebner-prize#contest2014\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2180,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reading_comprehension = Problem(\"Language comprehension and question-answering\", [\"language\", \"world-modelling\", \"agi\"])\n",
    "turing_test.add_subproblem(reading_comprehension)\n",
    "\n",
    "# Overview of Machine Reading Comprehension (MRC) datasets here:\n",
    "# http://eric-yuan.me/compare-popular-mrc-datasets/\n",
    "\n",
    "mctest160 = reading_comprehension.metric(\"Reading comprehension MCTest-160-all\", scale=correct_percent, url=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/MCTest_EMNLP2013.pdf\")\n",
    "mctest160.measure(date(2013, 10, 1), 69.16, \"SW+D+RTE\", url=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/MCTest_EMNLP2013.pdf\", papername=\"MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text\")\n",
    "mctest160.measure(date(2015, 7, 26), 75.27, \"Wang-et-al\", url=\"http://arxiv.org/abs/1603.08884\")\n",
    "mctest160.measure(date(2015, 7, 26), 73.27, \"Narasimhan-model3\", url=\"https://people.csail.mit.edu/regina/my_papers/MCDR15.pdf\", papername=\"Machine Comprehension with Discourse Relations\")\n",
    "mctest160.measure(date(2016, 3, 29), 74.58, \"Parallel-Hierarchical\", url=\"http://arxiv.org/abs/1603.08884\")\n",
    "\n",
    "mctest500 = reading_comprehension.metric(\"Reading comprehension MCTest-500-all\", scale=correct_percent, url=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/MCTest_EMNLP2013.pdf\")\n",
    "mctest500.measure(date(2013, 10, 1), 63.33, \"SW+D+RTE\", url=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/MCTest_EMNLP2013.pdf\", papername=\"MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text\")\n",
    "mctest500.measure(date(2015, 7, 26), 69.94, \"Wang-et-al\", url=\"http://arxiv.org/abs/1603.08884\")\n",
    "mctest500.measure(date(2015, 7, 26), 63.75, \"Narasimhan-model3\", url=\"https://people.csail.mit.edu/regina/my_papers/MCDR15.pdf\", papername=\"Machine Comprehension with Discourse Relations\")\n",
    "mctest500.measure(date(2015, 7, 26), 67.83, \"LSSVM\", url=\"https://pdfs.semanticscholar.org/f26e/088bc4659a9b7fce28b6604d26de779bcf93.pdf\", papername=\"Learning Answer-Entailing Structures for Machine Comprehension\")\n",
    "mctest500.measure(date(2016, 3, 29), 71.00, \"Parallel-Hierarchical\", url=\"http://arxiv.org/abs/1603.08884\")\n",
    "\n",
    "cbtest_ne = reading_comprehension.metric(\"bAbi Children's Book comprehension CBtest NE\", url=\"http://fb.ai/babi\", scale=correct_percent, target=81.6, target_source=\"https://arxiv.org/abs/1511.02301\")\n",
    "cbtest_cn = reading_comprehension.metric(\"bAbi Children's Book comprehension CBtest CN\", url=\"http://fb.ai/babi\", scale=correct_percent, target=81.6, target_source=\"https://arxiv.org/abs/1511.02301\")\n",
    "cnn = reading_comprehension.metric(\"CNN Comprehension test\", url=\"https://github.com/deepmind/rc-data/\", scale=correct_percent)\n",
    "daily_mail = reading_comprehension.metric(\"Daily Mail Comprehension test\", url=\"https://github.com/deepmind/rc-data/\", scale=correct_percent)\n",
    "\n",
    "cnn.measure(date(2015, 6, 10), 63.0, \"Attentive reader\", url=\"https://arxiv.org/abs/1506.03340\")\n",
    "cnn.measure(date(2015, 6, 10), 63.8, \"Impatient reader\", url=\"https://arxiv.org/abs/1506.03340\")\n",
    "daily_mail.measure(date(2015, 6, 10), 69.0, \"Attentive reader\", url=\"https://arxiv.org/abs/1506.03340\")\n",
    "daily_mail.measure(date(2015, 6, 10), 68.0, \"Impatient reader\", url=\"https://arxiv.org/abs/1506.03340\")\n",
    "\n",
    "cnn.measure(date(2016, 6, 7), 75.7, \"AIA\", url=\"https://arxiv.org/abs/1606.02245v1\")\n",
    "cbtest_ne.measure(date(2016, 6, 7), 72.0, \"AIA\", url=\"https://arxiv.org/abs/1606.02245v1\")\n",
    "cbtest_ne.measure(date(2016, 6, 7), 71.0, \"AIA\", url=\"https://arxiv.org/abs/1606.02245v1\")\n",
    "\n",
    "cnn.measure(date(2016, 11, 9), 76.1, \"AIA\", url=\"https://arxiv.org/abs/1606.02245v4\")\n",
    "\n",
    "cnn.measure(date(2016, 6, 7), 74.0, \"EpiReader\", url=\"https://arxiv.org/abs/1606.02270\")\n",
    "cbtest_ne.measure(date(2016, 6, 7), 69.7, \"EpiReader\", url=\"https://arxiv.org/abs/1606.02270\")\n",
    "cbtest_cn.measure(date(2016, 6, 7), 67.4, \"EpiReader\", url=\"https://arxiv.org/abs/1606.02270\")\n",
    "\n",
    "cbtest_cn.measure(date(2016, 6, 5), 69.4, \"GA reader\", url=\"https://arxiv.org/abs/1606.01549v1\")\n",
    "cbtest_ne.measure(date(2016, 6, 5), 71.9, \"GA reader\", url=\"https://arxiv.org/abs/1606.01549v1\")\n",
    "cnn.measure(date(2016, 6, 5), 77.4, \"GA reader\", url=\"https://arxiv.org/abs/1606.01549v1\")\n",
    "daily_mail.measure(date(2016, 6, 5), 78.1, \"GA reader\", url=\"https://arxiv.org/abs/1606.01549v1\")\n",
    "\n",
    "cnn.measure(None, 77.9, \"GA update L(w)\", url=\"https://arxiv.org/abs/1606.01549v2\")\n",
    "daily_mail.measure(None, 80.9, \"GA update L(w)\", url=\"https://arxiv.org/abs/1606.01549v2\")\n",
    "cbtest_ne.measure(None, 74.9, \"GA +feature, fix L(w)\", url=\"https://arxiv.org/abs/1606.01549v2\")\n",
    "cbtest_cn.measure(None, 70.7, \"GA +feature, fix L(w)\", url=\"https://arxiv.org/abs/1606.01549v2\")\n",
    "\n",
    "# Neural semantic encoders invented in https://arxiv.org/abs/1607.04315v1 and retrospectively applied to CBTest by other authors\n",
    "cbtest_ne.measure(date(2016, 12, 1), 73.2, \"NSE\", url=\"https://arxiv.org/abs/1606.01549v2\", algorithm_src_url=\"https://arxiv.org/abs/1607.04315\", min_date=date(2016,7,4))\n",
    "cbtest_cn.measure(date(2016, 12, 1), 71.9, \"NSE\", url=\"https://arxiv.org/abs/1606.01549v2\", algorithm_src_url=\"https://arxiv.org/abs/1607.04315\", min_date=date(2016,7,4))\n",
    "en_de_bleu.measure(date(2016, 7, 14), 17.93, \"NSE-NSE\", url=\"https://arxiv.org/abs/1607.04315v1\")\n",
    "\n",
    "\n",
    "cnn.measure(date(2016, 8, 4), 74.4, \"AoA reader\", url=\"https://arxiv.org/pdf/1607.04423\")\n",
    "cbtest_ne.measure(date(2016, 8, 4), 72.0, \"AoA reader\", url=\"https://arxiv.org/pdf/1607.04423\")\n",
    "cbtest_cn.measure(date(2016, 8, 4), 69.4, \"AoA reader\", url=\"https://arxiv.org/pdf/1607.04423\")\n",
    "\n",
    "cnn.measure(date(2016, 8, 8), 77.6, \"Attentive+relabling+ensemble\", url=\"https://arxiv.org/abs/1606.02858\")\n",
    "daily_mail.measure(date(2016, 8, 8), 79.2, \"Attentive+relabling+ensemble\", url=\"https://arxiv.org/abs/1606.02858\")\n",
    "\n",
    "\n",
    "squad_f1 = reading_comprehension.metric(\"Stanford Question Answering Dataset F1 test\", url=\"https://stanford-qa.com/\")\n",
    "squad_em = reading_comprehension.metric(\"Stanford Question Answering Dataset EM test\", url=\"https://stanford-qa.com/\")\n",
    "\n",
    "squad_f1.measure(date(2016, 11,7), 77.0, \"Match-LSTM+Ans-Ptr\", url=\"https://arxiv.org/pdf/1608.07905v2\")\n",
    "squad_em.measure(date(2016, 11,7), 67.9, \"Match-LSTM+Ans-Ptr\", url=\"https://arxiv.org/pdf/1608.07905v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bAbi = reading_comprehension.metric(\"bAbi 20 QA\", url=\"http://fb.ai/babi\", scale=correct_percent, target=99)\n",
    "bAbi.notes = \"\"\"\n",
    "A synthetic environment inspired by text adventures and SHRDLU, which enables generation\n",
    "of ground truths, describing sentences, and inferential questions. Includes:\n",
    "supporting facts, relations, yes/no questions, counting, lists/sets, negation, indefiniteness,\n",
    "conference, conjunction, time, basic deduction and induction, reasoning about position, size,\n",
    "path finding and motivation.\n",
    "\n",
    "Table 3 of https://arxiv.org/abs/1502.05698 actually breaks this down into 20 submeasures\n",
    "but initially we're lumping all of this together.\n",
    "\n",
    "Originally \"solving\" bABI was defined as 95% accuracy (or perhaps) 95% accuracy on all submeasures,\n",
    "but clearly humans and now algorithms are better than that.\n",
    "\n",
    "TODO: bAbi really needs to be decomposed into semi-supervised and unsupervised variants, and \n",
    "by amount of training data provided\n",
    "\"\"\"\n",
    "bAbi.measure(date(2015,2,19),  93.3, \"MemNN-AM+NG+NL\",  \"https://arxiv.org/abs/1502.05698\")\n",
    "bAbi.measure(date(2015,3,31),  93.4, \"MemN2N-PE+LS+RN\", \"https://arxiv.org/abs/1503.08895\")\n",
    "bAbi.measure(date(2015,6,24),  93.6, \"DMN\",             \"https://arxiv.org/abs/1506.07285\")\n",
    "bAbi.measure(date(2016,1,5),   96.2, \"DNC\",             \"https://www.gwern.net/docs/2016-graves.pdf\")\n",
    "bAbi.measure(date(2016,6,30),  97.2, \"DMN+\",            \"https://arxiv.org/abs/1607.00036\")\n",
    "bAbi.measure(date(2016,9,27),  97.1, \"SDNC\",            \"https://arxiv.org/abs/1606.04582v4\")\n",
    "bAbi.measure(date(2016,12,12), 99.5, \"EntNet\",          \"https://arxiv.org/abs/1612.03969\")\n",
    "bAbi.measure(date(2016,12,9),  99.7, \"QRN\",             \"https://arxiv.org/abs/1606.04582v4\")\n",
    "\n",
    "print \"How is bAbi solved?\", bAbi.solved\n",
    "# More papers:\n",
    "# https://www.aclweb.org/anthology/D/D13/D13-1020.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific and Technical capabilities\n",
    "\n",
    "Arguably reading and understanding scientific, technical, engineering and medical documents would be taxonomically related to general reading comprehension, but these technical tasks are probably much more difficult, and will certainly be solved with separate efforts. So we classify them separately for now. We also classify some of these problems as superintelligent, because only a tiny fraction of humans can read STEM papers, and only a miniscule fraction of humans are capable of reasonably comprehending STEM papers across a large range of fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "read_stem_papers = Problem(\"Read a scientific or technical paper, and comprehend its contents\", [\"language\", \"world-modelling\", \"super\"])\n",
    "\n",
    "# Getting some major results from an abstract, tables or conclusion is much easier than understanding the entire paper, its assumptions, robustness, support for its claims, etc\n",
    "extract_results = Problem(\"Extract major numerical results or progress claims from a STEM paper\", [\"language\", \"world-modelling\", \"agi\"])\n",
    "read_stem_papers.add_subproblem(extract_results)\n",
    "\n",
    "extract_results.metric(\"Automatically find new relevant ML results on arXiv\")\n",
    "extract_results.notes = \"\"\"\n",
    "This metric is the ability to automatically update the ipython Notebook you are reading by spotting results in pdfs uploaded to arxiv.org.\n",
    "Pull requests demonstrating solutions are welcome :)\n",
    "\"\"\"\n",
    "\n",
    "solve_technical_problems = Problem(\"Given an arbitrary technical problem, solve it as well as a typical professional in that field\", [\"language\", \"world-modelling\"])\n",
    "# This subset of technical problems is much easier; here we assume that a human / worldly problem has been reduced to something that can be\n",
    "# subjected to clear computational evaluation (\"is this purported proof of theorem X correct?\", \"does this circuit perform task Y efficiently?\"\n",
    "# \"will this airframe fly with reasonable characteristics?\")\n",
    "solve_constrained_technical_problems = Problem(\"Solve technical problems with clear constraints (proofs, circuit design, aerofoil design, etc)\")\n",
    "solve_technical_problems.add_subproblem(solve_constrained_technical_problems)\n",
    "solve_technical_problems.add_subproblem(read_stem_papers)\n",
    "\n",
    "# Note that this theorem proving problem (learning to prove theorems) is a little different from the pure search\n",
    "# through proof space that characterises the classic ATP field, though progress there may also be interesting\n",
    "theorem_proving = Problem(\"Given examples of proofs, find correct proofs of simple mathematical theorems\", [\"agi\", \"math\"])\n",
    "circuit_design = Problem(\"Given desired circuit characteristics, and many examples, design new circuits to spec\", [\"agi\", \"math\"])\n",
    "solve_constrained_technical_problems.add_subproblem(theorem_proving)\n",
    "solve_constrained_technical_problems.add_subproblem(circuit_design)\n",
    "\n",
    "# TODO: find well-defined metrics for some of these problems in the literature. Or create some!\n",
    "# Some relevant papers:\n",
    "# http://www.ise.bgu.ac.il/faculty/kalech/publications/ijcai13.pdf\n",
    "# https://www.researchgate.net/publication/2745078_Use_of_Automatically_Defined_Functions_and_ArchitectureAltering_Operations_in_Automated_Circuit_Synthesis_with_Genetic_Programming\n",
    "# https://link.springer.com/article/10.1007/s10817-014-9301-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalisation and Transfer Learning\n",
    "\n",
    "ML systems are making strong progress at solving specific problems with sufficient training data. But we know that humans are\n",
    "capable of _transfer learning_ -- applying things they've learned from one context, with appropriate variation, to another context.\n",
    "Humans are also very general; rather than just being taught to perform specific tasks, a single agent is able to do a very\n",
    "wide range of tasks, learning new things or not as required by the situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generalisation = Problem(\"Building systems that solve a wide range of diverse problems, rather than just specific ones\")\n",
    "generalisation.metric(\"Solve all other solved problems in this document, with a single system\", solved=False)\n",
    "\n",
    "transfer_learning = Problem(\"Transfer learning: apply relevant knowledge from a prior setting to a new slightly different one\")\n",
    "arcade_transfer = Problem(\"Transfer of learning within simple arcade game paradigms\")\n",
    "\n",
    "generalisation.add_subproblem(transfer_learning)\n",
    "transfer_learning.add_subproblem(arcade_transfer)\n",
    "\n",
    "# These will need to be specified a bit more clearly to be proper metrics, eg \"play galaga well having trained on Xenon 2\" or whatever\n",
    "# the literature has settled on\n",
    "# arcade_transfer.metric(\"Transfer learning of platform games\")\n",
    "# arcade_transfer.metric(\"Transfer learning of vertical shooter games\")\n",
    "# arcade_transfer.metric(\"Transfer from a few arcade games to all of them\")\n",
    "\n",
    "one_shot_learning = Problem(\"One shot learning: ingest important truths from a single example\", [\"agi\", \"world-modelling\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safety and Security Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Safety problems!\n",
    "\n",
    "adversarial_examples = Problem(\"Resistance to adversarial examples\", [\"safety\", \"agi\"], url=\"https://arxiv.org/abs/1312.6199\")\n",
    "\n",
    "adversarial_examples.notes = \"\"\"\n",
    "We know that humans have significant resistance to adversarial examples.  Although methods like camouflage sometimes\n",
    "work to fool us into thinking one thing is another, those\n",
    "\"\"\"\n",
    "\n",
    "# This section is essentially on teaching ML systems ethics and morality. Amodei et al call this \"scaleable supervision\".\n",
    "scalable_supervision = Problem(\"Scalable supervision of a learning system\", [\"safety\", \"agi\"])\n",
    "cirl = Problem(\"Cooperative inverse reinforcement learning of objective functions\", [\"safety\", \"agi\"])\n",
    "cirl.notes = \"This is tagged agi because most humans are able to learn ethics from their surrounding community\"\n",
    "# Co-operative inverse reinforcement learning might be equivalent to solving scalable supervision, or there might other subproblems here\n",
    "scalable_supervision.add_subproblem(cirl)\n",
    "\n",
    "safe_exploration = Problem(\"Safe exploration\", [\"safety\", \"agi\", \"world-modelling\"], url=\"https://arxiv.org/abs/1606.06565\")\n",
    "safe_exploration.notes = \"\"\"\n",
    "Sometimes, even doing something once is catastrophic. In such situations, how can an RL agent or some other AI system\n",
    "learn about the catastrophic consequences without even taking the action once? This is an ability that most humans acquire\n",
    "at some point between childhood and adolescence.\n",
    "\"\"\"\n",
    "# safe exploration may be related to one shot learning, though it's probably too early to mark that so clearly.\n",
    "\n",
    "avoiding_reward_hacking = Problem(\"Avoiding reward hacking\", [\"safety\"], url=\"https://arxiv.org/abs/1606.06565\")\n",
    "avoiding_reward_hacking.notes = \"\"\"\n",
    "Humans have only partial resistance to reward hacking.\n",
    "Addiction seems to be one failure to exhibit this resistance.\n",
    "Avoiding learning something because it might make us feel bad, or even building elaborate systems of self-deception, are also sometimes\n",
    "seen in humans. So this problem is not tagged \"agi\".\n",
    "\"\"\"\n",
    "\n",
    "robustness_to_distributional_change = Problem(\"Function correctly in novel environments (robustness to distributional change)\", [\"safety\", \"agi\"], url=\"https://arxiv.org/abs/1606.06565\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rodriguo Benenson's \"Are We There Yet?\" data!\n",
    "\n",
    "# Fetch the summary page\n",
    "awty_url = \"https://rodrigob.github.io/are_we_there_yet/build/\"\n",
    "req = requests.get(awty_url)\n",
    "page = req.content.replace(\"</html>\", \"\", 1) # There is a crazy weird </html> near the top of the page that breaks everything\n",
    "tree = lxml.html.fromstring(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2186,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from urlparse import urlparse\n",
    "\n",
    "awty_datasets = {}\n",
    "for e in CSSSelector('div.span7')(tree):\n",
    "    #print dir(e)\n",
    "    node = e.getchildren()[0].getchildren()[0]\n",
    "    link = node.attrib[\"href\"]\n",
    "    metric_name = node.text_content()\n",
    "    print \"%40s\" % metric_name, link\n",
    "    awty_datasets[metric_name] = urlparse(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2203,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    got = json.load(open(\".awyt_cache.json\"))\n",
    "except:\n",
    "    got = {}\n",
    "    \n",
    "\n",
    "# For lots of AWYT data, there's no date but there is a conference, so we can look up the dates that way.\n",
    "conf_dates = {}\n",
    "\n",
    "\n",
    "def parse_awyt_dataset(name, link, verbose=False):\n",
    "    print \"Handling\", repr(name), link.geturl()\n",
    "    if link.path not in got:\n",
    "        page = requests.get(awty_url + link.path).content\n",
    "        tree = lxml.html.fromstring(page.replace(\"</html>\", \"\", 1))\n",
    "        got[link.path] = tree\n",
    "    else:\n",
    "        tree = got[link.path]\n",
    "    #print dir(tree)\n",
    "    #print \"fragment:\", link.fragment\n",
    "    #print page\n",
    "    results_section = CSSSelector(\"div#\" + link.fragment)(tree)[0]\n",
    "    \n",
    "    rows = CSSSelector(\"tr\")(results_section)\n",
    "    results = []\n",
    "    for r in rows[1:]:\n",
    "        result, paperlink, journal, _ = CSSSelector(\"td\")(r)\n",
    "        result, papername, journal = [e.text_content() for e in (result, paperlink, journal)]\n",
    "        if \"%\" not in result:\n",
    "            print \"Skipping\", result, papername, journal\n",
    "            continue\n",
    "        if verbose: \n",
    "            print \"%6s\" % result, \"%90s\" % papername, \"%10s\" %journal\n",
    "        e = CSSSelector(\"a\")(paperlink)\n",
    "        paper_url = e[0].attrib[\"href\"] if e else None\n",
    "        results.append((result, papername, paper_url, journal))\n",
    "    return results\n",
    "\n",
    "for name, link in awty_datasets.items():\n",
    "    if not link.scheme:\n",
    "        parse_awyt_dataset(name, link)\n",
    "\n",
    "percent_re = re.compile(r'([0-9.]+) *%')\n",
    "\n",
    "def ingest_awty_dataset(name, metric, label):\n",
    "    m = 'CIFAR-10'\n",
    "    for result, papername, paper_url, journal in parse_awyt_dataset(name, awty_datasets[name]):\n",
    "        try:\n",
    "            value = float(percent_re.match(result).groups(0)[0])\n",
    "        except TypeError, AttributeError:\n",
    "            print \"result\", result, \"does not parse\"\n",
    "\n",
    "        print \"%s.measure%s\" % (label, ((None, value, papername, paper_url,  papername, journal),))\n",
    "        try:\n",
    "            metric.measure(None, value, papername, paper_url, papername=papername, venue=journal)\n",
    "        except requests.ConnectionError, e:\n",
    "            print \"Network error on {0} ({1}), skipping:\".format(paper_url, papername)\n",
    "            print e\n",
    "\n",
    "cifar100 = image_classification.metric(\"CIFAR-100\", \"http://https://www.cs.toronto.edu/~kriz/cifar.html\", scale=correct_percent)\n",
    "ingest_awty_dataset('CIFAR-100', cifar100, 'cifar100')\n",
    "cifar100.graph()\n",
    "\n",
    "cifar10 = image_classification.metric(\"CIFAR-10\", \"http://https://www.cs.toronto.edu/~kriz/cifar.html\", scale=correct_percent, target=94, target_source=\"http://karpathy.github.io/2011/04/27/manually-classifying-cifar10/\")\n",
    "ingest_awty_dataset('CIFAR-10', cifar10, 'cifar10')\n",
    "cifar10.graph()\n",
    "\n",
    "svhn = image_classification.metric(\"Street View House Numbers (SVHN)\", \"http://ufldl.stanford.edu/housenumbers/\", scale=error_percent, target=2.0, target_source=\"http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf\")\n",
    "ingest_awty_dataset('SVHN', svhn, 'svhn')\n",
    "svhn.graph()\n",
    "\n",
    "mnist = image_classification.metric(\"MNIST\", \"http://yann.lecun.com/exdb/mnist/\", scale=error_percent, target=0.2, target_source=\"http://people.idsia.ch/~juergen/superhumanpatternrecognition.html\")\n",
    "ingest_awty_dataset('MNIST', mnist, 'mnist')\n",
    "# This awty URL broken\n",
    "mnist.measure(date(2013,2,28), 0.52, 'COSFIRE', 'http://www.rug.nl/research/portal/files/2390194/2013IEEETPAMIAzzopardi.pdf', papername='Trainable COSFIRE Filters for Keypoint Detection and Pattern Recognition')\n",
    "# awty transcribes what's in this paper, but it seems to somehow have confused, wildly different units from everything else:\n",
    "for i, m in enumerate(mnist.measures):\n",
    "    if m.url == \"http://personal.ie.cuhk.edu.hk/~ccloy/files/aaai_2015_target_coding.pdf\":\n",
    "        del mnist.measures[i]\n",
    "mnist.graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2188,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def counts():\n",
    "    print \"Included thus far:\"\n",
    "    print \"=================================\"\n",
    "    print len(problems), \"problems\"\n",
    "    print len(metrics), \"metrics\", len([m for m in metrics.values() if m.solved]), \"solved\"\n",
    "    print len(measurements), \"measurements\"\n",
    "    print len([p for p in problems.values() if not p.metrics]), \"problems which do not yet have any metrics (either not in this notebook, or none in the open literature)\"\n",
    "    print \"=================================\\n\"\n",
    "    print \"Problems by Type:\"\n",
    "    print \"=================================\"\n",
    "\n",
    "    by_attr = {}\n",
    "    solved_by_attr = {}\n",
    "    for a in all_attributes:\n",
    "        print a, len([p for p in problems.values() if a in p.attributes]), \"solved:\", len([p for p in problems.values() if p.solved and a in p.attributes])\n",
    "    print \"=================================\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_problems():\n",
    "    for p in sorted(problems.values(), key=lambda x: x.attributes):\n",
    "        if not p.superproblems:\n",
    "            p.print_structure()\n",
    "            print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def venn_report():\n",
    "    print \"Sample of problems characterized thus far:\"\n",
    "    lang = set(p for p in problems.values() if \"language\" in p.attributes)\n",
    "    world = set(p for p in problems.values() if \"world-modelling\" in p.attributes)\n",
    "    vision = set(p for p in problems.values() if \"vision\" in p.attributes)\n",
    "\n",
    "    from matplotlib_venn import venn3\n",
    "    venn3((lang, world, vision), ('Language Problems', 'World-Modelling Problems', 'Vision Problems'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2191,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def graphs():\n",
    "    print \"Graphs of progress:\"\n",
    "    for name, metric in metrics.items():\n",
    "        print name, \"({0} measurements)\".format(len(metric.measures))\n",
    "        if len(metric.measures) > 2:\n",
    "            metric.graph()\n",
    "    plt.show()\n",
    "                \n",
    "graphs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2192,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## counts()\n",
    "print \"PROBLEMS AND METRICS\"\n",
    "venn_report()\n",
    "list_problems()\n",
    "learning_arbitrary_abstract_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for venue, n in sorted(conferences_wanted.items(), key=lambda x:x[1], reverse=True):\n",
    "    if n > 1: print venue, n"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}