{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A notebook for organising AI progress metrics\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "# We have the following structures:\n",
    "#\n",
    "# problem \n",
    "#     \\   \\\n",
    "#      \\   metrics  -  measures \n",
    "#       \\\n",
    "#        - subproblems\n",
    "#             \\\n",
    "#           metrics\n",
    "#              \\\n",
    "#             measures\n",
    "#\n",
    "# problems are tagged with attributes:\n",
    "# eg, vision, abstract-games, language, world-modelling, safety\n",
    "#     agi       -- most capable humans can do this, so AGIs can do this\n",
    "#     super     -- the very best humans can do this, or human organisations have solved this\n",
    "#     verysuper -- neither humans nor human orgs have solved this\n",
    "#\n",
    "# problems can have \"subproblems\", including simpler cases and preconditions\n",
    "\n",
    "class Problem:\n",
    "    def __init__(self, name, attributes=[], solved=False):\n",
    "        self.name = name\n",
    "        self.attributes = attributes\n",
    "        self.subproblems = []\n",
    "        self.superproblems = []\n",
    "        self.metrics = []\n",
    "        self.solved = solved\n",
    "        \n",
    "    def subproblem(self, other_problem):\n",
    "        self.superproblems.append(other_problem)\n",
    "        other_problem.subproblems.append(self)\n",
    "        \n",
    "    def metric(self, *args, **kwargs):\n",
    "        m = Metric(*args, **kwargs)\n",
    "        self.metrics.append(m)\n",
    "        return m\n",
    "\n",
    "\n",
    "# Different metrics and measurements for progress are made on very different types of scales\n",
    "# we have some helper functions to regularise these a little bit, so we can tell (for instance)\n",
    "# whether progress on some metric appears to be accelerating or decelerating.\n",
    "\n",
    "# Interface:\n",
    "#    improvement(score1, score2): retrns a consistent measure of how much better score2 is than score1\n",
    "#    pseudolinear(score): returns a modified version of score where we would expect vaguely linear progress\n",
    "\n",
    "class Linear:\n",
    "    def improvement(self, score1, score2):\n",
    "        return score2 - score1\n",
    "    def pseudolinear(self, score):\n",
    "        return score\n",
    "linear = Linear()\n",
    "\n",
    "class ELO:\n",
    "    def improvement(self, score1, score2):\n",
    "        \"\"\"\n",
    "        Normalise an ELO score\n",
    "        \n",
    "        An ELO increase of 400 improves your odds by 10x, so we could justify something like\n",
    "        return 10.0 ** ((score2 - score1)/400.)\n",
    "        However, it seems that at least for chess ELO progress has been roughly linear over\n",
    "        time, both for humans and computers (though with different coefficients). Perhaps this\n",
    "        tracks exponential increases in ability to search the game's state space, driven directly\n",
    "        by Moore's law on the computer side, and indirectly for humans by access to better training\n",
    "        tools and more profound libraries of past play.\n",
    "        \n",
    "        So for now let's treat this as linear? But ELO is not a chess-specific measure, and in other\n",
    "        contexts we may want to do exponentiation as documented above?\n",
    "        \"\"\"\n",
    "        return score2 - score1\n",
    "    def pseudolinear(self, score):\n",
    "        return score\n",
    "    \n",
    "elo = ELO()\n",
    "\n",
    "class ErrorRate:\n",
    "    \"\"\"Many labelling contests use these measures\"\"\"\n",
    "    def improvement(self, score1, score2):\n",
    "        # 0.5 / 0.25\n",
    "        return score1 / score2\n",
    "    def pseudolinear(self, score):\n",
    "        # The choice of base here is arbitrary. But since this is computer science, let's use base2!\n",
    "        from math import log\n",
    "        return log(score) / log(2.0)\n",
    "error_rate = ErrorRate()\n",
    "    \n",
    "class Metric:\n",
    "    def __init__(self, name, url=None, solved=False, notes=\"\", scale=linear):\n",
    "        self.name = name\n",
    "        self.measures = []\n",
    "        self.solved = solved\n",
    "        self.url = url\n",
    "        self.notes = notes\n",
    "        self.scale = scale\n",
    "        \n",
    "    def measure(self, *args, **kwargs):\n",
    "        m = Measurement(*args, **kwargs)\n",
    "        self.measures.append(m)\n",
    "        # Add logic for detection solutions at this point\n",
    "        return m\n",
    "\n",
    "\n",
    "class Measurement:\n",
    "    def __init__(self, date, value, name, url, uncertainty=0, minval=None, maxval=None):\n",
    "        self.date = date\n",
    "        self.value = value\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.minval = minval if minval else value - uncertainty\n",
    "        self.maxval = maxval if maxval else value + uncertainty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BEGIN ACTUALLY CLASSIFYING PROBLEMS\n",
    "\n",
    "scene_description = Problem(\"Scene description\", [\"agi\", \"vision\", \"language\", \"world-modelling\"])\n",
    "image_classification = Problem(\"Image classification\", [\"vision\", \"agi\"])\n",
    "scene_description.subproblem(image_classification)\n",
    "\n",
    "imagenet = image_classification.metric(\"imagenet\", \"http://image-net.org\", scale=error_rate)\n",
    "imagenet.notes = \"\"\"\n",
    "Correctly label images from the Imagenet dataset. As of 2016, this includes:\n",
    " - Object localization for 1000 categories.\n",
    " - Object detection for 200 fully labeled categories.\n",
    " - Object detection from video for 30 fully labeled categories.\n",
    " - Scene classification for 365 scene categories (Joint with MIT Places team) on Places2 Database http://places2.csail.mit.edu.\n",
    " - Scene parsing for 150 stuff and discrete object categories (Joint with MIT Places team).\n",
    "WARNING: these subchallenges were added in successive years of the Imagenet challenge, so results from years are not directly\n",
    "comparable; however progress should probably be understated by comparing them?\n",
    "\"\"\"\n",
    "\n",
    "# Data points gathered by Jack Clark:\n",
    "imagenet.measure(date(2010,8,31), 0.28191, \"NEC UIUC\", \"http://image-net.org/challenges/LSVRC/2010/results\")\n",
    "\"\"\"\n",
    "** 2010: 0.28191**\n",
    "**NEC UIUC**\n",
    "http://image-net.org/challenges/LSVRC/2010/results\n",
    "\n",
    "** 2011: 0.25770\n",
    " XRCE**\n",
    "\n",
    "** 2012: 0.16422**\n",
    "** Supervision**\n",
    "http://image-net.org/challenges/LSVRC/2012/results.html\n",
    "\n",
    "** 2013: 0.11743 **\n",
    "**Clarifai**\n",
    "http://www.image-net.org/challenges/LSVRC/2013/results.php\n",
    "\n",
    "** 2014: 0.07405**\n",
    "**VGG**\n",
    "http://image-net.org/challenges/LSVRC/2014/index\n",
    " \n",
    "\n",
    "**2015: 0.03567**\n",
    "**MSRA**\n",
    "http://image-net.org/challenges/LSVRC/2015/results\n",
    "\n",
    "** 2016: 0.02991**\n",
    "**Trimps-Soushen**\n",
    "http://image-net.org/challenges/LSVRC/2016/results\n",
    "* * *\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Abstract games like chess, go, checkers etc can be played with no knowldege of the human world\n",
    "# Although this domain has largely been solved to super-human performance levels, there are a\n",
    "# few ends that need to be completed in terms of having agents learn rules for arbitrary \n",
    "# abstract games effectively\n",
    "\n",
    "abstract_strategy_games = Problem(\"Abstract strategy games\", [\"agi\", \"abstract-games\"])\n",
    "\n",
    "playing_with_hints = Problem(\"Playing abstract games with extensive hints\", [\"abstract-games\"], solved=True)\n",
    "playing_with_hints.notes = \"\"\"\n",
    "  Complex abstract strategy games have been solved to super-human levels\n",
    "  by computer systems with extensive rule-hinting and heuristics,\n",
    "  in some cases combined with machine learning techniques.\n",
    "\"\"\"\n",
    "computer_chess = playing_with_hints.metric(\"computer chess\", scale=elo)\n",
    "# For some caveats, see https://en.wikipedia.org/w/index.php?title=Chess_engine&oldid=764341963#Ratings\n",
    "computer_chess.measure(date(2017,02,27), 3393, \"Stockfish\", uncertainty=50,\n",
    "                           url=\"https://web.archive.org/web/20170227044521/http://www.computerchess.org.uk/ccrl/4040/\")\n",
    "computer_chess.measure(date(1997,05,11), 2725, \"Deep Blue\", uncertainty=25,\n",
    "                           url=\"https://www.quora.com/What-was-Deep-Blues-Elo-rating\")\n",
    "\n",
    "mastering_historical_games = Problem(\"Mastering human abstract strategy games\", [\"super\", \"abstract-games\"])\n",
    "mastering_chess = mastering_historical_games.metric(\"mastering chess\")\n",
    "mastering_chess.notes = \"\"\"\n",
    "  Beating all humans at chess, given a corpus of past play amongst masters,\n",
    "  but no human-crafted policy constraints and heuristics. This will probably fall out\n",
    "  immediately once learning_abstract_game_rules is solved, since playing_with_hints\n",
    "  has been solved.\n",
    "\"\"\"\n",
    "\n",
    "# Are there any published metrics for these yet?\n",
    "learning_abstract_game_rules = Problem(\"Learning the rules of complex strategy games from examples\", [\"agi\", \"abstract-games\"])\n",
    "learning_chess = learning_abstract_game_rules.metric(\"learning chess\")\n",
    "learning_chess.notes = \"\"\"\n",
    "  Chess software contains hard-coded policy constraints for valid play; this metric is whether RL\n",
    "  or other agents can correctly build those policy constraints from examples or oracles\"\"\"\n",
    "learning_go = learning_abstract_game_rules.metric(\"learning go\")\n",
    "learning_go.notes = \"\"\"\n",
    "  Go software contains policy constraints for valid play and evaluating the number of\n",
    "  liberties for groups. This metric is whether RL or other agents can correctly build those \n",
    "  policy constraints from examples or oracles\"\"\"\n",
    "learning_arbitrary_abstract_games = Problem(\"Play an arbitrary abstract game, first learning the rules\", [\"agi\", \"abstract-games\"])\n",
    "                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
