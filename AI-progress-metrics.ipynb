{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring the Progress of AI Research\n",
    "\n",
    "This pilot project collects problems and metrics/datasets from the AI research literature, and tracks progress on them. \n",
    "\n",
    "You can use this [Notebook](https://jupyter.org/#about-notebook) to see how things are progressing in specific subfields or AI/ML as a whole, as a place to report new results you've obtained, as a place to look for problems that might benefit from having new datasets/metrics designed for them, or as a source to build on for data science projects.\n",
    "\n",
    "At EFF, we're ultimately most interested in how this data can influence our understanding of the likely implications of AI. To begin with, we're [focused on gathering it](https://www.eff.org/deeplinks/2017/06/help-eff-track-progress-ai-and-machine-learning).\n",
    "\n",
    "<a name=\"sources\"></a>\n",
    "_Original authors: [Peter Eckersley](https://www.eff.org/about/staff/peter-eckersley) and [Yomna Nasser](https://ynasser.github.io/) at EFF_. Contact: [ai-metrics@eff.org](mailto:ai-metrics@eff.org).<br>\n",
    "\n",
    "With contributions from: [Gennie Gebhart](https://www.eff.org/about/staff/gennie-gebhart) and [Owain Evans](https://www.fhi.ox.ac.uk/team/owain-evans/)<br>\n",
    "\n",
    "Inspired by and merging data from:\n",
    "\n",
    "* Rodrigo Benenson's [\"Who is the Best at X / Are we there yet?\"](https://rodrigob.github.io/are_we_there_yet/build/#about) collating machine vision datasets & progress\n",
    "* Jack Clark and Miles Brundage's [collection of AI progress measurements](https://raw.githubusercontent.com/AI-metrics/master_text/master/archive/AI-metrics-data.txt)\n",
    "* Sarah Constantin's [Performance Trends in AI](https://srconstantin.wordpress.com/2017/01/28/performance-trends-in-ai/)\n",
    "* Katja Grace's [Algorithmic Progress in Six Domains](https://intelligence.org/files/AlgorithmicProgress.pdf)\n",
    "* The Swedish Computer Chess Association's [History of Computer Chess performance](https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders)\n",
    "* Qi Wu _et al._'s [Visual Question Answering: A survey of Methods and Datasets](https://arxiv.org/abs/1607.05910)\n",
    "* Eric Yuan's [Comparison of Machine Reading Comprehension Datasets](http://eric-yuan.me/compare-popular-mrc-datasets/)\n",
    "\n",
    "Thanks to many others for valuable conversations, suggestions and corrections, including: Dario Amodei, Miles Brundage, Breandan Considine, Owen Cotton-Barrett, Eric Drexler, Ottavio Good, Katja Grace, Anselm Levskaya, Clare Lyle, Toby Ord, Michael Page, Anders Sandberg, Daisy Stanton, Gabriel Synnaeve, Stacey Svetlichnaya, Helen Toner, and Jason Weston. EFF's work on this project has been supported by the [Open Philanthropy Project](http://www.openphilanthropy.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Taxonomy](#Taxonomy)\n",
    "2. [Source code](#Source-Code) for defining and importing data\n",
    "3. [Problems, Metrics and Datasets](#Problems,-Metrics,-and-Datasets)\n",
    "    4. [Game Playing](#Game-Playing)\n",
    "        5. [Abstract Strategy Games](#Abstract-Strategy-Games)\n",
    "        6. [Real-time Video Games](#Real-time-video-games)\n",
    "    7. [Vision and image modelling](#Vision)\n",
    "        9. [Image recognition](#Vision)\n",
    "        8. [Visual Question Answering](#Visual-Question-Answering)\n",
    "        10. [Video recognition](#Vision)\n",
    "        11. [Generating images](#Image-Generation)\n",
    "    7. Written Language\n",
    "        8. [Reading Comprehension](#Reading-Comprehension)\n",
    "        9. [Language Modelling](#Language-Modelling-and-Comprehension)\n",
    "        10. [Conversation](#Conversation:-Chatbots-&-Conversational-Agents)\n",
    "        11. [Translation](#Translation)\n",
    "    8. Spoken Language\n",
    "        12. [Speech recognition](#Speech-recognition)\n",
    "    9. [Scientific and Technical Capabilities](#Scientific-and-Technical-capabilities)\n",
    "        10. [Solving constrained, well-specified technical problems](#Scientific-and-Technical-capabilities)\n",
    "        10. [Reading technical papers](#Scientific-and-Technical-capabilities)\n",
    "        10. [Solving real-world technical problems](#Scientific-and-Technical-capabilities)\n",
    "        10. [Generating computer programs from specifications](#Generating-computer-programs-from-specifications)\n",
    "    8. Learning to Learn Better\n",
    "        7. [Generalization](#Generalisation-and-Transfer-Learning)\n",
    "        7. [Transfer Learning](#Generalisation-and-Transfer-Learning)\n",
    "        7. [One-shot Learning](#Generalisation-and-Transfer-Learning)\n",
    "    8. [Safety and Security](#Safety-and-Security-Problems)\n",
    "        9. [\"Adversarial Examples\" and Manipulation of Classifiers](#\"Adversarial-Examples\"-and-manipulation-of-ML-classifiers)\n",
    "        9. [Safety for Reinforcement Learning Agents](#Safety-of-Reinforcement-Learning-Agents-and-similar-systems)\n",
    "        9. [Automated Hacking Systems](#Automated-Hacking-Systems)\n",
    "        9. [Pedestrian Detection for self-driving vehicles](#Pedestrian-Detection)\n",
    "    8. [Transparency, Explainability & Interpretability](#Explainability-and-Interpretability)\n",
    "    8. [Fairness and Debiasing](#Fairness-and-Debiasing)\n",
    "    8. [Privacy Problems](#Privacy)\n",
    "12. [Taxonomy and recorded progress to date](#Taxonomy-and-recorded-progress-to-date)\n",
    "    13. [Breakdown of Problems and Metrics by Type/Category](#Problems-and-Metrics-by-category)\n",
    "    \n",
    "14. [How to contribute to this project](#How-to-contribute-to-this-notebook)\n",
    "    15. [Notes on importing data](#Notes-on-importing-data)\n",
    "    15. [Exporting / building on this data](#Building-on-this-data)\n",
    "    15. [License](#License)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxonomy\n",
    "\n",
    "It collates data with the following structure:\n",
    "\n",
    "```\n",
    "problem \n",
    "    \\   \\\n",
    "     \\   metrics  -  measures \n",
    "      \\\n",
    "       - subproblems\n",
    "            \\\n",
    "          metrics\n",
    "             \\\n",
    "            measure[ment]s\n",
    "```\n",
    "\n",
    "Problems describe the ability to learn an important category of task.\n",
    "\n",
    "Metrics should ideally be formulated in the form \"software is able to learn to do X given training data of type Y\". In some cases X is the interesting part, but sometimes also Y.\n",
    "\n",
    "Measurements are the score that a specific instance of a specific algorithm was able to get on a Metric.\n",
    "\n",
    "problems are tagged with attributes:\n",
    "eg, vision, abstract-games, language, world-modelling, safety\n",
    "\n",
    "Some of these are about performance relative to humans (which is of course a very arbitrary standard, but one we're familiar with)\n",
    "* agi -- most capable humans can do this, so AGIs can do this (note it's conceivable that an agent might pass the Turing test before all of these are won)\n",
    "* super -- the very best humans can do this, or human organisations can do this\n",
    "* verysuper -- neither humans nor human orgs can presently do this\n",
    "\n",
    "problems can have \"subproblems\", including simpler cases and preconditions for solving the problem in general\n",
    "\n",
    "a \"metric\" is one way of measuring progress on a problem, commonly associated with a test dataset. There will often be several metrics\n",
    "for a given problem, but in some cases we'll start out with zero metrics and will need to start proposing some...\n",
    "\n",
    "a measure[ment] is a score on a given metric, by a particular codebase/team/project, at a particular time\n",
    "\n",
    "The present state of the actual taxonomy is [at the bottom of this notebook](#Taxonomy-and-recorded-progress-to-date)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''\n",
    "<script>\n",
    "    if (typeof code_show == \"undefined\") {\n",
    "        code_show=false;\n",
    "    } else {\n",
    "        code_show = !code_show; // FIXME hack, because we toggle on load :/\n",
    "    }\n",
    "    function toggle_one(mouse_event) {\n",
    "        console.log(\"Unhiding \"+button + document.getElementById(button.region));\n",
    "        parent = button.parentNode;\n",
    "        console.log(\"Parent\" + parent)\n",
    "        input = parent.querySelector(\".input\");\n",
    "        console.log(\"Input\" + input + \" \" + input.classList + \" \" + input.style.display)\n",
    "        input.style.display = \"block\";\n",
    "        //$(input).show();\n",
    "    }\n",
    "    function code_toggle() {\n",
    "        if (!code_show) {\n",
    "            inputs = $('div.input');\n",
    "            for (n = 0; n < inputs.length; n++) {\n",
    "                if (inputs[n].innerHTML.match('# hidd' + 'encode'))\n",
    "                    inputs[n].style.display = \"none\";\n",
    "                    button = document.createElement(\"button\");\n",
    "                    button.innerHTML=\"unhide code\";\n",
    "                    button.style.width = \"100px\";\n",
    "                    button.style.marginLeft = \"90px\";\n",
    "                    button.addEventListener(\"click\", toggle_one);\n",
    "                    button.classList.add(\"cell-specific-unhide\")\n",
    "                    // inputs[n].parentNode.appendChild(button);\n",
    "            }\n",
    "        } else { \n",
    "            $('div.input').show();\n",
    "            $('button.cell-specific-unhide').remove()\n",
    "        } \n",
    "        code_show = !code_show;\n",
    "    } \n",
    "    \n",
    "    $( document ).ready(code_toggle);\n",
    "    \n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\">\n",
    "    <input type=\"submit\" value=\"Click here to show/hide source code cells.\"> <br><br>(you can mark a cell as code with <tt># hiddencode</tt>)\n",
    "</form>\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# hiddencode\n",
    "\n",
    "%matplotlib inline  \n",
    "import matplotlib as mpl\n",
    "#mpl.use(\"nbAgg\")\n",
    "#mpl.use('svg')\n",
    "\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from matplotlib import markers\n",
    "\n",
    "try:\n",
    "    from lxml.cssselect import CSSSelector\n",
    "except ImportError:\n",
    "    # terrifying magic for Azure Notebooks\n",
    "    import os\n",
    "    if os.getcwd() == \"/home/nbuser\":\n",
    "        !pip install cssselect\n",
    "        from lxml.cssselect import CSSSelector\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "import lxml.html\n",
    "import numpy\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except ImportError:\n",
    "    print \"Seaborn style not installed\"\n",
    "\n",
    "date = datetime.date\n",
    "problems = {}\n",
    "metrics = {}\n",
    "measurements = set() # we don't try to guarantee unique names for these, so use a set\n",
    "all_attributes = set()\n",
    "\n",
    "class Problem:\n",
    "    def __init__(self, name, attributes=[], solved=False, url=None):\n",
    "        self.name = name\n",
    "        self.attributes = attributes\n",
    "        for a in attributes:\n",
    "            global all_attributes\n",
    "            all_attributes.add(a)\n",
    "        self.subproblems = []\n",
    "        self.superproblems = []\n",
    "        self.metrics = []\n",
    "        self.solved = solved\n",
    "        self.url = url\n",
    "        global problems, metrics\n",
    "        problems[name] = self\n",
    "        \n",
    "    def add_subproblem(self, other_problem):\n",
    "        # add this other problem as a subproblem of us\n",
    "        other_problem.superproblems.append(self)\n",
    "        self.subproblems.append(other_problem)\n",
    "        \n",
    "    def metric(self, *args, **kwargs):\n",
    "        m = Metric(*args, **kwargs)\n",
    "        m.parent = self\n",
    "        self.metrics.append(m)\n",
    "        return m\n",
    "    \n",
    "    def check_solved(self):\n",
    "        if all(m.solved for m in self.metrics + self.subproblems):\n",
    "            self.solved = True\n",
    "            for p in self.superproblems:\n",
    "                p.check_solved()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Problem({0})\".format(self.name)\n",
    "    \n",
    "    def print_structure(self, indent=0):\n",
    "        print indent * \" \" + str(self)\n",
    "        for m in self.metrics:\n",
    "            print (indent + 4) * \" \" + str(m)\n",
    "        for p in self.subproblems:\n",
    "            p.print_structure(indent + 4)\n",
    "\n",
    "            \n",
    "# Different metrics and measurements for progress are made on very different types of scales\n",
    "# we have some helper functions to regularise these a little bit, so we can tell (for instance)\n",
    "# whether progress on some metric appears to be accelerating or decelerating.\n",
    "\n",
    "# Interface:\n",
    "#    improvement(score1, score2): retrns a consistent measure of how much better score2 is than score1\n",
    "#    pseudolinear(score): returns a modified version of score where we would expect vaguely linear progress\n",
    "\n",
    "class Linear():\n",
    "    offset = (2,-2)\n",
    "    axis_label = \"Score\"\n",
    "    def improvement(self, score1, score2):\n",
    "        return score2 - score1\n",
    "    def pseudolinear(self, score):\n",
    "        return score\n",
    "\n",
    "class AtariLinear():\n",
    "    offset = (2,-2)\n",
    "    axis_label = \"Score (Rewards Normalized)\"\n",
    "    def improvement(self, score1, score2):\n",
    "        return score2 - score1\n",
    "    def pseudolinear(self, score):\n",
    "        return score    \n",
    "    \n",
    "linear = Linear()\n",
    "score = Linear()\n",
    "atari_linear = AtariLinear()\n",
    "\n",
    "class ELO:\n",
    "    offset = (2,-2)\n",
    "    axis_label = \"ELO rating\"\n",
    "    def improvement(self, score1, score2):\n",
    "        \"\"\"\n",
    "        Normalise an ELO score\n",
    "        \n",
    "        An ELO increase of 400 improves your odds by 10x, so we could justify something like\n",
    "        return 10.0 ** ((score2 - score1)/400.)\n",
    "        However, it seems that at least for chess ELO progress has been roughly linear over\n",
    "        time, both for humans and computers (though with different coefficients). Perhaps this\n",
    "        tracks exponential increases in ability to search the game's state space, driven directly\n",
    "        by Moore's law on the computer side, and indirectly for humans by access to better training\n",
    "        tools and more profound libraries of past play.\n",
    "        \n",
    "        So for now let's treat this as linear? But ELO is not a chess-specific measure, and in other\n",
    "        contexts we may want to do exponentiation as documented above?\n",
    "        \"\"\"\n",
    "        return score2 - score1\n",
    "    def pseudolinear(self, score):\n",
    "        return score\n",
    "    \n",
    "elo = ELO()\n",
    "\n",
    "class ErrorRate:\n",
    "    \"\"\"Many labelling contests use these measures\"\"\"\n",
    "    offset = (2,2)\n",
    "    axis_label = \"Error rate\"\n",
    "    def improvement(self, score1, score2):\n",
    "        # improvement is measured as a negative log of the error rate\n",
    "        return log(score1) - log(score2)\n",
    "    def pseudolinear(self, score):\n",
    "        # error rate 1 => 0\n",
    "        # error rate 0 => infinity\n",
    "        return -log(score)\n",
    "error_rate = ErrorRate()\n",
    "\n",
    "# some problems have performance measured in bits per X (bits per character, bits per pixel, etc), \n",
    "# reflecting the amount of information necessary for a model to accurately encode something from a corpus.\n",
    "# Lower is better and zero is infinitely good, so we can re-use the error rate math for now (though\n",
    "# scores above 1 are possible)\n",
    "bits_per_x = ErrorRate()\n",
    "bits_per_x.axis_label = \"Model Entropy\"\n",
    "# perplexity is 2 to the bits_per_x\n",
    "perplexity = ErrorRate()\n",
    "perplexity.axis_label = \"Perplexity\"\n",
    "\n",
    "class CorrectPercent:\n",
    "    \"100 - error rate\"\n",
    "    offset = (3,-6)\n",
    "    axis_label = \"Percentage correct\"\n",
    "    def erate(self, score):\n",
    "        return (100. - score)/100.\n",
    "\n",
    "    def improvement(self, score1, score2):\n",
    "        return score2 - score1\n",
    "    \n",
    "    def pseudolinear(self, score):\n",
    "        from math import log\n",
    "        return -log(self.erate(score))\n",
    "\n",
    "correct_percent = CorrectPercent()\n",
    "\n",
    "class BLEUScore:\n",
    "    \"50 is a perfect BLEU score, meaning a system produces exact matches to professional human translations\"\n",
    "    offset = (3,-6)\n",
    "    axis_label = \"BLEU score\"\n",
    "    def erate(self, score):\n",
    "        return (50. - score)/50.\n",
    "\n",
    "    def improvement(self, score1, score2):\n",
    "        return score2 - score1\n",
    "    \n",
    "    def pseudolinear(self, score):\n",
    "        from math import log\n",
    "        return -log(self.erate(score))\n",
    "\n",
    "bleu_score = BLEUScore()\n",
    "\n",
    "class ErrorPercent:\n",
    "    \"100 * error rate\"\n",
    "    offset = (3,-6)\n",
    "    axis_label = \"Percentage error\"\n",
    "    def erate(self, score):\n",
    "        return score/100.\n",
    "\n",
    "    def improvement(self, score1, score2):\n",
    "        return score1 - score2\n",
    "    \n",
    "    def pseudolinear(self, score):\n",
    "        from math import log\n",
    "        return log(self.erate(score))\n",
    "    \n",
    "error_percent = ErrorPercent()\n",
    "\n",
    "mpl.rcParams[\"legend.fontsize\"] = u\"x-small\"\n",
    "mpl.rcParams[\"xtick.labelsize\"] = u\"xx-small\"\n",
    "mpl.rcParams[\"ytick.labelsize\"] = u\"x-small\"\n",
    "\n",
    "class Metric:\n",
    "    def __init__(self, name, url=None, solved=False, notes=\"\", scale=linear, target=None, target_source=None,\n",
    "                 parent=None, changeable=False, axis_label=None, target_label=None):\n",
    "        self.name = name\n",
    "        self.measures = []\n",
    "        self.solved = solved\n",
    "        self.url = url\n",
    "        self.notes = notes\n",
    "        self.scale = scale\n",
    "        self.target = target\n",
    "        self.target_source = target_source # Source for human-level performance number\n",
    "        self.changeable = changeable # True if a metric changes over time\n",
    "        self.graphed = False\n",
    "        global metrics\n",
    "        metrics[name] = self\n",
    "        self.parent = parent\n",
    "        self.target_label = target_label\n",
    "        self.axis_label = (     axis_label            if axis_label \n",
    "                           else self.scale.axis_label if hasattr(self.scale, \"axis_label\") \n",
    "                           else self.name)\n",
    "\n",
    "        \n",
    "    def __str__(self):\n",
    "        solved = \"SOLVED\" if self.solved else \"?\" if not self.target else \"not solved\"\n",
    "        return \"{0:<60}{1}\".format(\"Metric(%s)\" % self.name, solved)\n",
    "        \n",
    "    def measure(self, *args, **kwargs):\n",
    "        try:\n",
    "            m = Measurement(*args, **kwargs)\n",
    "        except AssertionError:\n",
    "            print \"WARNING, failed to create measurement\", args, kwargs\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "        m.metric = self\n",
    "        if self.target:\n",
    "            if self.target_source == m.url and self.target == m.value:\n",
    "                print \"Skipping apparent human performance (target_source) paper\", m.url\n",
    "                return None\n",
    "            if self.scale.improvement(self.target, m.value) >= 0:\n",
    "                self.solved = True\n",
    "                self.parent.check_solved()\n",
    "        self.measures.append(m)\n",
    "        return m\n",
    "    \n",
    "    def graph(self, size=(7,5), scale=1.0):\n",
    "        if len(self.measures) < 2:\n",
    "            return\n",
    "        fig = plt.figure(dpi=300)\n",
    "        fig.set_size_inches((7*scale, 5*scale))\n",
    "        #fig.add_subplot(111).set_ylabel(self.name)\n",
    "        subplot = fig.add_subplot(111)\n",
    "        subplot.set_ylabel(self.axis_label)\n",
    "        subplot.set_title(self.name)\n",
    "        #fig.add_subplot(111).set_ylabel(self.name)\n",
    "        \n",
    "        self.measures.sort(key=lambda m: (m.date, m.metric.scale.pseudolinear(m.value)))\n",
    "        \n",
    "        # scatter plot of results in the literature\n",
    "        available_markers = markers.MarkerStyle().markers\n",
    "        for n, m in enumerate(self.measures):\n",
    "            kwargs = {\"c\": \"r\"}\n",
    "            if self.target and self.scale.improvement(self.target, m.value) >= 0:\n",
    "                kwargs[\"c\"] = \"b\"\n",
    "            if m.not_directly_comparable or self.changeable:\n",
    "                kwargs[\"c\"] = \"#000000\"\n",
    "                if \"*\" in available_markers:\n",
    "                    kwargs[\"marker\"] = \"*\"\n",
    "            if m.withdrawn:\n",
    "                if \"X\" in available_markers:\n",
    "                    kwargs[\"marker\"] = \"X\"\n",
    "                kwargs[\"c\"] = \"#aaaaaa\"\n",
    "            plt.plot_date([m.date], [m.value], **kwargs)\n",
    "            \n",
    "            label = m.name\n",
    "            if m.withdrawn and not \"withdrawn\" in label.lower():\n",
    "                label = \"WITHDRAWN \" + label\n",
    "            if len(label) >= 28 and not m.long_label:\n",
    "                label = label[:25] + \"...\"\n",
    "                \n",
    "            plt.annotate('%s' % label, xy=(m.date, m.value), xytext=m.metric.scale.offset, fontsize=scale * 6, textcoords='offset points')\n",
    "            # cases where either results or dates of publication are uncertain\n",
    "            kwargs = {\"c\": \"#80cf80\", \"linewidth\": scale*1.0, \"capsize\": scale*1.5, \"capthick\": scale*0.5, \"dash_capstyle\": 'projecting'}\n",
    "                \n",
    "            if m.min_date or m.max_date:\n",
    "                before = (m.date - m.min_date) if m.min_date else datetime.timedelta(0)\n",
    "                after = (m.max_date - m.date) if m.max_date else datetime.timedelta(0)\n",
    "                kwargs[\"xerr\"] = numpy.array([[before], [after]])\n",
    "            if self.measures[n].value != self.measures[n].minval:\n",
    "                kwargs[\"yerr\"] = numpy.array([[m.value - self.measures[n].minval], [self.measures[n].maxval - m.value]])\n",
    "            if \"xerr\" in kwargs or \"yerr\" in kwargs:\n",
    "                subplot.errorbar(m.date, m.value, **kwargs)\n",
    "        \n",
    "        # line graph of the frontier of best results\n",
    "        if not self.changeable:\n",
    "            best = self.measures[0].value\n",
    "            frontier_x, frontier_y = [], []\n",
    "            for m in self.measures:\n",
    "                if self.scale.improvement(best, m.value) >= 0 and not m.withdrawn and not m.not_directly_comparable:\n",
    "                    frontier_x.append(m.date)\n",
    "                    frontier_y.append(m.value)\n",
    "                    xy = (m.date, m.value)       \n",
    "                    best = m.value\n",
    "            plt.plot_date(frontier_x, frontier_y, \"g-\")\n",
    "        \n",
    "        # dashed line for \"solved\" / strong human performance\n",
    "        if self.target:\n",
    "            target_label = (       self.target_label  if self.target_label\n",
    "                             else \"Human performance\" if self.parent and \"agi\" in self.parent.attributes\n",
    "                             else \"Target\")\n",
    "            start = min([self.measures[0].date]  + [m.min_date for m in self.measures if m.min_date])\n",
    "            end =   max([self.measures[-1].date] + [m.max_date for m in self.measures if m.max_date])\n",
    "\n",
    "            plt.plot_date([start, end], 2 * [self.target], \"r--\", label=target_label)\n",
    "            \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        self.graphed = True\n",
    "\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hiddencode\n",
    "def canonicalise(url):\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    if url.startswith(\"http://arxiv.org\"):\n",
    "        url = url.replace(\"http\", \"https\")\n",
    "    if url.startswith(\"https://arxiv.org/pdf/\"):\n",
    "        url = url.replace(\"pdf\", \"abs\", 1)\n",
    "        url = url.replace(\".pdf\", \"\", 1)\n",
    "    return url\n",
    "# dates of conferences help us date papers from the \"Are We There Yet\" dataset\n",
    "conference_dates = {\"ICML 2016\": date(2016, 6, 19),\n",
    "                    \"NIPS 2015\": date(2015, 12, 7),\n",
    "                    \"ICLR 2014\": date(2014, 4, 14),\n",
    "                    \"ICML 2013\": date(2013, 6, 16),\n",
    "                    \"CVPR 2012\": date(2012, 6, 16),\n",
    "                    \"NIPS 2012\": date(2012, 12, 3),\n",
    "                    \"CVPR 2015\": date(2015, 6, 8),\n",
    "                    \"NIPS 2011\": date(2011, 12, 17),\n",
    "                    \"NIPS 2014\": date(2014, 12, 8),\n",
    "               \"TUM-I1222 2013\": date(2013, 10, 29),\n",
    "                     \"WMT 2014\": date(2014, 2, 24)}\n",
    "conferences_wanted = defaultdict(lambda: 0)\n",
    "\n",
    "offline = False\n",
    "try:\n",
    "    r = requests.get('http://arxiv.org/abs/1501.02876')\n",
    "    if str(r.status_code).startswith(\"4\"):\n",
    "        offline = True\n",
    "        print \"Arxiv blocked!\"\n",
    "except requests.ConnectionError:\n",
    "    print \"In Offline mode!\"\n",
    "    offline = True\n",
    "\n",
    "class Measurement:\n",
    "    def __init__(self, d, value, name, url, algorithms=[], uncertainty=0, minval=None, maxval=None, opensource=False, replicated=\"\",\n",
    "                 papername=None, venue=None, min_date=None, max_date=None, algorithm_src_url=None, withdrawn=False, \n",
    "                 not_directly_comparable=False, long_label=False):\n",
    "        self.date = d\n",
    "        self.value = value\n",
    "        assert isinstance(value, float) or isinstance(value, int), \"Measurements on metrics need to be numbers\"\n",
    "        self.name = name\n",
    "        \n",
    "        # For papers on arxiv, always use the abstract link rather than the PDF link\n",
    "        self.url = canonicalise(url)\n",
    "        assert self.url or papername, \"Measurements must have a URL or a paper name\"\n",
    "        self.min_date = min_date\n",
    "        self.max_date = max_date\n",
    "        self.aglorithm_src_url = algorithm_src_url\n",
    "        if algorithm_src_url and not min_date:\n",
    "            _, prev_dates, _ = ade.get_paper_data(algorithm_src_url)\n",
    "            if prev_dates:\n",
    "                self.min_date = min(prev_dates.values())\n",
    "                    \n",
    "        self.minval = minval if minval else value - uncertainty\n",
    "        self.maxval = maxval if maxval else value + uncertainty\n",
    "        self.opensource = opensource\n",
    "        self.not_directly_comparable = not_directly_comparable\n",
    "        self.replicated_url = replicated\n",
    "        self.long_label = long_label\n",
    "        self.algorithms = []\n",
    "        arxiv_papername, arxiv_dates, arxiv_withdrawn = ade.get_paper_data(self.url)\n",
    "        self.withdrawn = withdrawn or arxiv_withdrawn \n",
    "        if \"arxiv.org\" in self.url and not offline:\n",
    "            assert arxiv_dates, \"Failed to extract arxiv dates for \"+ self.url\n",
    "        self.papername = papername if papername else arxiv_papername\n",
    "        self.determine_paper_dates(d, arxiv_dates, venue)\n",
    "            \n",
    "        global measurements\n",
    "        measurements.add(self)\n",
    "\n",
    "    \n",
    "    year_re=re.compile(r\"([0-9][0-9][0-9][0-9])\")\n",
    "    def determine_paper_dates(self, d, arxiv_dates, venue):\n",
    "        \"\"\"\n",
    "        Try to figure out when a result was obtained, and our uncertainty on that.\n",
    "        \n",
    "        :param datetime.date d: date supplied at paper entry time. We may not be able to trust this if the paper had multiple versions\n",
    "                                   and the person doing the entry didn't specify which version they got their result numbers from :/\n",
    "        :param dict arxiv_dates:   either None or a dict like {\"1\": date(2017,1,13), \"2\": date(2017, 3, 4)...}\n",
    "        :param venue:              for Rodriguo Benenson's data, a publication venue like \"ICML 2016\" or \"arXiv 2014\"\n",
    "        \"\"\"\n",
    "        # begin by trusting whoever entered the data\n",
    "        self.date = d\n",
    "\n",
    "        # but warn if it doesn't match arXiv dates\n",
    "        adates = sorted(arxiv_dates.values()) if arxiv_dates else []\n",
    "\n",
    "        if arxiv_dates and d:\n",
    "            if d < min(adates) and d > max(adates):\n",
    "                print \"WARNING, date\", self.date, \"for\", self.url, \n",
    "                print \"does not match any of the arXiv versions (%s)\" % \" \".join(str(s) for s in arxiv_dates.values())\n",
    "        if arxiv_dates:\n",
    "            if len(arxiv_dates) == 1:\n",
    "                if not self.date:\n",
    "                    self.date = adates[0]\n",
    "            else:\n",
    "                # multiple arxiv dates means the url wasn't versioned, and we might not have gotten the date exactly right\n",
    "                self.min_date = self.min_date if self.min_date else min(adates)\n",
    "                self.max_date = self.max_date if self.max_date else max(adates)\n",
    "                if not self.date:\n",
    "                    midrange = datetime.timedelta(days=0.5 * (self.max_date - self.min_date).days)\n",
    "                    self.date = self.min_date + midrange\n",
    "        elif venue and not self.date:\n",
    "            # if all we have is a conference / journal, we might be able to still figure something out..\n",
    "            if venue.upper() in conference_dates:\n",
    "                self.date = conference_dates[venue]\n",
    "            else:\n",
    "                conferences_wanted[venue] += 1\n",
    "                year = int(self.year_re.search(venue).groups(0)[0])\n",
    "                self.date = date(year, 7, 1)\n",
    "                self.min_date = date(year, 1, 1)\n",
    "                self.max_date = date(year, 12, 31)\n",
    "        if not self.date:\n",
    "            print d, arxiv_dates, venue\n",
    "        assert self.date, \"Need a date for paper {0} {1}\".format(self.url, self.papername)\n",
    "\n",
    "#print canonicalise('http://arxiv.org/pdf/1412.6806.pdf')\n",
    "#cifar10.measure(None, 96.53, 'Fractional Max-Pooling', 'http://arxiv.org/abs/1412.6071', \n",
    "# papername='Fractional Max-Pooling', venue='arXiv 2015')\n",
    "#cifar10.measure(None, 95.59, 'Striving for Simplicity: The All Convolutional Net', \n",
    "# 'http://arxiv.org/pdf/1412.6806.pdf', papername='Striving for Simplicity: The All Convolutional Net', venue='ICLR 2015')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hiddencode\n",
    "# simple hooks for letting us save & restore datetime.date objects in a JSON cache\n",
    "class DateEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, datetime.date):\n",
    "            return str(obj)\n",
    "        return super(MyEncoder, self).default(obj)\n",
    "\n",
    "def parse_date(json_dict):\n",
    "    if \"dates\" in json_dict:\n",
    "        for v, date_str in json_dict[\"dates\"].items():\n",
    "            json_dict[\"dates\"][v] = datetime.datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
    "    return json_dict\n",
    "\n",
    "\n",
    "class ArxivDataExtractor:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            with open(\".paper_cache.json\") as f:\n",
    "                self.cache = json.load(f, object_hook=parse_date)\n",
    "        except:\n",
    "            print \"Failed to load local paper cache, trying a network copy...\"\n",
    "            try:\n",
    "                req = requests.get('https://raw.githubusercontent.com/AI-metrics/master_text/master/.paper_cache.json')\n",
    "                self.cache = json.loads(req.content, object_hook=parse_date)\n",
    "            except:\n",
    "                traceback.print_exc()\n",
    "                print \"(Continuing with an empty cache)\"\n",
    "                self.cache = {}\n",
    "        self.arxiv_re = re.compile(r\"\\[[0-9v.]+\\] (.*)\")\n",
    "        \n",
    "    def save_cache(self):\n",
    "        try:\n",
    "            with open(\".paper_cache.json\", \"w\") as f:\n",
    "                json.dump(self.cache, f, indent=4, sort_keys=True, cls=DateEncoder)\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "            print \"Not able to save cache\"\n",
    "    \n",
    "    ends_with_version = re.compile(r\".*v([0-9])+$\")\n",
    "    def arxiv_link_version(self, url):\n",
    "        m = self.ends_with_version.match(url)\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_paper_data(self, url):\n",
    "        \"Ask arxiv for a (papername, {version:date}) if we don't know it\"\n",
    "        if not url:\n",
    "            return (None, None, None)    \n",
    "        if url in self.cache:\n",
    "            c = self.cache[url]\n",
    "            return (c[\"name\"], c.get(\"dates\"), c.get(\"withdrawn\", False))\n",
    "        \n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except requests.ConnectionError:\n",
    "            print \"Failed to fetch\", url\n",
    "            #traceback.print_exc()\n",
    "            return (None, None, None)\n",
    "        record = {}\n",
    "        tree = lxml.html.fromstring(req.content)\n",
    "        withdrawn = self.detect_withdrawn(tree, url)\n",
    "        if withdrawn:\n",
    "            record[\"withdrawn\"] = True\n",
    "        papername = tree.findtext('.//title')\n",
    "        dates = None\n",
    "        if papername:\n",
    "            match = self.arxiv_re.match(papername)\n",
    "            if match:\n",
    "                papername = match.groups(0)[0]\n",
    "                v = self.arxiv_link_version(url)\n",
    "                dates = self.get_submission_dates(tree, v)\n",
    "                record[\"dates\"] = dates\n",
    "        record[\"name\"] = papername\n",
    "        self.cache[url] = record\n",
    "        print \"Caching paper name:\", papername\n",
    "        self.save_cache()\n",
    "        return papername, dates, withdrawn\n",
    "    \n",
    "    def detect_withdrawn(self, tree, url):\n",
    "        comment = CSSSelector(\".tablecell.comments\")(tree)\n",
    "        if comment:\n",
    "            comment = comment[0].text_content()\n",
    "            if \"withdrawn\" in comment.lower():\n",
    "                print \"Paper\", url, \"appears to be withdrawn!\"\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    version_re = re.compile(r\"\\[v([0-9]+)\\] (.*[0-9][0-9][0-9][0-9]) \")\n",
    "\n",
    "    def get_submission_dates(self, arxiv_tree, queried_version):\n",
    "        links = CSSSelector(\"div.submission-history\")(arxiv_tree)[0]\n",
    "        versions = {}\n",
    "        #print \"Parsing\", links.text_content()\n",
    "        for line in links.text_content().split(\"\\n\"):\n",
    "            match = self.version_re.match(line)\n",
    "            if match:\n",
    "                version, d = match.group(1), match.group(2)\n",
    "                d = datetime.datetime.strptime(d,'%a, %d %b %Y').date()\n",
    "                versions[version] = d\n",
    "                if queried_version == version:\n",
    "                    return {version: d}\n",
    "                #print version, date\n",
    "\n",
    "        return versions\n",
    "\n",
    "ade = ArxivDataExtractor()\n",
    "\n",
    "#ade.get_paper_data(\"https://arxiv.org/abs/1501.02876\")\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Problems, Metrics, and Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision\n",
    "\n",
    "<div style=\"float: right\">\n",
    "<br>\n",
    "<img src=\"images/imagenet.jpg\" style=\"width:80%; height: 80%\">\n",
    "<div style=\"text-align:center\">(Imagenet example data)</div>\n",
    "</div>\n",
    "\n",
    "The simplest vision subproblem is probably image classification, which determines what objects are present in a picture. From 2010-2017, Imagenet has been a closely watched contest for progress in this domain.\n",
    "\n",
    "Image classification includes not only recognising single things within an image, but localising them and essentially specifying which pixels are which object. MSRC-21 is a metric that is specifically for that task:\n",
    "\n",
    "<br>\n",
    "<div style=\"float:left; margin-left: 5%\">\n",
    "<img src=\"images/msrc_21.png\" style=\"margin-bottom: 10px\">\n",
    "<div style=\"text-align:center\">(MSRC 21 example data)</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# BEGIN ACTUALLY CLASSIFYING PROBLEMS\n",
    "\n",
    "vision = Problem(\"Vision\", [\"agi\", \"vision\", \"world-modelling\"])\n",
    "\n",
    "image_comprehension = Problem(\"Image comprehension\", [\"agi\", \"vision\", \"language\", \"world-modelling\"])\n",
    "image_classification = Problem(\"Image classification\", [\"vision\", \"agi\"])\n",
    "image_classification.add_subproblem(image_comprehension)\n",
    "vision.add_subproblem(image_classification)\n",
    "\n",
    "imagenet = image_classification.metric(\"Imagenet Image Recognition\", \"http://image-net.org\", scale=error_rate, target=0.051)\n",
    "imagenet.notes = \"\"\"\n",
    "Correctly label images from the Imagenet dataset. As of 2016, this includes:\n",
    " - Object localization for 1000 categories.\n",
    " - Object detection for 200 fully labeled categories.\n",
    " - Object detection from video for 30 fully labeled categories.\n",
    " - Scene classification for 365 scene categories (Joint with MIT Places team) on Places2 Database http://places2.csail.mit.edu.\n",
    " - Scene parsing for 150 stuff and discrete object categories (Joint with MIT Places team).\n",
    "\"\"\"\n",
    "imagenet.measure(date(2010,8,31), 0.28191, \"NEC UIUC\", \"http://image-net.org/challenges/LSVRC/2010/results\")\n",
    "imagenet.measure(date(2011,10,26), 0.2577, \"XRCE\",\"http://image-net.org/challenges/LSVRC/2011/results\")\n",
    "imagenet.measure(date(2012,10,13), 0.16422, \"SuperVision\", \"http://image-net.org/challenges/LSVRC/2012/results.html\")\n",
    "imagenet.measure(date(2013,11,14), 0.11743, \"Clarifai\",\"http://www.image-net.org/challenges/LSVRC/2013/results.php\")\n",
    "imagenet.measure(date(2014,8,18), 0.07405, \"VGG\", \"http://image-net.org/challenges/LSVRC/2014/index\")\n",
    "imagenet.measure(date(2015,12,10), 0.03567, \"MSRA\", \"http://image-net.org/challenges/LSVRC/2015/results\", algorithms=[\"residual-networks\"])\n",
    "imagenet.measure(date(2016,9,26), 0.02991, \"Trimps-Soushen\", \"http://image-net.org/challenges/LSVRC/2016/results\")\n",
    "\n",
    "# Test automatic detection of withdrawn papers\n",
    "imagenet.measure(None, 0.0458, \"withdrawn\", \"https://arxiv.org/abs/1501.02876\")\n",
    "\n",
    "video_classification = Problem(\"Recognise events in videos\")\n",
    "vision.add_subproblem(video_classification)\n",
    "video_classification.metric(\"YouTube-8M video labelling\", url=\"https://research.google.com/youtube8m/\")\n",
    "imagenet.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/vqa.jpg\" style=\"width: 50%; height: 50%; float: right\" float=\"right\">\n",
    "\n",
    "### Visual Question Answering\n",
    "\n",
    "\n",
    "Comprehending an image involves more than recognising what objects or entities are within it, but recognising events, relationships, and context from the image. This problem requires both sophisticated image recognition, language, world-modelling, and \"image comprehension\". There are several datasets in use. The illustration is from VQA, which was generated by asking Amazon Mechanical Turk workers to propose questions about photos from Microsoft's COCO image collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The VQA paper breaks human performance down by real/abstract image so we need to compute the overall number...\n",
    "# Also they don't seem to have human performance numbers for VQA multiple choice?\n",
    "vqa_abstract_human_performance = 87.49\n",
    "vqa_real_human_performance = 83.3\n",
    "vqa_imgs = 50000 + 204751.\n",
    "vqa_target = (50000/vqa_imgs) * vqa_abstract_human_performance + (204751 / vqa_imgs) * vqa_real_human_performance\n",
    "\n",
    "vqa_oe = image_comprehension.metric(\"COCO Visual Question Answering (VQA) 1.0 open ended\", url=\"http://visualqa.org/\", target=vqa_target, target_source=\"https://arxiv.org/abs/1505.00468\", scale=correct_percent)\n",
    "vqa_mc = image_comprehension.metric(\"COCO Visual Question Answering (VQA) 1.0 multiple choice\", url=\"http://visualqa.org/\", scale=correct_percent, solved=False)\n",
    "# other visual question answering metrics (we don't have data for these yet)\n",
    "# For a survey: https://arxiv.org/pdf/1607.05910\n",
    "image_comprehension.metric(\"Toronto COCO-QA\", url=\"http://www.cs.toronto.edu/~mren/imageqa/data/cocoqa/\" )\n",
    "image_comprehension.metric(\"DAQUAR\", url=\"https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/\", scale=correct_percent, target=60.27, target_source=\"https://arxiv.org/abs/1505.02074\")\n",
    "visual_genome_pairs = image_comprehension.metric(\"Visual Genome (pairs)\", url=\"http://visualgenome.org\", scale=correct_percent, axis_label=\"Top-1 precision\")\n",
    "visual_genome_subjects = image_comprehension.metric(\"Visual Genome (subjects)\", url=\"http://visualgenome.org\", scale=correct_percent, axis_label=\"Top-1 precision\")\n",
    "\n",
    "visual7w = image_comprehension.metric(\"Visual7W\", url=\"https://arxiv.org/abs/1511.03416\", scale=correct_percent)\n",
    "image_comprehension.metric(\"FM-IQA\", url=\"http://idl.baidu.com/FM-IQA.html\")\n",
    "image_comprehension.metric(\"Visual Madlibs\", url=\"http://tamaraberg.com/visualmadlibs/\")\n",
    "\n",
    "vqa_oe.measure(date(2015,12,15), 55.89, \"iBOWIMG baseline\", url=\"https://arxiv.org/abs/1512.02167\")\n",
    "vqa_mc.measure(date(2015,12,15), 61.97, \"iBOWIMG baseline\", url=\"https://arxiv.org/abs/1512.02167\")\n",
    "\n",
    "vqa_oe.measure(None, 58.24, \"SMem-VQA\", url=\"https://arxiv.org/abs/1511.05234v2\")\n",
    "\n",
    "# not so clear what the number in the SANv1 paper was...\n",
    "#vqa_oe.measure(None, 57.6, \"SAN(2,CNN)\", url=\"https://arxiv.org/abs/1511.02274v1\")\n",
    "vqa_oe.measure(None, 58.9, \"SAN\", url=\"https://arxiv.org/abs/1511.02274v2\")\n",
    "\n",
    "vqa_oe.measure(None, 59.5, \"CNN-RNN\", url=\"https://arxiv.org/abs/1603.02814v1\")\n",
    "\n",
    "vqa_oe.measure(None, 59.5, \"FDA\", url=\"https://arxiv.org/abs/1604.01485v1\")\n",
    "vqa_mc.measure(None, 64.2, \"FDA\", url=\"https://arxiv.org/abs/1604.01485v1\")\n",
    "\n",
    "vqa_oe.measure(None, 62.1, \"HQI+ResNet\", url=\"https://arxiv.org/abs/1606.00061v1\")\n",
    "vqa_mc.measure(None, 66.1, \"HQI+ResNet\", url=\"https://arxiv.org/abs/1606.00061v1\")\n",
    "\n",
    "vqa_oe.measure(None, 58.2, \"LSTM Q+I\", url=\"https://arxiv.org/abs/1505.00468v1\")\n",
    "vqa_mc.measure(None, 63.1, \"LSTM Q+I\", url=\"https://arxiv.org/abs/1505.00468v1\")\n",
    "\n",
    "vqa_oe.measure(None, 63.2, \"joint-loss\", url=\"https://arxiv.org/abs/1606.03647\")\n",
    "vqa_mc.measure(None, 67.3, \"joint-loss\", url=\"https://arxiv.org/abs/1606.03647\")\n",
    "\n",
    "vqa_oe.measure(None, 66.5, \"7 att. ensemble\", url=\"https://arxiv.org/abs/1606.01847v1\")\n",
    "vqa_mc.measure(None, 70.1, \"7 att. ensemble\", url=\"https://arxiv.org/abs/1606.01847v1\")\n",
    "visual7w.measure(None, 62.2, \"MCB+Att.\", url=\"https://arxiv.org/abs/1606.01847v1\")\n",
    "\n",
    "visual7w.measure(None, 72.53, \"CMN\", url=\"https://arxiv.org/abs/1611.09978v1\")\n",
    "visual_genome_pairs.measure(None, 28.52, \"CMN\", url=\"https://arxiv.org/abs/1611.09978v1\")\n",
    "visual_genome_subjects.measure(None, 44.24, \"CMN\", url=\"https://arxiv.org/abs/1611.09978v1\")\n",
    "\n",
    "vqa_oe.graph()\n",
    "vqa_mc.graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hiddencode\n",
    "# Rodriguo Benenson's \"Are We There Yet?\" data!\n",
    "reimport_awty = False\n",
    "if reimport_awty and not offline:\n",
    "    # Fetch the summary page\n",
    "    awty_url = \"https://rodrigob.github.io/are_we_there_yet/build/\"\n",
    "    req = requests.get(awty_url)\n",
    "    page = req.content.replace(\"</html>\", \"\", 1) # There is a crazy weird </html> near the top of the page that breaks everything\n",
    "    tree = lxml.html.fromstring(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hiddencode\n",
    "from urlparse import urlparse\n",
    "awty_datasets = {}\n",
    "\n",
    "if reimport_awty and not offline:\n",
    "    for e in CSSSelector('div.span7')(tree):\n",
    "        #print dir(e)\n",
    "        node = e.getchildren()[0].getchildren()[0]\n",
    "        link = node.attrib[\"href\"]\n",
    "        metric_name = node.text_content()\n",
    "        print \"%40s\" % metric_name, link\n",
    "        awty_datasets[metric_name] = urlparse(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hiddencode\n",
    "try:\n",
    "    got = json.load(open(\".awty_cache.json\"))\n",
    "except:\n",
    "    got = {}\n",
    "    \n",
    "\n",
    "# For lots of AWTY data, there's no date but there is a conference, so we can look up the dates that way.\n",
    "conf_dates = {}\n",
    "\n",
    "\n",
    "def parse_awty_dataset(name, link, verbose=False):\n",
    "    if offline or not reimport_awty: return []\n",
    "    print \"# Handling\", repr(name), link.geturl()\n",
    "    if link.path not in got:\n",
    "        page = requests.get(awty_url + link.path).content\n",
    "        tree = lxml.html.fromstring(page.replace(\"</html>\", \"\", 1))\n",
    "        got[link.path] = tree\n",
    "    else:\n",
    "        tree = got[link.path]\n",
    "    #print dir(tree)\n",
    "    #print \"fragment:\", link.fragment\n",
    "    #print page\n",
    "    results_section = CSSSelector(\"div#\" + link.fragment)(tree)[0]\n",
    "    \n",
    "    rows = CSSSelector(\"tr\")(results_section)\n",
    "    results = []\n",
    "    for r in rows[1:]:\n",
    "        result, paperlink, journal, _ = CSSSelector(\"td\")(r)\n",
    "        result, papername, journal = [e.text_content() for e in (result, paperlink, journal)]\n",
    "        if \"%\" not in result:\n",
    "            print \"# Skipping\", result, papername, journal\n",
    "            continue\n",
    "        if verbose: \n",
    "            print \"%6s\" % result, \"%90s\" % papername, \"%10s\" %journal\n",
    "        e = CSSSelector(\"a\")(paperlink)\n",
    "        paper_url = e[0].attrib[\"href\"] if e else None\n",
    "        results.append((result, papername, paper_url, journal))\n",
    "    return results\n",
    "\n",
    "percent_re = re.compile(r'([0-9.]+) *% *(\\(?\u00b1([0-9\\.]+))?')\n",
    "done = {}\n",
    "def ingest_awty_dataset(name, metric, label, regex=percent_re):\n",
    "    if offline or not reimport_awty:\n",
    "        #print \"Offline, not ingesting\", name\n",
    "        return None\n",
    "    done[name] = True\n",
    "    for result, papername, paper_url, journal in parse_awty_dataset(name, awty_datasets[name]):\n",
    "        try:\n",
    "            match = regex.match(result)\n",
    "            value = float(match.group(1))\n",
    "        except AttributeError:\n",
    "            print \"result\", result, \"does not parse\"\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            uncertainty = float(match.group(3)) if match.group(3) else 0.0\n",
    "        except IndexError:\n",
    "            uncertainty = 0.0\n",
    "\n",
    "        print \"%s.measure(%s, %s, '%s', url=%r, papername='%s', uncertainty=%s, venue='%s')\" % (\n",
    "               label, None, value, papername, paper_url,  papername, uncertainty, journal)\n",
    "        try:\n",
    "            metric.measure(None, value, papername, url=paper_url, papername=papername, uncertainty=uncertainty, venue=journal)\n",
    "        except requests.ConnectionError, e:\n",
    "            print \"Network error on {0} ({1}), skipping:\".format(paper_url, papername)\n",
    "            print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "msrc21_pc = image_classification.metric(\"MSRC-21 image semantic labelling (per-class)\", \"http://jamie.shotton.org/work/data.html\", scale=correct_percent)\n",
    "msrc21_pp = image_classification.metric(\"MSRC-21 image semantic labelling (per-pixel)\", \"http://jamie.shotton.org/work/data.html\", scale=correct_percent)\n",
    "\n",
    "\n",
    "cifar100 = image_classification.metric(\"CIFAR-100 Image Recognition\", \"http://https://www.cs.toronto.edu/~kriz/cifar.html\", scale=correct_percent)\n",
    "\n",
    "cifar10 = image_classification.metric(\"CIFAR-10 Image Recognition\", \"http://https://www.cs.toronto.edu/~kriz/cifar.html\", scale=correct_percent, target=94, target_source=\"http://karpathy.github.io/2011/04/27/manually-classifying-cifar10/\")\n",
    "\n",
    "svhn = image_classification.metric(\"Street View House Numbers (SVHN)\", \"http://ufldl.stanford.edu/housenumbers/\", scale=error_percent, target=2.0, target_source=\"http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf\")\n",
    "\n",
    "# We declare MNIST solved because the gap between best performance and human performance appears to be less than the uncertainty in human performance\n",
    "mnist = image_classification.metric(\"MNIST handwritten digit recognition\", \"http://yann.lecun.com/exdb/mnist/\", scale=error_percent, target=0.2, target_source=\"http://people.idsia.ch/~juergen/superhumanpatternrecognition.html\", solved=True)\n",
    "\n",
    "# This awty URL broken\n",
    "mnist.measure(date(2013,2,28), 0.52, 'COSFIRE', 'http://www.rug.nl/research/portal/files/2390194/2013IEEETPAMIAzzopardi.pdf', papername='Trainable COSFIRE Filters for Keypoint Detection and Pattern Recognition')\n",
    "# awty transcribes what's in this paper, but it seems to somehow have confused, wildly different units from everything else:\n",
    "for i, m in enumerate(mnist.measures):\n",
    "    if m.url == \"http://personal.ie.cuhk.edu.hk/~ccloy/files/aaai_2015_target_coding.pdf\":\n",
    "        del mnist.measures[i]\n",
    "\n",
    "stl10 = image_classification.metric(\"STL-10 Image Recognition\", \"https://cs.stanford.edu/~acoates/stl10/\", scale=correct_percent)\n",
    "if not offline: ingest_awty_dataset('STL-10', stl10, 'stl10')\n",
    "\n",
    "leeds_sport_poses = image_classification.metric(\"Leeds Sport Poses\")\n",
    "\n",
    "if reimport_awty and not offline:\n",
    "    ingest_awty_dataset('SVHN', svhn, 'svhn')\n",
    "    ingest_awty_dataset('CIFAR-100', cifar100, 'cifar100')\n",
    "    ingest_awty_dataset('CIFAR-10', cifar10, 'cifar10')\n",
    "    ingest_awty_dataset('MNIST', mnist, 'mnist')\n",
    "    ingest_awty_dataset('MSRC-21', msrc21_pc, 'msrc21_pc')\n",
    "    ingest_awty_dataset('MSRC-21', msrc21_pp, 'msrc21_pp', regex=re.compile(\"[0-9.]+ *% */ * ([0-9.]+) *%\"))\n",
    "\n",
    "    for name, link in awty_datasets.items():\n",
    "        if not link.scheme and name not in done:\n",
    "            parse_awty_dataset(name, link, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hiddencode\n",
    "# Automatically generated AWTY import data, but we may want to start editing these, moving the current name to papername, and adding a short\n",
    "# algorithm name\n",
    "# Handling 'MSRC-21' semantic_labeling_datasets_results.html#4d5352432d3231\n",
    "msrc21_pc.measure(None, 80.9, 'Large-Scale Semantic Co-Labeling of Image Sets', url='http://www.cvc.uab.es/~jalvarez/research_cosegment.php', papername='Large-Scale Semantic Co-Labeling of Image Sets', uncertainty=0.0, venue='WACV 2014')\n",
    "msrc21_pc.measure(None, 80.0, 'Harmony Potentials - Fusing Local and Global Scale for Semantic Image Segmentation', url='http://link.springer.com/article/10.1007%2Fs11263-011-0449-8', papername='Harmony Potentials - Fusing Local and Global Scale for Semantic Image Segmentation', uncertainty=0.0, venue='IJCV 2012')\n",
    "msrc21_pc.measure(None, 79.0, 'Describing the Scene as a Whole: Joint Object Detection, Scene Classification and Semantic Segmentation', url='http://ttic.uchicago.edu/~rurtasun/publications/yao_et_al_cvpr12.pdf', papername='Describing the Scene as a Whole: Joint Object Detection, Scene Classification and Semantic Segmentation', uncertainty=0.0, venue='CVPR 2012')\n",
    "msrc21_pc.measure(None, 78.2, 'Morphological Proximity Priors: Spatial Relationships for Semantic Segmentation', url='http://mediatum.ub.tum.de/doc/1175516/1175516.pdf', papername='Morphological Proximity Priors: Spatial Relationships for Semantic Segmentation', uncertainty=0.0, venue='TUM-I1222 2013')\n",
    "msrc21_pc.measure(None, 78.0, 'Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials', url='http://graphics.stanford.edu/projects/densecrf/densecrf.pdf', papername='Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials', uncertainty=0.0, venue='NIPS 2011')\n",
    "msrc21_pc.measure(None, 77.0, 'Graph Cut based Inference with Co-occurrence Statistics', url='http://research.microsoft.com/en-us/um/people/pkohli/papers/lrkt_eccv2010.pdf', papername='Graph Cut based Inference with Co-occurrence Statistics', uncertainty=0.0, venue='ECCV 2010')\n",
    "msrc21_pc.measure(None, 77.0, 'Are Spatial and Global Constraints Really Necessary for Segmentation?', url='http://infoscience.epfl.ch/record/169178/files/lucchi_ICCV11.pdf', papername='Are Spatial and Global Constraints Really Necessary for Segmentation?', uncertainty=0.0, venue='ICCV 2011')\n",
    "msrc21_pc.measure(None, 76.0, 'Structured Image Segmentation using Kernelized Features', url='http://infoscience.epfl.ch/record/180190', papername='Structured Image Segmentation using Kernelized Features', uncertainty=0.0, venue='ECCV 2012')\n",
    "msrc21_pc.measure(None, 72.8, 'PatchMatchGraph: Building a Graph of Dense Patch Correspondences for Label Transfer', url='http://users.cecs.anu.edu.au/~sgould/papers/eccv12-patchGraph.pdf', papername='PatchMatchGraph: Building a Graph of Dense Patch Correspondences for Label Transfer', uncertainty=0.0, venue='ECCV 2012')\n",
    "msrc21_pc.measure(None, 69.0, 'Auto-Context and Its Application to High-Level Vision Tasks and 3D Brain Image Segmentation', url='http://pages.ucsd.edu/~ztu/publication/pami_autocontext.pdf', papername='Auto-Context and Its Application to High-Level Vision Tasks and 3D Brain Image Segmentation', uncertainty=0.0, venue='PAMI 2010')\n",
    "msrc21_pc.measure(None, 67.0, 'Semantic Texton Forests for Image Categorization and Segmentation', url='http://mi.eng.cam.ac.uk/~cipolla/publications/inproceedings/2008-CVPR-semantic-texton-forests.pdf', papername='Semantic Texton Forests for Image Categorization and Segmentation', uncertainty=0.0, venue='CVPR 2008')\n",
    "msrc21_pc.measure(None, 57.0, 'TextonBoost for Image Understanding', url='http://research.microsoft.com/pubs/117885/ijcv07a.pdf', papername='TextonBoost for Image Understanding', uncertainty=0.0, venue='IJCV 2009')\n",
    "#Handling 'MSRC-21' semantic_labeling_datasets_results.html#4d5352432d3231\n",
    "msrc21_pp.measure(None, 86.8, 'Large-Scale Semantic Co-Labeling of Image Sets', url='http://www.cvc.uab.es/~jalvarez/research_cosegment.php', papername='Large-Scale Semantic Co-Labeling of Image Sets', uncertainty=0.0, venue='WACV 2014')\n",
    "msrc21_pp.measure(None, 83.0, 'Harmony Potentials - Fusing Local and Global Scale for Semantic Image Segmentation', url='http://link.springer.com/article/10.1007%2Fs11263-011-0449-8', papername='Harmony Potentials - Fusing Local and Global Scale for Semantic Image Segmentation', uncertainty=0.0, venue='IJCV 2012')\n",
    "msrc21_pp.measure(None, 86.0, 'Describing the Scene as a Whole: Joint Object Detection, Scene Classification and Semantic Segmentation', url='http://ttic.uchicago.edu/~rurtasun/publications/yao_et_al_cvpr12.pdf', papername='Describing the Scene as a Whole: Joint Object Detection, Scene Classification and Semantic Segmentation', uncertainty=0.0, venue='CVPR 2012')\n",
    "msrc21_pp.measure(None, 85.0, 'Morphological Proximity Priors: Spatial Relationships for Semantic Segmentation', url='http://mediatum.ub.tum.de/doc/1175516/1175516.pdf', papername='Morphological Proximity Priors: Spatial Relationships for Semantic Segmentation', uncertainty=0.0, venue='TUM-I1222 2013')\n",
    "msrc21_pp.measure(None, 86.0, 'Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials', url='http://graphics.stanford.edu/projects/densecrf/densecrf.pdf', papername='Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials', uncertainty=0.0, venue='NIPS 2011')\n",
    "msrc21_pp.measure(None, 87.0, 'Graph Cut based Inference with Co-occurrence Statistics', url='http://research.microsoft.com/en-us/um/people/pkohli/papers/lrkt_eccv2010.pdf', papername='Graph Cut based Inference with Co-occurrence Statistics', uncertainty=0.0, venue='ECCV 2010')\n",
    "msrc21_pp.measure(None, 85.0, 'Are Spatial and Global Constraints Really Necessary for Segmentation?', url='http://infoscience.epfl.ch/record/169178/files/lucchi_ICCV11.pdf', papername='Are Spatial and Global Constraints Really Necessary for Segmentation?', uncertainty=0.0, venue='ICCV 2011')\n",
    "msrc21_pp.measure(None, 82.0, 'Structured Image Segmentation using Kernelized Features', url='http://infoscience.epfl.ch/record/180190', papername='Structured Image Segmentation using Kernelized Features', uncertainty=0.0, venue='ECCV 2012')\n",
    "msrc21_pp.measure(None, 79.0, 'PatchMatchGraph: Building a Graph of Dense Patch Correspondences for Label Transfer', url='http://users.cecs.anu.edu.au/~sgould/papers/eccv12-patchGraph.pdf', papername='PatchMatchGraph: Building a Graph of Dense Patch Correspondences for Label Transfer', uncertainty=0.0, venue='ECCV 2012')\n",
    "msrc21_pp.measure(None, 78.0, 'Auto-Context and Its Application to High-Level Vision Tasks and 3D Brain Image Segmentation', url='http://pages.ucsd.edu/~ztu/publication/pami_autocontext.pdf', papername='Auto-Context and Its Application to High-Level Vision Tasks and 3D Brain Image Segmentation', uncertainty=0.0, venue='PAMI 2010')\n",
    "msrc21_pp.measure(None, 72.0, 'Semantic Texton Forests for Image Categorization and Segmentation', url='http://mi.eng.cam.ac.uk/~cipolla/publications/inproceedings/2008-CVPR-semantic-texton-forests.pdf', papername='Semantic Texton Forests for Image Categorization and Segmentation', uncertainty=0.0, venue='CVPR 2008')\n",
    "msrc21_pp.measure(None, 72.0, 'TextonBoost for Image Understanding', url='http://research.microsoft.com/pubs/117885/ijcv07a.pdf', papername='TextonBoost for Image Understanding', uncertainty=0.0, venue='IJCV 2009')\n",
    "\n",
    "\n",
    "#Handling 'CIFAR-100' classification_datasets_results.html#43494641522d313030\n",
    "cifar100.measure(None, 75.72, 'Fast and Accurate Deep Network Learning by Exponential Linear Units', url='http://arxiv.org/abs/1511.07289', papername='Fast and Accurate Deep Network Learning by Exponential Linear Units', uncertainty=0.0, venue='arXiv 2015')\n",
    "cifar100.measure(None, 75.7, 'Spatially-sparse convolutional neural networks', url='http://arxiv.org/abs/1409.6070', papername='Spatially-sparse convolutional neural networks', uncertainty=0.0, venue='arXiv 2014')\n",
    "cifar100.measure(None, 73.61, 'Fractional Max-Pooling', url='http://arxiv.org/abs/1412.6071', papername='Fractional Max-Pooling', uncertainty=0.0, venue='arXiv 2015')\n",
    "cifar100.measure(None, 72.6, 'Scalable Bayesian Optimization Using Deep Neural Networks', url='http://arxiv.org/abs/1502.05700', papername='Scalable Bayesian Optimization Using Deep Neural Networks', uncertainty=0.0, venue='ICML 2015')\n",
    "cifar100.measure(None, 72.44, 'Competitive Multi-scale Convolution', url='http://arxiv.org/abs/1511.05635', papername='Competitive Multi-scale Convolution', uncertainty=0.0, venue='arXiv 2015')\n",
    "cifar100.measure(None, 72.34, 'All you need is a good init', url='http://arxiv.org/abs/1511.06422', papername='All you need is a good init', uncertainty=0.0, venue='ICLR 2015')\n",
    "cifar100.measure(None, 71.14, 'Batch-normalized Maxout Network in Network', url='http://arxiv.org/abs/1511.02583', papername='Batch-normalized Maxout Network in Network', uncertainty=0.0, venue='arXiv 2015')\n",
    "cifar100.measure(None, 70.8, 'On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units', url='http://arxiv.org/abs/1508.00330', papername='On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units', uncertainty=0.0, venue='arXiv 2015')\n",
    "cifar100.measure(None, 69.17, 'Learning Activation Functions to Improve Deep Neural Networks', url='http://arxiv.org/abs/1412.6830', papername='Learning Activation Functions to Improve Deep Neural Networks', uncertainty=0.0, venue='ICLR 2015')\n",
    "cifar100.measure(None, 69.12, 'Stacked What-Where Auto-encoders', url='http://arxiv.org/abs/1506.02351', papername='Stacked What-Where Auto-encoders', uncertainty=0.0, venue='arXiv 2015')\n",
    "cifar100.measure(None, 68.53, 'Multi-Loss Regularized Deep Neural Network', url='http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7258343', papername='Multi-Loss Regularized Deep Neural Network', uncertainty=0.0, venue='CSVT 2015')\n",
    "cifar100.measure(None, 68.4, 'Spectral Representations for Convolutional Neural Networks', url='http://papers.nips.cc/paper/5649-spectral-representations-for-convolutional-neural-networks.pdf', papername='Spectral Representations for Convolutional Neural Networks', uncertainty=0.0, venue='NIPS 2015')\n",
    "cifar100.measure(None, 68.25, 'Recurrent Convolutional Neural Network for Object Recognition', url='http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2B_004.pdf', papername='Recurrent Convolutional Neural Network for Object Recognition', uncertainty=0.0, venue='CVPR 2015')\n",
    "cifar100.measure(None, 67.76, 'Training Very Deep Networks', url='http://people.idsia.ch/~rupesh/very_deep_learning/', papername='Training Very Deep Networks', uncertainty=0.0, venue='NIPS 2015')\n",
    "cifar100.measure(None, 67.68, 'Deep Convolutional Neural Networks as Generic Feature Extractors', url='http://www.isip.uni-luebeck.de/fileadmin/uploads/tx_wapublications/hertel_ijcnn_2015.pdf', papername='Deep Convolutional Neural Networks as Generic Feature Extractors', uncertainty=0.0, venue='IJCNN 2015')\n",
    "cifar100.measure(None, 67.63, 'Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree', url='http://arxiv.org/abs/1509.08985', papername='Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree', uncertainty=0.0, venue='AISTATS 2016')\n",
    "cifar100.measure(None, 67.38, 'HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition', url='https://sites.google.com/site/homepagezhichengyan/home/hdcnn', papername='HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition', uncertainty=0.0, venue='ICCV 2015')\n",
    "cifar100.measure(None, 67.16, 'Universum Prescription: Regularization using Unlabeled Data', url='http://arxiv.org/abs/1511.03719', papername='Universum Prescription: Regularization using Unlabeled Data', uncertainty=0.0, venue='arXiv 2015')\n",
    "cifar100.measure(None, 66.29, 'Striving for Simplicity: The All Convolutional Net', url='http://arxiv.org/pdf/1412.6806.pdf', papername='Striving for Simplicity: The All Convolutional Net', uncertainty=0.0, venue='ICLR 2014')\n",
    "cifar100.measure(None, 66.22, 'Deep Networks with Internal Selective Attention through Feedback Connections', url='http://papers.nips.cc/paper/5276-deep-networks-with-internal-selective-attention-through-feedback-connections.pdf', papername='Deep Networks with Internal Selective Attention through Feedback Connections', uncertainty=0.0, venue='NIPS 2014')\n",
    "cifar100.measure(None, 65.43, 'Deeply-Supervised Nets', url='http://vcl.ucsd.edu/~sxie/2014/09/12/dsn-project/', papername='Deeply-Supervised Nets', uncertainty=0.0, venue='arXiv 2014')\n",
    "cifar100.measure(None, 64.77, 'Deep Representation Learning with Target Coding', url='http://personal.ie.cuhk.edu.hk/~ccloy/files/aaai_2015_target_coding.pdf', papername='Deep Representation Learning with Target Coding', uncertainty=0.0, venue='AAAI 2015')\n",
    "cifar100.measure(None, 64.32, 'Network in Network', url='http://openreview.net/document/9b05a3bb-3a5e-49cb-91f7-0f482af65aea#9b05a3bb-3a5e-49cb-91f7-0f482af65aea', papername='Network in Network', uncertainty=0.0, venue='ICLR 2014')\n",
    "cifar100.measure(None, 63.15, 'Discriminative Transfer Learning with Tree-based Priors', url='http://www.cs.toronto.edu/~nitish/treebasedpriors.pdf', papername='Discriminative Transfer Learning with Tree-based Priors', uncertainty=0.0, venue='NIPS 2013')\n",
    "cifar100.measure(None, 61.86, 'Improving Deep Neural Networks with Probabilistic Maxout Units', url='http://openreview.net/document/28d9c3ab-fe88-4836-b898-403d207a037c#28d9c3ab-fe88-4836-b898-403d207a037c', papername='Improving Deep Neural Networks with Probabilistic Maxout Units', uncertainty=0.0, venue='ICLR 2014')\n",
    "cifar100.measure(None, 61.43, 'Maxout Networks', url='http://jmlr.org/proceedings/papers/v28/goodfellow13.pdf', papername='Maxout Networks', uncertainty=0.0, venue='ICML 2013')\n",
    "cifar100.measure(None, 60.8, 'Stable and Efficient Representation Learning with Nonnegativity Constraints ', url='http://jmlr.org/proceedings/papers/v32/line14.pdf', papername='Stable and Efficient Representation Learning with Nonnegativity Constraints ', uncertainty=0.0, venue='ICML 2014')\n",
    "cifar100.measure(None, 59.75, 'Empirical Evaluation of Rectified Activations in Convolution Network', url='http://arxiv.org/pdf/1505.00853.pdf', papername='Empirical Evaluation of Rectified Activations in Convolution Network', uncertainty=0.0, venue='ICML workshop 2015')\n",
    "cifar100.measure(None, 57.49, 'Stochastic Pooling for Regularization of Deep Convolutional Neural Networks', url='http://arxiv.org/pdf/1301.3557.pdf', papername='Stochastic Pooling for Regularization of Deep Convolutional Neural Networks', uncertainty=0.0, venue='arXiv 2013')\n",
    "cifar100.measure(None, 56.29, 'Learning Smooth Pooling Regions for Visual Recognition', url='http://www.d2.mpi-inf.mpg.de/content/learning-smooth-pooling-regions-visual-recognition', papername='Learning Smooth Pooling Regions for Visual Recognition', uncertainty=0.0, venue='BMVC 2013')\n",
    "cifar100.measure(None, 54.23, 'Beyond Spatial Pyramids: Receptive Field Learning for Pooled Image Features', url='http://www.eecs.berkeley.edu/~jiayq/assets/pdf/cvpr12_pooling.pdf', papername='Beyond Spatial Pyramids: Receptive Field Learning for Pooled Image Features', uncertainty=0.0, venue='CVPR 2012')\n",
    "\n",
    "#Handling 'CIFAR-10' classification_datasets_results.html#43494641522d3130\n",
    "cifar10.measure(None, 96.53, 'Fractional Max-Pooling', url='http://arxiv.org/abs/1412.6071', papername='Fractional Max-Pooling', uncertainty=0.0, venue='arXiv 2015')\n",
    "cifar10.measure(None, 95.59, 'Striving for Simplicity: The All Convolutional Net', url='http://arxiv.org/pdf/1412.6806.pdf', papername='Striving for Simplicity: The All Convolutional Net', uncertainty=0.0, venue='ICLR 2015')\n",
    "cifar10.measure(None, 94.16, 'All you need is a good init', url='http://arxiv.org/abs/1511.06422', papername='All you need is a good init', uncertainty=0.0, venue='ICLR 2016')\n",
    "cifar10.measure(None, 94.0, 'Lessons learned from manually classifying CIFAR-10', url='http://karpathy.github.io/2011/04/27/manually-classifying-cifar10/', papername='Lessons learned from manually classifying CIFAR-10', uncertainty=0.0, venue='unpublished 2011')\n",
    "#Skipping aparent source paper http://karpathy.github.io/2011/04/27/manually-classifying-cifar10/\n",
    "cifar10.measure(None, 93.95, 'Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree', url='http://arxiv.org/abs/1509.08985', papername='Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree', uncertainty=0.0, venue='AISTATS 2016')\n",
    "cifar10.measure(None, 93.72, 'Spatially-sparse convolutional neural networks', url='http://arxiv.org/abs/1409.6070', papername='Spatially-sparse convolutional neural networks', uncertainty=0.0, venue='arXiv 2014')\n",
    "cifar10.measure(None, 93.63, 'Scalable Bayesian Optimization Using Deep Neural Networks', url='http://arxiv.org/abs/1502.05700', papername='Scalable Bayesian Optimization Using Deep Neural Networks', uncertainty=0.0, venue='ICML 2015')\n",
    "cifar10.measure(None, 93.57, 'Deep Residual Learning for Image Recognition', url='http://arxiv.org/abs/1512.03385', papername='Deep Residual Learning for Image Recognition', uncertainty=0.0, venue='arXiv 2015')\n",
    "cifar10.measure(None, 93.45, 'Fast and Accurate Deep Network Learning by Exponential Linear Units', url='http://arxiv.org/abs/1511.07289', papername='Fast and Accurate Deep Network Learning by Exponential Linear Units', uncertainty=0.0, venue='arXiv 2015')\n",
    "cifar10.measure(None, 93.34, 'Universum Prescription: Regularization using Unlabeled Data', url='http://arxiv.org/abs/1511.03719', papername='Universum Prescription: Regularization using Unlabeled Data', uncertainty=0.0, venue='arXiv 2015')\n",
    "cifar10.measure(None, 93.25, 'Batch-normalized Maxout Network in Network', url='http://arxiv.org/abs/1511.02583', papername='Batch-normalized Maxout Network in Network', uncertainty=0.0, venue='arXiv 2015')\n",
    "cifar10.measure(None, 93.13, 'Competitive Multi-scale Convolution', url='http://arxiv.org/abs/1511.05635', papername='Competitive Multi-scale Convolution', uncertainty=0.0, venue='arXiv 2015')\n",
    "cifar10.measure(None, 92.91, 'Recurrent Convolutional Neural Network for Object Recognition', url='http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2B_004.pdf', papername='Recurrent Convolutional Neural Network for Object Recognition', uncertainty=0.0, venue='CVPR 2015')\n",
    "cifar10.measure(None, 92.49, 'Learning Activation Functions to Improve Deep Neural Networks', url='http://arxiv.org/abs/1412.6830', papername='Learning Activation Functions to Improve Deep Neural Networks', uncertainty=0.0, venue='ICLR 2015')\n",
    "cifar10.measure(None, 92.45, 'cifar.torch', url='http://torch.ch/blog/2015/07/30/cifar.html', papername='cifar.torch', uncertainty=0.0, venue='unpublished 2015')\n",
    "cifar10.measure(None, 92.4, 'Training Very Deep Networks', url='http://people.idsia.ch/~rupesh/very_deep_learning/', papername='Training Very Deep Networks', uncertainty=0.0, venue='NIPS 2015')\n",
    "cifar10.measure(None, 92.23, 'Stacked What-Where Auto-encoders', url='http://arxiv.org/abs/1506.02351', papername='Stacked What-Where Auto-encoders', uncertainty=0.0, venue='arXiv 2015')\n",
    "cifar10.measure(None, 91.88, 'Multi-Loss Regularized Deep Neural Network', url='http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7258343', papername='Multi-Loss Regularized Deep Neural Network', uncertainty=0.0, venue='CSVT 2015')\n",
    "cifar10.measure(None, 91.78, 'Deeply-Supervised Nets', url='http://vcl.ucsd.edu/~sxie/2014/09/12/dsn-project/', papername='Deeply-Supervised Nets', uncertainty=0.0, venue='arXiv 2014')\n",
    "cifar10.measure(None, 91.73, 'BinaryConnect: Training Deep Neural Networks with binary weights during propagations', url='http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf', papername='BinaryConnect: Training Deep Neural Networks with binary weights during propagations', uncertainty=0.0, venue='NIPS 2015')\n",
    "cifar10.measure(None, 91.48, 'On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units', url='http://arxiv.org/abs/1508.00330', papername='On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units', uncertainty=0.0, venue='arXiv 2015')\n",
    "cifar10.measure(None, 91.4, 'Spectral Representations for Convolutional Neural Networks', url='http://papers.nips.cc/paper/5649-spectral-representations-for-convolutional-neural-networks.pdf', papername='Spectral Representations for Convolutional Neural Networks', uncertainty=0.0, venue='NIPS 2015')\n",
    "cifar10.measure(None, 91.2, 'Network In Network', url='http://openreview.net/document/9b05a3bb-3a5e-49cb-91f7-0f482af65aea#9b05a3bb-3a5e-49cb-91f7-0f482af65aea', papername='Network In Network', uncertainty=0.0, venue='ICLR 2014')\n",
    "cifar10.measure(None, 91.19, 'Speeding up Automatic Hyperparameter Optimization of Deep Neural Networks by Extrapolation of Learning Curves', url='http://aad.informatik.uni-freiburg.de/papers/15-IJCAI-Extrapolation_of_Learning_Curves.pdf', papername='Speeding up Automatic Hyperparameter Optimization of Deep Neural Networks by Extrapolation of Learning Curves', uncertainty=0.0, venue='IJCAI 2015')\n",
    "cifar10.measure(None, 90.78, 'Deep Networks with Internal Selective Attention through Feedback Connections', url='http://papers.nips.cc/paper/5276-deep-networks-with-internal-selective-attention-through-feedback-connections.pdf', papername='Deep Networks with Internal Selective Attention through Feedback Connections', uncertainty=0.0, venue='NIPS 2014')\n",
    "cifar10.measure(None, 90.68, 'Regularization of Neural Networks using DropConnect', url='http://cs.nyu.edu/~wanli/dropc/', papername='Regularization of Neural Networks using DropConnect', uncertainty=0.0, venue='ICML 2013')\n",
    "cifar10.measure(None, 90.65, 'Maxout Networks', url='http://jmlr.org/proceedings/papers/v28/goodfellow13.pdf', papername='Maxout Networks', uncertainty=0.0, venue='ICML 2013')\n",
    "cifar10.measure(None, 90.61, 'Improving Deep Neural Networks with Probabilistic Maxout Units', url='http://openreview.net/document/28d9c3ab-fe88-4836-b898-403d207a037c#28d9c3ab-fe88-4836-b898-403d207a037c', papername='Improving Deep Neural Networks with Probabilistic Maxout Units', uncertainty=0.0, venue='ICLR 2014')\n",
    "cifar10.measure(None, 90.5, 'Practical Bayesian Optimization of Machine Learning Algorithms ', url='http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf', papername='Practical Bayesian Optimization of Machine Learning Algorithms ', uncertainty=0.0, venue='NIPS 2012')\n",
    "cifar10.measure(None, 89.67, 'APAC: Augmented PAttern Classification with Neural Networks', url='http://arxiv.org/abs/1505.03229', papername='APAC: Augmented PAttern Classification with Neural Networks', uncertainty=0.0, venue='arXiv 2015')\n",
    "cifar10.measure(None, 89.14, 'Deep Convolutional Neural Networks as Generic Feature Extractors', url='http://www.isip.uni-luebeck.de/fileadmin/uploads/tx_wapublications/hertel_ijcnn_2015.pdf', papername='Deep Convolutional Neural Networks as Generic Feature Extractors', uncertainty=0.0, venue='IJCNN 2015')\n",
    "cifar10.measure(None, 89.0, 'ImageNet Classification with Deep Convolutional Neural Networks', url='http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks', papername='ImageNet Classification with Deep Convolutional Neural Networks', uncertainty=0.0, venue='NIPS 2012')\n",
    "cifar10.measure(None, 88.8, 'Empirical Evaluation of Rectified Activations in Convolution Network', url='http://arxiv.org/pdf/1505.00853.pdf', papername='Empirical Evaluation of Rectified Activations in Convolution Network', uncertainty=0.0, venue='ICML workshop 2015')\n",
    "cifar10.measure(None, 88.79, 'Multi-Column Deep Neural Networks for Image Classification ', url='http://www.idsia.ch/~ciresan/data/cvpr2012.pdf', papername='Multi-Column Deep Neural Networks for Image Classification ', uncertainty=0.0, venue='CVPR 2012')\n",
    "cifar10.measure(None, 87.65, 'ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks', url='http://arxiv.org/abs/1505.00393', papername='ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks', uncertainty=0.0, venue='arXiv 2015')\n",
    "cifar10.measure(None, 86.7, 'An Analysis of Unsupervised Pre-training in Light of Recent Advances', url='http://arxiv.org/abs/1412.6597', papername='An Analysis of Unsupervised Pre-training in Light of Recent Advances', uncertainty=0.0, venue='ICLR 2015')\n",
    "cifar10.measure(None, 84.87, 'Stochastic Pooling for Regularization of Deep Convolutional Neural Networks', url='http://arxiv.org/pdf/1301.3557.pdf', papername='Stochastic Pooling for Regularization of Deep Convolutional Neural Networks', uncertainty=0.0, venue='arXiv 2013')\n",
    "cifar10.measure(None, 84.4, 'Improving neural networks by preventing co-adaptation of feature detectors', url='http://arxiv.org/pdf/1207.0580.pdf', papername='Improving neural networks by preventing co-adaptation of feature detectors', uncertainty=0.0, venue='arXiv 2012')\n",
    "cifar10.measure(None, 83.96, 'Discriminative Learning of Sum-Product Networks', url='http://papers.nips.cc/paper/4516-discriminative-learning-of-sum-product-networks', papername='Discriminative Learning of Sum-Product Networks', uncertainty=0.0, venue='NIPS 2012')\n",
    "cifar10.measure(None, 82.9, 'Stable and Efficient Representation Learning with Nonnegativity Constraints ', url='http://jmlr.org/proceedings/papers/v32/line14.pdf', papername='Stable and Efficient Representation Learning with Nonnegativity Constraints ', uncertainty=0.0, venue='ICML 2014')\n",
    "cifar10.measure(None, 82.2, 'Learning Invariant Representations with Local Transformations', url='http://icml.cc/2012/papers/659.pdf', papername='Learning Invariant Representations with Local Transformations', uncertainty=0.0, venue='ICML 2012')\n",
    "cifar10.measure(None, 82.18, 'Convolutional Kernel Networks', url='http://arxiv.org/abs/1406.3332', papername='Convolutional Kernel Networks', uncertainty=0.0, venue='arXiv 2014')\n",
    "cifar10.measure(None, 82.0, 'Discriminative Unsupervised Feature Learning with Convolutional Neural Networks', url='http://papers.nips.cc/paper/5548-discriminative-unsupervised-feature-learning-with-convolutional-neural-networks.pdf', papername='Discriminative Unsupervised Feature Learning with Convolutional Neural Networks', uncertainty=0.0, venue='NIPS 2014')\n",
    "cifar10.measure(None, 80.02, 'Learning Smooth Pooling Regions for Visual Recognition', url='http://www.d2.mpi-inf.mpg.de/content/learning-smooth-pooling-regions-visual-recognition', papername='Learning Smooth Pooling Regions for Visual Recognition', uncertainty=0.0, venue='BMVC 2013')\n",
    "cifar10.measure(None, 80.0, 'Object Recognition with Hierarchical Kernel Descriptors', url='http://research.cs.washington.edu/istc/lfb/paper/cvpr11.pdf', papername='Object Recognition with Hierarchical Kernel Descriptors', uncertainty=0.0, venue='CVPR 2011')\n",
    "cifar10.measure(None, 79.7, 'Learning with Recursive Perceptual Representations', url='http://papers.nips.cc/paper/4747-learning-with-recursive-perceptual-representations', papername='Learning with Recursive Perceptual Representations', uncertainty=0.0, venue='NIPS 2012')\n",
    "cifar10.measure(None, 79.6, 'An Analysis of Single-Layer Networks in Unsupervised Feature Learning ', url='http://www.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf', papername='An Analysis of Single-Layer Networks in Unsupervised Feature Learning ', uncertainty=0.0, venue='AISTATS 2011')\n",
    "cifar10.measure(None, 78.67, 'PCANet: A Simple Deep Learning Baseline for Image Classification?', url='http://arxiv.org/abs/1404.3606', papername='PCANet: A Simple Deep Learning Baseline for Image Classification?', uncertainty=0.0, venue='arXiv 2014')\n",
    "cifar10.measure(None, 75.86, 'Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network', url='http://arxiv.org/abs/1503.04596', papername='Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network', uncertainty=0.0, venue='arXiv 2015')\n",
    "\n",
    "#Handling 'SVHN' classification_datasets_results.html#5356484e\n",
    "svhn.measure(None, 1.69, 'Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree', url='http://arxiv.org/abs/1509.08985', papername='Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree', uncertainty=0.0, venue='AISTATS 2016')\n",
    "svhn.measure(None, 1.76, 'Competitive Multi-scale Convolution', url='http://arxiv.org/abs/1511.05635', papername='Competitive Multi-scale Convolution', uncertainty=0.0, venue='arXiv 2015')\n",
    "svhn.measure(None, 1.77, 'Recurrent Convolutional Neural Network for Object Recognition', url='http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2B_004.pdf', papername='Recurrent Convolutional Neural Network for Object Recognition', uncertainty=0.0, venue='CVPR 2015')\n",
    "svhn.measure(None, 1.81, 'Batch-normalized Maxout Network in Network', url='http://arxiv.org/abs/1511.02583', papername='Batch-normalized Maxout Network in Network', uncertainty=0.0, venue='arXiv 2015')\n",
    "svhn.measure(None, 1.92, 'Deeply-Supervised Nets', url='http://vcl.ucsd.edu/~sxie/2014/09/12/dsn-project/', papername='Deeply-Supervised Nets', uncertainty=0.0, venue='arXiv 2014')\n",
    "svhn.measure(None, 1.92, 'Multi-Loss Regularized Deep Neural Network', url='http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7258343', papername='Multi-Loss Regularized Deep Neural Network', uncertainty=0.0, venue='CSVT 2015')\n",
    "svhn.measure(None, 1.94, 'Regularization of Neural Networks using DropConnect', url='http://cs.nyu.edu/~wanli/dropc/', papername='Regularization of Neural Networks using DropConnect', uncertainty=0.0, venue='ICML 2013')\n",
    "svhn.measure(None, 1.97, 'On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units', url='http://arxiv.org/abs/1508.00330', papername='On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units', uncertainty=0.0, venue='arXiv 2015')\n",
    "svhn.measure(None, 2.0, 'Estimated human performance', url='http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf', papername='Estimated human performance', uncertainty=0.0, venue='NIPS 2011')\n",
    "# Skipping aparent source paper http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf\n",
    "svhn.measure(None, 2.15, 'BinaryConnect: Training Deep Neural Networks with binary weights during propagations', url='http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf', papername='BinaryConnect: Training Deep Neural Networks with binary weights during propagations', uncertainty=0.0, venue='NIPS 2015')\n",
    "svhn.measure(None, 2.16, 'Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks', url='http://openreview.net/document/0c571b22-f4b6-4d58-87e4-99d7de42a893#0c571b22-f4b6-4d58-87e4-99d7de42a893', papername='Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks', uncertainty=0.0, venue='ICLR 2014')\n",
    "svhn.measure(None, 2.35, 'Network in Network', url='http://openreview.net/document/9b05a3bb-3a5e-49cb-91f7-0f482af65aea#9b05a3bb-3a5e-49cb-91f7-0f482af65aea', papername='Network in Network', uncertainty=0.0, venue='ICLR 2014')\n",
    "svhn.measure(None, 2.38, 'ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks', url='http://arxiv.org/abs/1505.00393', papername='ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks', uncertainty=0.0, venue='arXiv 2015')\n",
    "svhn.measure(None, 2.47, 'Maxout Networks', url='http://jmlr.org/proceedings/papers/v28/goodfellow13.pdf', papername='Maxout Networks', uncertainty=0.0, venue='ICML 2013')\n",
    "svhn.measure(None, 2.8, 'Stochastic Pooling for Regularization of Deep Convolutional Neural Networks', url='http://arxiv.org/pdf/1301.3557.pdf', papername='Stochastic Pooling for Regularization of Deep Convolutional Neural Networks', uncertainty=0.0, venue='arXiv 2013')\n",
    "svhn.measure(None, 3.96, 'Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network', url='http://arxiv.org/abs/1503.04596', papername='Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network', uncertainty=0.0, venue='arXiv 2015')\n",
    "svhn.measure(None, 4.9, 'Convolutional neural networks applied to house numbers digit classi\ufb01cation', url='http://yann.lecun.com/exdb/publis/pdf/sermanet-icpr-12.pdf', papername='Convolutional neural networks applied to house numbers digit classi\ufb01cation', uncertainty=0.0, venue='ICPR 2012')\n",
    "\n",
    "#Handling 'MNIST' classification_datasets_results.html#4d4e495354\n",
    "mnist.measure(None, 0.21, 'Regularization of Neural Networks using DropConnect', url='http://cs.nyu.edu/~wanli/dropc/', papername='Regularization of Neural Networks using DropConnect', uncertainty=0.0, venue='ICML 2013')\n",
    "mnist.measure(None, 0.23, 'Multi-column Deep Neural Networks for Image Classi\ufb01cation ', url='http://www.idsia.ch/~ciresan/data/cvpr2012.pdf', papername='Multi-column Deep Neural Networks for Image Classi\ufb01cation ', uncertainty=0.0, venue='CVPR 2012')\n",
    "mnist.measure(None, 0.23, 'APAC: Augmented PAttern Classification with Neural Networks', url='http://arxiv.org/abs/1505.03229', papername='APAC: Augmented PAttern Classification with Neural Networks', uncertainty=0.0, venue='arXiv 2015')\n",
    "mnist.measure(None, 0.24, 'Batch-normalized Maxout Network in Network', url='http://arxiv.org/abs/1511.02583', papername='Batch-normalized Maxout Network in Network', uncertainty=0.0, venue='arXiv 2015')\n",
    "mnist.measure(None, 0.29, 'Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree', url='http://arxiv.org/abs/1509.08985', papername='Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree', uncertainty=0.0, venue='AISTATS 2016')\n",
    "mnist.measure(None, 0.31, 'Recurrent Convolutional Neural Network for Object Recognition', url='http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2B_004.pdf', papername='Recurrent Convolutional Neural Network for Object Recognition', uncertainty=0.0, venue='CVPR 2015')\n",
    "mnist.measure(None, 0.31, 'On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units', url='http://arxiv.org/abs/1508.00330', papername='On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units', uncertainty=0.0, venue='arXiv 2015')\n",
    "mnist.measure(None, 0.32, 'Fractional Max-Pooling', url='http://arxiv.org/abs/1412.6071', papername='Fractional Max-Pooling', uncertainty=0.0, venue='arXiv 2015')\n",
    "mnist.measure(None, 0.33, 'Competitive Multi-scale Convolution', url='http://arxiv.org/abs/1511.05635', papername='Competitive Multi-scale Convolution', uncertainty=0.0, venue='arXiv 2015')\n",
    "mnist.measure(None, 0.35, 'Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition', url='http://arxiv.org/pdf/1003.0358.pdf', papername='Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition', uncertainty=0.0, venue='Neural Computation 2010')\n",
    "mnist.measure(None, 0.35, 'C-SVDDNet: An Effective Single-Layer Network for Unsupervised Feature Learning', url='http://arxiv.org/abs/1412.7259', papername='C-SVDDNet: An Effective Single-Layer Network for Unsupervised Feature Learning', uncertainty=0.0, venue='arXiv 2014')\n",
    "mnist.measure(None, 0.37, 'Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network', url='http://arxiv.org/abs/1503.04596', papername='Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network', uncertainty=0.0, venue='arXiv 2015')\n",
    "mnist.measure(None, 0.39, 'Efficient Learning of Sparse Representations with an Energy-Based Model', url='http://papers.nips.cc/paper/3112-efficient-learning-of-sparse-representations-with-an-energy-based-model', papername='Ef\ufb01cient Learning of Sparse Representations with an Energy-Based Model', uncertainty=0.0, venue='NIPS 2006')\n",
    "mnist.measure(None, 0.39, 'Convolutional Kernel Networks', url='http://arxiv.org/abs/1406.3332', papername='Convolutional Kernel Networks', uncertainty=0.0, venue='arXiv 2014')\n",
    "mnist.measure(None, 0.39, 'Deeply-Supervised Nets', url='http://vcl.ucsd.edu/~sxie/2014/09/12/dsn-project/', papername='Deeply-Supervised Nets', uncertainty=0.0, venue='arXiv 2014')\n",
    "mnist.measure(None, 0.4, 'Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis', url='http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=D1C7D701BD39935473808DA5A93426C5?doi=10.1.1.160.8494&rep=rep1&type=pdf', papername='Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis', uncertainty=0.0, venue='Document Analysis and Recognition 2003')\n",
    "mnist.measure(None, 0.4, 'Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to Probe and Learn Neural Networks', url='http://arxiv.org/pdf/1502.00702.pdf', papername='Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to Probe and Learn Neural Networks', uncertainty=0.0, venue=' arXiv 2015')\n",
    "mnist.measure(None, 0.42, 'Multi-Loss Regularized Deep Neural Network', url='http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7258343', papername='Multi-Loss Regularized Deep Neural Network', uncertainty=0.0, venue='CSVT 2015')\n",
    "mnist.measure(None, 0.45, 'Maxout Networks', url='http://jmlr.org/proceedings/papers/v28/goodfellow13.pdf', papername='Maxout Networks', uncertainty=0.0, venue='ICML 2013')\n",
    "mnist.measure(None, 0.45, 'Training Very Deep Networks', url='http://people.idsia.ch/~rupesh/very_deep_learning/', papername='Training Very Deep Networks', uncertainty=0.0, venue='NIPS 2015')\n",
    "mnist.measure(None, 0.45, 'ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks', url='http://arxiv.org/abs/1505.00393', papername='ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks', uncertainty=0.0, venue='arXiv 2015')\n",
    "mnist.measure(None, 0.46, 'Deep Convolutional Neural Networks as Generic Feature Extractors', url='http://www.isip.uni-luebeck.de/fileadmin/uploads/tx_wapublications/hertel_ijcnn_2015.pdf', papername='Deep Convolutional Neural Networks as Generic Feature Extractors', uncertainty=0.0, venue='IJCNN 2015')\n",
    "mnist.measure(None, 0.47, 'Network in Network', url='http://openreview.net/document/9b05a3bb-3a5e-49cb-91f7-0f482af65aea#9b05a3bb-3a5e-49cb-91f7-0f482af65aea', papername='Network in Network', uncertainty=0.0, venue='ICLR 2014')\n",
    "mnist.measure(None, 0.52, 'Trainable COSFIRE filters for keypoint detection and pattern recognition', url='http://iwi.eldoc.ub.rug.nl/FILES/root/2013/IEEETPAMIAzzopardi/2013IEEETPAMIAzzopardi.pdf', papername='Trainable COSFIRE filters for keypoint detection and pattern recognition', uncertainty=0.0, venue='PAMI 2013')\n",
    "#Network error on http://iwi.eldoc.ub.rug.nl/FILES/root/2013/IEEETPAMIAzzopardi/2013IEEETPAMIAzzopardi.pdf (Trainable COSFIRE filters for keypoint detection and pattern recognition), skipping:\n",
    "#HTTPConnectionPool(host='iwi.eldoc.ub.rug.nl', port=80): Max retries exceeded with url: /FILES/root/2013/IEEETPAMIAzzopardi/2013IEEETPAMIAzzopardi.pdf (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7fcdfa474f50>: Failed to establish a new connection: [Errno -5] No address associated with hostname',))\n",
    "mnist.measure(None, 0.53, 'What is the Best Multi-Stage Architecture for Object Recognition?', url='http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf', papername='What is the Best Multi-Stage Architecture for Object Recognition?', uncertainty=0.0, venue='ICCV 2009')\n",
    "mnist.measure(None, 0.54, 'Deformation Models for Image Recognition', url='http://www.keysers.net/daniel/files/Keysers--Deformation-Models--TPAMI2007.pdf', papername='Deformation Models for Image Recognition', uncertainty=0.0, venue='PAMI 2007')\n",
    "mnist.measure(None, 0.54, 'A trainable feature extractor for handwritten digit recognition', url='http://hal.inria.fr/docs/00/05/75/61/PDF/LauerSuenBlochPR.pdf', papername='A trainable feature extractor for handwritten digit recognition', uncertainty=0.0, venue='Journal Pattern Recognition 2007')\n",
    "mnist.measure(None, 0.56, 'Training Invariant Support Vector Machines', url='http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.88.9924&rep=rep1&type=pdf', papername='Training Invariant Support Vector Machines', uncertainty=0.0, venue='Machine Learning 2002')\n",
    "mnist.measure(None, 0.59, 'Simple Methods for High-Performance Digit Recognition Based on Sparse Coding', url='http://www.inb.uni-luebeck.de/publikationen/pdfs/LaBaMa08c.pdf', papername='Simple Methods for High-Performance Digit Recognition Based on Sparse Coding', uncertainty=0.0, venue='TNN 2008')\n",
    "mnist.measure(None, 0.62, 'Unsupervised learning of invariant feature hierarchies with applications to object recognition', url='http://yann.lecun.com/exdb/publis/pdf/ranzato-cvpr-07.pdf', papername='Unsupervised learning of invariant feature hierarchies with applications to object recognition', uncertainty=0.0, venue='CVPR 2007')\n",
    "mnist.measure(None, 0.62, 'PCANet: A Simple Deep Learning Baseline for Image Classification?', url='http://arxiv.org/abs/1404.3606', papername='PCANet: A Simple Deep Learning Baseline for Image Classification?', uncertainty=0.0, venue='arXiv 2014')\n",
    "mnist.measure(None, 0.63, 'Shape matching and object recognition using shape contexts', url='http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=B2AAC2BC3824F19757CAC66986D5F3FF?doi=10.1.1.18.8852&rep=rep1&type=pdf', papername='Shape matching and object recognition using shape contexts', uncertainty=0.0, venue='PAMI 2002')\n",
    "mnist.measure(None, 0.64, 'Beyond Spatial Pyramids: Receptive Field Learning for Pooled Image Features', url='http://www.icsi.berkeley.edu/pubs/vision/beyondspatial12.pdf', papername='Beyond Spatial Pyramids: Receptive Field Learning for Pooled Image Features', uncertainty=0.0, venue='CVPR 2012')\n",
    "mnist.measure(None, 0.68, 'Handwritten Digit Recognition using Convolutional Neural Networks and Gabor Filters', url='http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.133.6559&rep=rep1&type=pdf', papername='Handwritten Digit Recognition using Convolutional Neural Networks and Gabor Filters', uncertainty=0.0, venue='ICCI 2003')\n",
    "mnist.measure(None, 0.69, 'On Optimization Methods for Deep Learning', url='http://ai.stanford.edu/~quocle/LeNgiCoaLahProNg11.pdf', papername='On Optimization Methods for Deep Learning', uncertainty=0.0, venue='ICML 2011')\n",
    "mnist.measure(None, 0.71, 'Deep Fried Convnets', url='http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yang_Deep_Fried_Convnets_ICCV_2015_paper.pdf', papername='Deep Fried Convnets', uncertainty=0.0, venue='ICCV 2015')\n",
    "mnist.measure(None, 0.75, 'Sparse Activity and Sparse Connectivity in Supervised Learning', url='http://jmlr.org/papers/v14/thom13a.html', papername='Sparse Activity and Sparse Connectivity in Supervised Learning', uncertainty=0.0, venue='JMLR 2013')\n",
    "mnist.measure(None, 0.78, 'Explaining and Harnessing Adversarial Examples', url='http://arxiv.org/abs/1412.6572', papername='Explaining and Harnessing Adversarial Examples', uncertainty=0.0, venue='ICLR 2015')\n",
    "mnist.measure(None, 0.82, 'Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations', url=None, papername='Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations', uncertainty=0.0, venue='ICML 2009')\n",
    "mnist.measure(None, 0.84, 'Supervised Translation-Invariant Sparse Coding', url='http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.206.339&rep=rep1&type=pdf', papername='Supervised Translation-Invariant Sparse Coding', uncertainty=0.0, venue='CVPR 2010')\n",
    "mnist.measure(None, 0.94, 'Large-Margin kNN Classification using a Deep Encoder Network', url=None, papername='Large-Margin kNN Classification using a Deep Encoder Network', uncertainty=0.0, venue=' 2009')\n",
    "mnist.measure(None, 0.95, 'Deep Boltzmann Machines', url='http://www.utstat.toronto.edu/~rsalakhu/papers/dbm.pdf', papername='Deep Boltzmann Machines', uncertainty=0.0, venue='AISTATS 2009')\n",
    "mnist.measure(None, 1.01, 'BinaryConnect: Training Deep Neural Networks with binary weights during propagations', url='http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf', papername='BinaryConnect: Training Deep Neural Networks with binary weights during propagations', uncertainty=0.0, venue='NIPS 2015')\n",
    "mnist.measure(None, 1.1, 'StrongNet: mostly unsupervised image recognition with strong neurons', url='http://www.alglib.net/articles/tr-20140813-strongnet.pdf', papername='StrongNet: mostly unsupervised image recognition with strong neurons', uncertainty=0.0, venue='technical report on ALGLIB website 2014')\n",
    "mnist.measure(None, 1.12, 'CS81: Learning words with Deep Belief Networks', url=None, papername='CS81: Learning words with Deep Belief Networks', uncertainty=0.0, venue=' 2008')\n",
    "mnist.measure(None, 1.19, 'Convolutional Neural Networks', url=None, papername='Convolutional Neural Networks', uncertainty=0.0, venue=' 2003')\n",
    "mnist.measure(None, 1.2, 'Reducing the dimensionality of data with neural networks', url=None, papername='Reducing the dimensionality of data with neural networks', uncertainty=0.0, venue=' 2006')\n",
    "mnist.measure(None, 1.4, 'Convolutional Clustering for Unsupervised Learning', url='http://arxiv.org/abs/1511.06241', papername='Convolutional Clustering for Unsupervised Learning', uncertainty=0.0, venue='arXiv 2015')\n",
    "mnist.measure(None, 1.5, 'Deep learning via semi-supervised embedding', url=None, papername='Deep learning via semi-supervised embedding', uncertainty=0.0, venue=' 2008')\n",
    "# This algorithm is so bad at MNIST that including it messes up the graph!\n",
    "# mnist.measure(None, 14.53, 'Deep Representation Learning with Target Coding', url='http://personal.ie.cuhk.edu.hk/~ccloy/files/aaai_2015_target_coding.pdf', papername='Deep Representation Learning with Target Coding', uncertainty=0.0, venue='AAAI 2015')\n",
    "\n",
    "#Handling 'STL-10' classification_datasets_results.html#53544c2d3130\n",
    "stl10.measure(None, 74.33, 'Stacked What-Where Auto-encoders', url='http://arxiv.org/abs/1506.02351', papername='Stacked What-Where Auto-encoders', uncertainty=0.0, venue='arXiv 2015')\n",
    "stl10.measure(None, 74.1, 'Convolutional Clustering for Unsupervised Learning', url='http://arxiv.org/abs/1511.06241', papername='Convolutional Clustering for Unsupervised Learning', uncertainty=0.0, venue='arXiv 2015')\n",
    "stl10.measure(None, 73.15, 'Deep Representation Learning with Target Coding', url='http://personal.ie.cuhk.edu.hk/~ccloy/files/aaai_2015_target_coding.pdf', papername='Deep Representation Learning with Target Coding', uncertainty=0.0, venue='AAAI 2015')\n",
    "stl10.measure(None, 72.8, 'Discriminative Unsupervised Feature Learning with Convolutional Neural Networks', url='http://papers.nips.cc/paper/5548-discriminative-unsupervised-feature-learning-with-convolutional-neural-networks.pdf', papername='Discriminative Unsupervised Feature Learning with Convolutional Neural Networks', uncertainty=0.0, venue='NIPS 2014')\n",
    "stl10.measure(None, 70.2, 'An Analysis of Unsupervised Pre-training in Light of Recent Advances', url='http://arxiv.org/abs/1412.6597', papername='An Analysis of Unsupervised Pre-training in Light of Recent Advances', uncertainty=0.0, venue='ICLR 2015')\n",
    "stl10.measure(None, 70.1, 'Multi-Task Bayesian Optimization', url='http://hips.seas.harvard.edu/files/swersky-multi-nips-2013.pdf', papername='Multi-Task Bayesian Optimization', uncertainty=0.0, venue='NIPS 2013')\n",
    "stl10.measure(None, 68.23, 'C-SVDDNet: An Effective Single-Layer Network for Unsupervised Feature Learning', url='http://arxiv.org/abs/1412.7259', papername='C-SVDDNet: An Effective Single-Layer Network for Unsupervised Feature Learning', uncertainty=0.0, venue='arXiv 2014')\n",
    "stl10.measure(None, 68.0, 'Committees of deep feedforward networks trained with few data', url='http://arxiv.org/abs/1406.5947', papername='Committees of deep feedforward networks trained with few data', uncertainty=0.0, venue='arXiv 2014')\n",
    "stl10.measure(None, 67.9, 'Stable and Efficient Representation Learning with Nonnegativity Constraints ', url='http://jmlr.org/proceedings/papers/v32/line14.pdf', papername='Stable and Efficient Representation Learning with Nonnegativity Constraints ', uncertainty=0.0, venue='ICML 2014')\n",
    "stl10.measure(None, 64.5, 'Unsupervised Feature Learning for RGB-D Based Object Recognition', url='http://homes.cs.washington.edu/~lfb/paper/iser12.pdf', papername='Unsupervised Feature Learning for RGB-D Based Object Recognition', uncertainty=0.0, venue='ISER 2012')\n",
    "stl10.measure(None, 62.32, 'Convolutional Kernel Networks', url='http://arxiv.org/abs/1406.3332', papername='Convolutional Kernel Networks', uncertainty=0.0, venue='arXiv 2014')\n",
    "stl10.measure(None, 62.3, 'Discriminative Learning of Sum-Product Networks', url='http://homes.cs.washington.edu/~rcg/papers/dspn.pdf', papername='Discriminative Learning of Sum-Product Networks', uncertainty=0.0, venue='NIPS 2012')\n",
    "stl10.measure(None, 61.0, 'No more meta-parameter tuning in unsupervised sparse feature learning', url='http://arxiv.org/abs/1402.5766', papername='No more meta-parameter tuning in unsupervised sparse feature learning', uncertainty=0.0, venue='arXiv 2014')\n",
    "stl10.measure(None, 61.0, 'Deep Learning of Invariant Features via Simulated Fixations in Video', url='http://papers.nips.cc/paper/4730-deep-learning-of-invariant-features-via-simulated-fixations-in-video', papername='Deep Learning of Invariant Features via Simulated Fixations in Video', uncertainty=0.0, venue='NIPS 2012 2012')\n",
    "stl10.measure(None, 60.1, 'Selecting Receptive Fields in Deep Networks ', url='http://www.stanford.edu/~acoates/papers/coatesng_nips_2011.pdf', papername='Selecting Receptive Fields in Deep Networks ', uncertainty=0.0, venue='NIPS 2011')\n",
    "stl10.measure(None, 58.7, 'Learning Invariant Representations with Local Transformations', url='http://web.eecs.umich.edu/~honglak/icml12-invariantFeatureLearning.pdf', papername='Learning Invariant Representations with Local Transformations', uncertainty=0.0, venue='ICML 2012')\n",
    "stl10.measure(None, 58.28, 'Pooling-Invariant Image Feature Learning ', url='http://arxiv.org/pdf/1302.5056v1.pdf', papername='Pooling-Invariant Image Feature Learning ', uncertainty=0.0, venue='arXiv 2012')\n",
    "stl10.measure(None, 56.5, 'Deep Learning of Invariant Features via Simulated Fixations in Video', url='http://ai.stanford.edu/~wzou/nips_ZouZhuNgYu12.pdf', papername='Deep Learning of Invariant Features via Simulated Fixations in Video', uncertainty=0.0, venue='NIPS 2012')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cifar100.measure(None, 100 - 22.71, \"ResNet-1001\", url=\"https://arxiv.org/pdf/1603.05027\", uncertainty=0.22)\n",
    "cifar10.measure(None, 100 - 4.62, \"ResNet-1001\", url=\"https://arxiv.org/pdf/1603.05027\", uncertainty=0.20)\n",
    "\n",
    "for m in image_classification.metrics: \n",
    "    if m != imagenet: m.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWTY, not yet imported:\n",
    "\n",
    "```\n",
    "Handling 'Pascal VOC 2011 comp3' detection_datasets_results.html#50617363616c20564f43203230313120636f6d7033\n",
    "Skipping 40.6 mAP Fisher and VLAD with FLAIR CVPR 2014\n",
    "Handling 'Leeds Sport Poses' pose_estimation_datasets_results.html#4c656564732053706f727420506f736573\n",
    "69.2 %                  Strong Appearance and Expressive Spatial Models for Human Pose Estimation  ICCV 2013\n",
    "64.3 %                                    Appearance sharing for collective human pose estimation  ACCV 2012\n",
    "63.3 %                                                   Poselet conditioned pictorial structures  CVPR 2013\n",
    "60.8 %                                Articulated pose estimation with flexible mixtures-of-parts  CVPR 2011\n",
    " 55.6%           Pictorial structures revisited: People detection and articulated pose estimation  CVPR 2009\n",
    "Handling 'Pascal VOC 2007 comp3' detection_datasets_results.html#50617363616c20564f43203230303720636f6d7033\n",
    "Skipping 22.7 mAP Ensemble of Exemplar-SVMs for Object Detection and Beyond ICCV 2011\n",
    "Skipping 27.4 mAP Measuring the objectness of image windows PAMI 2012\n",
    "Skipping 28.7 mAP Automatic discovery of meaningful object parts with latent CRFs CVPR 2010\n",
    "Skipping 29.0 mAP Object Detection with Discriminatively Trained Part Based Models PAMI 2010\n",
    "Skipping 29.6 mAP Latent Hierarchical Structural Learning for Object Detection CVPR 2010\n",
    "Skipping 32.4 mAP Deformable Part Models with Individual Part Scaling BMVC 2013\n",
    "Skipping 34.3 mAP Histograms of Sparse Codes for Object Detection CVPR 2013\n",
    "Skipping 34.3 mAP Boosted local structured HOG-LBP for object localization CVPR 2011\n",
    "Skipping 34.7 mAP Discriminatively Trained And-Or Tree Models for Object Detection CVPR 2013\n",
    "Skipping 34.7 mAP Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection CVPR 2013\n",
    "Skipping 34.8 mAP Color Attributes for Object Detection CVPR 2012\n",
    "Skipping 35.4 mAP Object Detection with Discriminatively Trained Part Based Models PAMI 2010\n",
    "Skipping 36.0 mAP Machine Learning Methods for Visual Object Detection archives-ouvertes 2011\n",
    "Skipping 38.7 mAP Detection Evolution with Multi-Order Contextual Co-occurrence CVPR 2013\n",
    "Skipping 40.5 mAP Segmentation Driven Object Detection with Fisher Vectors ICCV 2013\n",
    "Skipping 41.7 mAP Regionlets for Generic Object Detection ICCV 2013\n",
    "Skipping 43.7 mAP Beyond Bounding-Boxes: Learning Object Shape by Model-Driven Grouping ECCV 2012\n",
    "Handling 'Pascal VOC 2007 comp4' detection_datasets_results.html#50617363616c20564f43203230303720636f6d7034\n",
    "Skipping 59.2 mAP Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition ECCV 2014\n",
    "Skipping 58.5 mAP Rich feature hierarchies for accurate object detection and semantic segmentation CVPR 2014\n",
    "Skipping 29.0 mAP Multi-Component Models for Object Detection ECCV 2012\n",
    "Handling 'Pascal VOC 2010 comp3' detection_datasets_results.html#50617363616c20564f43203230313020636f6d7033\n",
    "Skipping 24.98 mAP Learning Collections of Part Models for Object Recognition CVPR 2013\n",
    "Skipping 29.4 mAP Discriminatively Trained And-Or Tree Models for Object Detection CVPR 2013\n",
    "Skipping 33.4 mAP Object Detection with Discriminatively Trained Part Based Models PAMI 2010\n",
    "Skipping 34.1 mAP Segmentation as selective search for object recognition ICCV 2011\n",
    "Skipping 35.1 mAP Selective Search for Object Recognition IJCV 2013\n",
    "Skipping 36.0 mAP Latent Hierarchical Structural Learning for Object  Detection CVPR 2010\n",
    "Skipping 36.8 mAP Object Detection by Context and Boosted HOG-LBP ECCV 2010\n",
    "Skipping 38.4 mAP Segmentation Driven Object Detection with Fisher Vectors ICCV 2013\n",
    "Skipping 39.7 mAP Regionlets for Generic Object Detection ICCV 2013\n",
    "Skipping 40.4 mAP Fisher and VLAD with FLAIR CVPR 2014\n",
    "Handling 'Pascal VOC 2010 comp4' detection_datasets_results.html#50617363616c20564f43203230313020636f6d7034\n",
    "Skipping 53.7 mAP Rich feature hierarchies for accurate object detection and semantic segmentation CVPR 2014\n",
    "Skipping 40.4 mAP Bottom-up Segmentation for Top-down Detection CVPR 2013\n",
    "Skipping 33.1 mAP Multi-Component Models for Object Detection ECCV 2012\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Game Playing\n",
    "\n",
    "In principle, games are a sufficiently open-ended framework that all of intelligence could be captured within them. We can imagine a \"ladder of games\" which grow in sophistication and complexity, from simple strategy and arcade games to others which require very sophisticated language, world-modelling, vision and reasoning ability. At present, published reinforcement agents are climbing the first few rungs of this ladder.\n",
    "\n",
    "\n",
    "## Abstract Strategy Games\n",
    "\n",
    "As an easier case, abstract games like chess, go, checkers etc can be played with no knowldege of the human world or physics. Although this domain has largely been solved to super-human performance levels, there are a few ends that need to be tied up, especially in terms of having agents learn rules for arbitrary abstract games effectively given various plausible starting points (eg, textual descriptions of the rules or examples of correct play)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "abstract_strategy_games = Problem(\"Abstract strategy games\", [\"agi\", \"abstract-games\"])\n",
    "\n",
    "playing_with_hints = Problem(\"Playing abstract games with extensive hints\", [\"abstract-games\"], solved=True)\n",
    "abstract_strategy_games.add_subproblem(playing_with_hints)\n",
    "playing_with_hints.notes = \"\"\"\n",
    "  Complex abstract strategy games have been solved to super-human levels\n",
    "  by computer systems with extensive rule-hinting and heuristics,\n",
    "  in some cases combined with machine learning techniques.\n",
    "\"\"\"\n",
    "computer_chess = playing_with_hints.metric(\"computer chess\", scale=elo, target=2882, target_label=\"Best human play\", target_source=\"https://en.wikipedia.org/w/index.php?title=Comparison_of_top_chess_players_throughout_history&oldid=777500496#Elo_system\")\n",
    "computer_go = playing_with_hints.metric(\"computer go\", scale=elo, target=3632, target_label=\"Best human play\", target_source=\"https://www.goratings.org/en/history/\")\n",
    "computer_go.solved = True # until we get proper data\n",
    "\n",
    "# For some caveats, see https://en.wikipedia.org/w/index.php?title=Chess_engine&oldid=764341963#Ratings\n",
    "# We could script ingestion of data from CCRL, or get data from Katja\n",
    "computer_chess.measure(date(1997,05,11), 2725, \"Deep Blue\", uncertainty=25, url=\"https://www.quora.com/What-was-Deep-Blues-Elo-rating\")\n",
    "computer_chess.measure(date(2006,05,27), 2995, \"Rybka 1.1 64bit\", uncertainty=25, url=\"https://web.archive.org/web/20060531091049/http://www.computerchess.org.uk/ccrl/4040/rating_list_all.html\")\n",
    "computer_chess.measure(date(2010,8,7), 3269, \"Rybka 4 64bit\", uncertainty=22, url=\"https://web.archive.org/web/20100923131123/http://www.computerchess.org.uk/ccrl/4040/rating_list_all.html\")\n",
    "computer_chess.measure(date(2013,7,20), 3248, \"Houdini 3 64bit\", uncertainty=16, url=\"https://web.archive.org/web/20130415000000*/http://www.computerchess.org.uk/ccrl/4040/rating_list_all.html\")\n",
    "computer_chess.measure(date(2015,7,4), 3332, \"Komodo 9\", uncertainty=24, url=\"https://web.archive.org/web/20150708104805/http://www.computerchess.org.uk/ccrl/4040/rating_list_all.html\")\n",
    "computer_chess.measure(date(2017,02,27), 3393, \"Stockfish\", uncertainty=50, url=\"https://web.archive.org/web/20170227044521/http://www.computerchess.org.uk/ccrl/4040/\")\n",
    "# Wikipedia has some nice data here:\n",
    "computer_chess.measure(date(1984,12,31), 1631, \"Novag Super Constellation 6502 4 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1985,12,31), 1827, \"Mephisto Amsterdam 68000 12 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1986,12,31), 1827, \"Mephisto Amsterdam 68000 12 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1987,12,31), 1923, \"Mephisto Dallas 68020 14 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1988,12,31), 1993, \"Mephisto MM 4 Turbo Kit 6502 16 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1989,12,31), 2027, \"Mephisto Portorose 68020 12 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1990,12,31), 2138, \"Mephisto Portorose 68030 36 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1991,12,31), 2127, \"Mephisto Vancouver 68030 36 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1992,12,31), 2174, \"Chess Machine Schroder 3.0 ARM2 30 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1993,12,31), 2235, \"Mephisto Genius 2.0 486/50-66 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1995,12,31), 2306, \"MChess Pro 5.0 Pentium 90 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1996,12,31), 2337, \"Rebel 8.0 Pentium 90 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1997,12,31), 2418, \"HIARCS 6.0 49MB P200 MMX\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1998,12,31), 2460, \"Fritz 5.0 PB29% 67MB P200 MMX\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(1999,12,31), 2594, \"Chess Tiger 12.0 DOS 128MB K6-2 450 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2000,12,31), 2607, \"Fritz 6.0 128MB K6-2 450 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2001,12,31), 2709, \"Chess Tiger 14.0 CB 256MB Athlon 1200\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2002,12,31), 2759, \"Deep Fritz 7.0 256MB Athlon 1200 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2003,12,31), 2791, \"Shredder 7.04 UCI 256MB Athlon 1200 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2004,12,31), 2800, \"Shredder 8.0 CB 256MB Athlon 1200 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2005,12,31), 2808, \"Shredder 9.0 UCI 256MB Athlon 1200 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2006,12,31), 2902, \"Rybka 1.2 256MB Athlon 1200 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2007,12,31), 2935, \"Rybka 2.3.1 Arena 256MB Athlon 1200 MHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2008,12,31), 3238, \"Deep Rybka 3 2GB Q6600 2.4 GHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2009,12,31), 3232, \"Deep Rybka 3 2GB Q6600 2.4 GHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2010,12,31), 3227, \"Deep Rybka 3 2GB Q6600 2.4 GHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2011,12,31), 3216, \"Deep Rybka 4 2GB Q6600 2.4 GHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2012,12,31), 3221, \"Deep Rybka 4 x64 2GB Q6600 2.4 GHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2013,12,31), 3241, \"Komodo 5.1 MP x64 2GB Q6600 2.4 GHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2014,12,31), 3295, \"Komodo 7.0 MP x64 2GB Q6600 2.4 GHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2015,12,31), 3334, \"Stockfish 6 MP x64 2GB Q6600 2.4 GHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "computer_chess.measure(date(2016,12,31), 3366, \"Komodo 9.1 MP x64 2GB Q6600 2.4 GHz\", url=\"https://en.wikipedia.org/wiki/Swedish_Chess_Computer_Association#Rating_list_year-end_leaders\")\n",
    "\n",
    "mastering_historical_games = Problem(\"Superhuman mastery of arbitrary abstract strategy games\", [\"super\", \"abstract-games\"])\n",
    "abstract_strategy_games.add_subproblem(mastering_historical_games)\n",
    "mastering_chess = mastering_historical_games.metric(\"mastering chess\")\n",
    "mastering_chess.notes = \"\"\"\n",
    "  Beating all humans at chess, given a corpus of past play amongst masters,\n",
    "  but no human-crafted policy constraints and heuristics. This will probably fall out\n",
    "  immediately once learning_abstract_game_rules is solved, since playing_with_hints\n",
    "  has been solved.\n",
    "\"\"\"\n",
    "\n",
    "# Are there any published metrics for these yet?\n",
    "learning_abstract_game_rules = Problem(\"Learning the rules of complex strategy games from examples\", [\"agi\", \"abstract-games\"])\n",
    "abstract_strategy_games.add_subproblem(learning_abstract_game_rules)\n",
    "learning_chess = learning_abstract_game_rules.metric(\"learning chess\")\n",
    "learning_chess.notes = \"\"\"\n",
    "  Chess software contains hard-coded policy constraints for valid play; this metric is whether RL\n",
    "  or other agents can correctly build those policy constraints from examples or oracles\"\"\"\n",
    "learning_go = learning_abstract_game_rules.metric(\"learning go\")\n",
    "learning_go.notes = \"\"\"\n",
    "  Go software contains policy constraints for valid play and evaluating the number of\n",
    "  liberties for groups. This metric is whether RL or other agents can correctly build those \n",
    "  policy constraints from examples or oracles\"\"\"\n",
    "\n",
    "learning_arbitrary_abstract_games = Problem(\"Play an arbitrary abstract game, first learning the rules\", [\"agi\", \"abstract-games\"])\n",
    "abstract_strategy_games.add_subproblem(learning_arbitrary_abstract_games)\n",
    "computer_chess.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time video games\n",
    "\n",
    "Computer and video games are a very open-ended domain. It is possible that some existing or future games could be so elaborate that they are \"AI complete\". In the mean time, a lot of interesting progress is likely in exploring the \"ladder of games\" of increasing complexity on various fronts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "computer_games = Problem(\"Play real-time computer & video games\", [\"world-modelling\", \"realtime-games\", \"agi\", \"language\"])\n",
    "\n",
    "games_requiring_novel_language = Problem(\"Games that require inventing novel language, forms of speech, or communication\")\n",
    "games_requiring_speech = Problem(\"Games that require both understanding and speaking a language\")\n",
    "games_requiring_speech.metric(\"Starcraft\")\n",
    "\n",
    "games_requiring_language_comprehension = Problem(\"Games that require language comprehension\", [\"agi\", \"languge\"])\n",
    "\n",
    "computer_games.add_subproblem(games_requiring_novel_language)\n",
    "games_requiring_novel_language.add_subproblem(games_requiring_speech)\n",
    "games_requiring_speech.add_subproblem(games_requiring_language_comprehension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Atari 2600 Games: Breakout, Enduro, Pong, Q*Bert, Seaquest, S. Invaders. Each game has its own metric.\n",
    "# Compiled by Yomna Nasser and Miles Brundage\n",
    "\n",
    "simple_games = Problem(\"Simple video games\", [\"world-modelling\", \"realtime-games\", \"agi\"]) \n",
    "computer_games.add_subproblem(simple_games)\n",
    "\n",
    "# Alien\n",
    "alien_metric = simple_games.metric(\"Atari 2600 Alien\", scale=atari_linear, target=6875, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "alien_metric.measure(date(2015, 2, 26), 3069, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "alien_metric.measure(date(2015,11,20), 1620, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "alien_metric.measure(date(2015,11,20), 3747.7, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "alien_metric.measure(date(2015,11,20), 4461.4, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Amidar\n",
    "amidar_metric = simple_games.metric(\"Atari 2600 Amidar\", scale=atari_linear, target=1676, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "amidar_metric.measure(date(2015, 2, 26), 739.5, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "amidar_metric.measure(date(2015,11,20), 978, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "amidar_metric.measure(date(2015,11,20), 1793.3, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "amidar_metric.measure(date(2015,11,20), 2354.5, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Assault\n",
    "assault_metric = simple_games.metric(\"Atari 2600 Assault\", scale=atari_linear, target=1496, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "assault_metric.measure(date(2015, 2, 26), 3359, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "assault_metric.measure(date(2015,11,20), 4280.4, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "assault_metric.measure(date(2015,11,20), 5393.2, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "assault_metric.measure(date(2015,11,20), 4621.0, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Asterix\n",
    "asterix_metric = simple_games.metric(\"Atari 2600 Asterix\", scale=atari_linear, target=8503, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "asterix_metric.measure(date(2015, 2, 26), 6012, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "asterix_metric.measure(date(2015,11,20), 4359, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "asterix_metric.measure(date(2015,11,20), 17356, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "asterix_metric.measure(date(2015,11,20), 28188, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Asteroids\n",
    "asteroids_metric = simple_games.metric(\"Atari 2600 Asteroids\", scale=atari_linear, target=13157, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "asteroids_metric.measure(date(2015, 2, 26), 1629, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "asteroids_metric.measure(date(2015,11,20), 1364.5, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "asteroids_metric.measure(date(2015,11,20), 734.7, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "asteroids_metric.measure(date(2015,11,20), 2837.7, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Atlantis\n",
    "atlantis_metric = simple_games.metric(\"Atari 2600 Atlantis\", scale=atari_linear, target=29028, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "atlantis_metric.measure(date(2015, 2, 26), 85641, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "atlantis_metric.measure(date(2015,11,20), 279987, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "atlantis_metric.measure(date(2015,11,20), 106056, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "atlantis_metric.measure(date(2015,11,20), 382572, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Bank Heist\n",
    "bank_heist_metric = simple_games.metric(\"Atari 2600 Bank Heist\", scale=atari_linear, target=734.4, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "bank_heist_metric.measure(date(2015, 2, 26), 429.7, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "bank_heist_metric.measure(date(2015,11,20), 455, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "bank_heist_metric.measure(date(2015,11,20), 1030.6, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "bank_heist_metric.measure(date(2015,11,20), 1611.9, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Battle Zone\n",
    "battle_zone_metric = simple_games.metric(\"Atari 2600 Battle Zone\", scale=atari_linear, target=37800, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "battle_zone_metric.measure(date(2015, 2, 26), 26300, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "battle_zone_metric.measure(date(2015,11,20), 29900, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "battle_zone_metric.measure(date(2015,11,20), 31700, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "battle_zone_metric.measure(date(2015,11,20), 37150, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Beam Rider\n",
    "beam_rider_metric = simple_games.metric(\"Atari 2600 Beam Rider\", scale=atari_linear, target=7456, target_source=\"https://arxiv.org/pdf/1312.5602.pdf\")\n",
    "beam_rider_metric.measure(date(2013,12,19), 4092, \"DQN\", \"https://arxiv.org/pdf/1312.5602.pdf\")\n",
    "beam_rider_metric.measure(date(2015,11,20), 8627, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "beam_rider_metric.measure(date(2015,11,20), 13772, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "beam_rider_metric.measure(date(2015,11,20), 12164, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Berzerk \n",
    "berzerk_metric = simple_games.metric(\"Atari 2600 Berzerk\", scale=atari_linear, target=2630.4, target_source=\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "berzerk_metric.measure(date(2015,11,20), 585, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "berzerk_metric.measure(date(2015,11,20), 1225, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "berzerk_metric.measure(date(2015,11,20), 1472, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Bowling\n",
    "bowling_metric = simple_games.metric(\"Atari 2600 Bowling\", scale=atari_linear, target=154.8, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "bowling_metric.measure(date(2015, 2, 26), 42.8, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "bowling_metric.measure(date(2015,11,20), 50.4, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "bowling_metric.measure(date(2015,11,20), 68.1, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "bowling_metric.measure(date(2015,11,20), 65.5, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Boxing\n",
    "boxing_metric = simple_games.metric(\"Atari 2600 Boxing\", scale=atari_linear, target=4.3, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "boxing_metric.measure(date(2015, 2, 26), 71.8, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "boxing_metric.measure(date(2015,11,20), 88, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "boxing_metric.measure(date(2015,11,20), 91.6, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "boxing_metric.measure(date(2015,11,20), 99.4, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Breakout\n",
    "breakout_metric = simple_games.metric(\"Atari 2600 Breakout\", scale=atari_linear, target=31.8, target_source=\"https://pdfs.semanticscholar.org/340f/48901f72278f6bf78a04ee5b01df208cc508.pdf\")\n",
    "breakout_metric.measure(date(2013,12,19), 225, \"DQN\", \"https://arxiv.org/pdf/1312.5602.pdf\")\n",
    "breakout_metric.measure(date(2015,2,26), 401.2, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "breakout_metric.measure(date(2015,10,22), 375, \"DoubleDQN\", \"https://pdfs.semanticscholar.org/3b97/32bb07dc99bde5e1f9f75251c6ea5039373e.pdf\")\n",
    "\n",
    "breakout_metric.measure(date(2015,11,20), 385.5, \"DQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "breakout_metric.measure(date(2015,11,20), 418.5, \"DDQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "breakout_metric.measure(date(2015,11,20), 345.3, \"Duel\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "breakout_metric.measure(date(2016,6,16), 766.8, \"A3C LSTM\", \"https://arxiv.org/pdf/1602.01783.pdf\")\n",
    "\n",
    "# Centipede\n",
    "centipede_metric = simple_games.metric(\"Atari 2600 Centipede\", scale=atari_linear, target=11963, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "centipede_metric.measure(date(2015, 2, 26), 8309, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "centipede_metric.measure(date(2015,11,20), 4657, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "centipede_metric.measure(date(2015,11,20), 5409, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "centipede_metric.measure(date(2015,11,20), 7561, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Chopper Command\n",
    "chopper_command_metric = simple_games.metric(\"Atari 2600 Chopper Command\", scale=atari_linear, target=9882, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "chopper_command_metric.measure(date(2015, 2, 26), 6687, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "chopper_command_metric.measure(date(2015,11,20), 6126, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "chopper_command_metric.measure(date(2015,11,20), 5809, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "chopper_command_metric.measure(date(2015,11,20), 11215, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Crazy Climber\n",
    "crazy_climber_metric = simple_games.metric(\"Atari 2600 Crazy Climber\", scale=atari_linear, target=35411, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "crazy_climber_metric.measure(date(2015, 2, 26), 114103, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "crazy_climber_metric.measure(date(2015,11,20), 110763, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "crazy_climber_metric.measure(date(2015,11,20), 117282, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "crazy_climber_metric.measure(date(2015,11,20), 143570, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Demon Attack\n",
    "demon_attack_metric = simple_games.metric(\"Atari 2600 Demon Attack\", scale=atari_linear, target=3401, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "demon_attack_metric.measure(date(2015, 2, 26), 9711, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "demon_attack_metric.measure(date(2015,11,20), 12149, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "demon_attack_metric.measure(date(2015,11,20), 58044, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "demon_attack_metric.measure(date(2015,11,20), 60813, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Double Dunk\n",
    "# TODO: investigate alternate scale\n",
    "double_dunk_metric = simple_games.metric(\"Atari 2600 Double Dunk\", scale=atari_linear, target=-15.5, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "double_dunk_metric.measure(date(2015, 2, 26), -18.1, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "double_dunk_metric.measure(date(2015,11,20), -6.6, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "double_dunk_metric.measure(date(2015,11,20), -5.5, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "double_dunk_metric.measure(date(2015,11,20), 0.1, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Enduro\n",
    "enduro_metric = simple_games.metric(\"Atari 2600 Enduro\", scale=atari_linear, target=309.6, target_source=\"https://pdfs.semanticscholar.org/340f/48901f72278f6bf78a04ee5b01df208cc508.pdf\")\n",
    "enduro_metric.measure(date(2013,12,19), 661, \"DQN\", \"https://arxiv.org/pdf/1312.5602.pdf\")\n",
    "enduro_metric.measure(date(2015,2,26), 301, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "enduro_metric.measure(date(2015,10,22), 319, \"DoubleDQN\", \"https://pdfs.semanticscholar.org/3b97/32bb07dc99bde5e1f9f75251c6ea5039373e.pdf\")\n",
    "\n",
    "enduro_metric.measure(date(2015,11,20), 729.0, \"DQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "enduro_metric.measure(date(2015,11,20), 1211.8, \"DDQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "enduro_metric.measure(date(2015,11,20), 2258.2, \"Duel\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "enduro_metric.measure(date(2016,6,16), 82.5, \"A3C LSTM\", \"https://arxiv.org/pdf/1602.01783.pdf\")\n",
    "\n",
    "# Fishing Derby\n",
    "fishing_derby_metric = simple_games.metric(\"Atari 2600 Fishing Derby\", scale=atari_linear, target=5.5, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "fishing_derby_metric.measure(date(2015, 2, 26), -0.8, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "fishing_derby_metric.measure(date(2015,11,20), -4.9, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "fishing_derby_metric.measure(date(2015,11,20), 15.5, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "fishing_derby_metric.measure(date(2015,11,20), 46.4, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Freeway\n",
    "freeway_metric = simple_games.metric(\"Atari 2600 Freeway\", scale=atari_linear, target=29.6, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "freeway_metric.measure(date(2015, 2, 26), 30.3, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "freeway_metric.measure(date(2015,11,20), 30.8, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "freeway_metric.measure(date(2015,11,20), 33.3, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "freeway_metric.measure(date(2015,11,20), 0, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Frostbite\n",
    "frostbite_metric = simple_games.metric(\"Atari 2600 Frostbite\", scale=atari_linear, target=4355, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "frostbite_metric.measure(date(2015, 2, 26), 328.3, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "frostbite_metric.measure(date(2015,11,20), 797.7, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "frostbite_metric.measure(date(2015,11,20), 1683.3, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "frostbite_metric.measure(date(2015,11,20), 4672, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Gopher\n",
    "gopher_metric = simple_games.metric(\"Atari 2600 Gopher\", scale=atari_linear, target=2321, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "gopher_metric.measure(date(2015, 2, 26), 8520, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "gopher_metric.measure(date(2015,11,20), 8777, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "gopher_metric.measure(date(2015,11,20), 14840, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "gopher_metric.measure(date(2015,11,20), 15718, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Gravitar\n",
    "gravitar_metric = simple_games.metric(\"Atari 2600 Gravitar\", scale=atari_linear, target=2672, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "gravitar_metric.measure(date(2015, 2, 26), 306.7, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "gravitar_metric.measure(date(2015,11,20), 473, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "gravitar_metric.measure(date(2015,11,20), 412, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "gravitar_metric.measure(date(2015,11,20), 588, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# H.E.R.O.\n",
    "hero_metric = simple_games.metric(\"Atari 2600 HERO\", scale=atari_linear, target=25763, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "hero_metric.measure(date(2015, 2, 26), 19950, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "hero_metric.measure(date(2015,11,20), 20437, \"DQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "hero_metric.measure(date(2015,11,20), 20130, \"DDQN\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "hero_metric.measure(date(2015,11,20), 20818, \"Duel\",\"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "# Ice Hockey\n",
    "ice_hockey_metric = simple_games.metric(\"Atari 2600 Ice Hockey\", scale=atari_linear, target=0.9, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "ice_hockey_metric.measure(date(2015, 2, 26), -1.6, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# James Bond\n",
    "james_bond_metric = simple_games.metric(\"Atari 2600 James Bond\", scale=atari_linear, target=406.7, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "james_bond_metric.measure(date(2015, 2, 26), 576.7, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Kangaroo\n",
    "kangaroo_metric = simple_games.metric(\"Atari 2600 Kangaroo\", scale=atari_linear, target=3035, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "kangaroo_metric.measure(date(2015, 2, 26), 6740, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Krull\n",
    "krull_metric = simple_games.metric(\"Atari 2600 Krull\", scale=atari_linear, target=2395, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "krull_metric.measure(date(2015, 2, 26), 3805, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Kung-Fu Master\n",
    "kung_fu_master_metric = simple_games.metric(\"Atari 2600 Kung-Fu Master\", scale=atari_linear, target=22736, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "kung_fu_master_metric.measure(date(2015, 2, 26), 23270, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Montezuma's Revenge\n",
    "montezumas_revenge_metric = simple_games.metric(\"Atari 2600 Montezuma's Revenge\", scale=atari_linear, target=4367, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "montezumas_revenge_metric.measure(date(2015, 2, 26), 0, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Ms. Pacman\n",
    "ms_pacman_metric = simple_games.metric(\"Atari 2600 Ms. Pacman\", scale=atari_linear, target=15693, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "ms_pacman_metric.measure(date(2015, 2, 26), 2311, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Name This Game\n",
    "name_this_game_metric = simple_games.metric(\"Atari 2600 Name This Game\", scale=atari_linear, target=4076, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "name_this_game_metric.measure(date(2015, 2, 26), 7257, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Pong\n",
    "pong_metric = simple_games.metric(\"Atari 2600 Pong\", scale=atari_linear, target=9.3, target_source=\"https://pdfs.semanticscholar.org/340f/48901f72278f6bf78a04ee5b01df208cc508.pdf\")\n",
    "pong_metric.measure(date(2013,12,19), 21, \"DQN\", \"https://arxiv.org/pdf/1312.5602.pdf\")\n",
    "pong_metric.measure(date(2015,2,26), 18.9, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "pong_metric.measure(date(2015,10,22), 21, \"DoubleDQN\", \"https://pdfs.semanticscholar.org/3b97/32bb07dc99bde5e1f9f75251c6ea5039373e.pdf\")\n",
    "\n",
    "pong_metric.measure(date(2015,11,20), 21, \"DQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf?_ga=1.123524811.1334652001.1475539859\")\n",
    "pong_metric.measure(date(2015,11,20), 20.9, \"DDQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf?_ga=1.123524811.1334652001.1475539859\")\n",
    "pong_metric.measure(date(2015,11,20), 19.5, \"Duel\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf?_ga=1.123524811.1334652001.1475539859\")\n",
    "\n",
    "pong_metric.measure(date(2016,6,16), 10.7, \"A3C LSTM\", \"https://arxiv.org/pdf/1602.01783.pdf\")\n",
    "\n",
    "# Private Eye\n",
    "private_eye_metric = simple_games.metric(\"Atari 2600 Private Eye\", scale=atari_linear, target=69571, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "private_eye_metric.measure(date(2015, 2, 26), 1788, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Q*Bert\n",
    "q_bert_metric = simple_games.metric(\"Atari 2600 Q*Bert\", scale=atari_linear, target=13455, target_source=\"https://pdfs.semanticscholar.org/340f/48901f72278f6bf78a04ee5b01df208cc508.pdf\")\n",
    "q_bert_metric.measure(date(2013,12,19), 4500, \"DQN\", \"https://arxiv.org/pdf/1312.5602.pdf\")\n",
    "q_bert_metric.measure(date(2015,2,26), 10596, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "q_bert_metric.measure(date(2015,10,22), 14875, \"DoubleDQN\", \"https://pdfs.semanticscholar.org/3b97/32bb07dc99bde5e1f9f75251c6ea5039373e.pdf\")\n",
    "\n",
    "q_bert_metric.measure(date(2015,11,20), 13117.3, \"DQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "q_bert_metric.measure(date(2015,11,20), 15088.5, \"DDQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "q_bert_metric.measure(date(2015,11,20), 19220, \"Duel\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf\")\n",
    "\n",
    "q_bert_metric.measure(date(2016,6,16), 21307, \"A3C LSTM\", \"https://arxiv.org/pdf/1602.01783.pdf\")\n",
    "\n",
    "# River Raid\n",
    "river_raid_metric = simple_games.metric(\"Atari 2600 River Raid\", scale=atari_linear, target=13513, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "river_raid_metric.measure(date(2015, 2, 26), 8316, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Road Runner\n",
    "road_runner_metric = simple_games.metric(\"Atari 2600 Road Runner\", scale=atari_linear, target=7845, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "road_runner_metric.measure(date(2015, 2, 26), 18257, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Robotank\n",
    "robotank_metric = simple_games.metric(\"Atari 2600 Robotank\", scale=atari_linear, target=11.9, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "robotank_metric.measure(date(2015, 2, 26), 51.6, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Seaquest\n",
    "seaquest_metric = simple_games.metric(\"Atari 2600 Seaquest\", scale=atari_linear, target=20182, target_source=\"https://pdfs.semanticscholar.org/340f/48901f72278f6bf78a04ee5b01df208cc508.pdf\")\n",
    "seaquest_metric.measure(date(2013,12,19), 1740, \"DQN\", \"https://arxiv.org/pdf/1312.5602.pdf\")\n",
    "seaquest_metric.measure(date(2015,2,26), 5286, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "seaquest_metric.measure(date(2015,10,22), 7995, \"DoubleDQN\", \"https://pdfs.semanticscholar.org/3b97/32bb07dc99bde5e1f9f75251c6ea5039373e.pdf?_ga=1.165640319.1334652001.1475539859\")\n",
    "\n",
    "seaquest_metric.measure(date(2015,11,20), 5860.6, \"DQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf?_ga=1.123524811.1334652001.1475539859\")\n",
    "seaquest_metric.measure(date(2015,11,20), 16452.7, \"DDQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf?_ga=1.123524811.1334652001.1475539859\")\n",
    "seaquest_metric.measure(date(2015,11,20), 50254.2, \"Duel\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf?_ga=1.123524811.1334652001.1475539859\")\n",
    "\n",
    "seaquest_metric.measure(date(2016,6,16), 1326.1, \"A3C LSTM\", \"https://arxiv.org/pdf/1602.01783.pdf\")\n",
    "\n",
    "# Space Invaders\n",
    "space_invaders_metric = simple_games.metric(\"Atari 2600 Space Invaders\", scale=atari_linear, target=1652, target_source=\"https://pdfs.semanticscholar.org/340f/48901f72278f6bf78a04ee5b01df208cc508.pdf\")\n",
    "space_invaders_metric.measure(date(2013,12,19), 1075, \"DQN\", \"https://arxiv.org/pdf/1312.5602.pdf\")\n",
    "space_invaders_metric.measure(date(2015,2,26), 1976, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "space_invaders_metric.measure(date(2015,10,22), 3154, \"DoubleDQN\", \"https://pdfs.semanticscholar.org/3b97/32bb07dc99bde5e1f9f75251c6ea5039373e.pdf?_ga=1.165640319.1334652001.1475539859\")\n",
    "\n",
    "space_invaders_metric.measure(date(2015,11,20), 1692.3, \"DQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf?_ga=1.123524811.1334652001.1475539859\")\n",
    "space_invaders_metric.measure(date(2015,11,20), 2525.5, \"DDQN\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf?_ga=1.123524811.1334652001.1475539859\")\n",
    "space_invaders_metric.measure(date(2015,11,20), 6427.3, \"Duel\", \"https://pdfs.semanticscholar.org/13b5/8f3108709dbbed5588759bc0496f82a261c4.pdf?_ga=1.123524811.1334652001.1475539859\")\n",
    "\n",
    "space_invaders_metric.measure(date(2016,6,16), 23846, \"A3C LSTM\", \"https://arxiv.org/pdf/1602.01783.pdf\")\n",
    "\n",
    "# Star Gunner\n",
    "star_gunner_metric = simple_games.metric(\"Atari 2600 Star Gunner\", scale=atari_linear, target=10250, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "star_gunner_metric.measure(date(2015, 2, 26), 57997, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Tennis\n",
    "# TODO: negative linear scale?\n",
    "tennis_metric = simple_games.metric(\"Atari 2600 Tennis\", scale=atari_linear, target=-8.9, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "tennis_metric.measure(date(2015, 2, 26), -2.5, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Time Pilot\n",
    "time_pilot_metric = simple_games.metric(\"Atari 2600 Time Pilot\", scale=atari_linear, target=5925, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "time_pilot_metric.measure(date(2015, 2, 26), 5947, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Tutankham\n",
    "tutankham_metric = simple_games.metric(\"Atari 2600 Tutankham\", scale=atari_linear, target=167.6, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "tutankham_metric.measure(date(2015, 2, 26), 186.7, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Up and Down\n",
    "up_and_down_metric = simple_games.metric(\"Atari 2600 Up and Down\", scale=atari_linear, target=9082, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "up_and_down_metric.measure(date(2015, 2, 26), 8456, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Venture\n",
    "venture_metric = simple_games.metric(\"Atari 2600 Venture\", scale=atari_linear, target=1188, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "venture_metric.measure(date(2015, 2, 26), 3800, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Video Pinball\n",
    "video_pinball_metric = simple_games.metric(\"Atari 2600 Video Pinball\", scale=atari_linear, target=17298, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "video_pinball_metric.measure(date(2015, 2, 26), 42684, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Wizard of Wor\n",
    "wizard_of_wor_metric = simple_games.metric(\"Atari 2600 Wizard of Wor\", scale=atari_linear, target=4757, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "wizard_of_wor_metric.measure(date(2015, 2, 26), 3393, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "# Zaxxon\n",
    "zaxxon_metric = simple_games.metric(\"Atari 2600 Zaxxon\", scale=atari_linear, target=9173, target_source=\"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "zaxxon_metric.measure(date(2015, 2, 26), 4977, \"DQN\", \"https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/340f48901f72278f6bf78a04ee5b01df208cc508\")\n",
    "\n",
    "for m in simple_games.metrics: m.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        http://melodi.ee.washington.edu/s3tp/\n",
    "\n",
    "* * *\n",
    "**_Word error rate on Switchboard (specify details): [Month, Year: Score [SWB]: Team].  Compiled by Jack Clark._**\n",
    "\n",
    "A note about measurement: We're measuring Switchboard (SWB) and Call Home (CH) performance (mostly) from the Hub5'00 dataset, with main scores assesses in terms of word error rate on SWB. We also create \n",
    "\n",
    "Why do we care: Reflects the improvement of audio processing systems on speech over time.\n",
    "\n",
    "\"\"\"\n",
    "speech_recognition = Problem(name=\"Speech Recognition\", attributes=[\"language\", \"agi\"])\n",
    "swb_hub_500 = speech_recognition.metric(name=\"Word error rate on Switchboard trained against the Hub5'00 dataset\",\n",
    "                                               scale=error_percent, target=5.9)\n",
    "swb_hub_500.measure(date(2011,8,31), 16.1, \"CD-DNN\", \"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/CD-DNN-HMM-SWB-Interspeech2011-Pub.pdf\")\n",
    "swb_hub_500.measure(date(2012,4,27), 18.5, \"DNN-HMM\", \"https://pdfs.semanticscholar.org/ce25/00257fda92338ec0a117bea1dbc0381d7c73.pdf?_ga=1.195375081.452266805.1483390947\")\n",
    "\n",
    "swb_hub_500.measure(date(2013,8,25), 12.9, \"DNN MMI\", \"http://www.danielpovey.com/files/2013_interspeech_dnn.pdf\")\n",
    "swb_hub_500.measure(date(2013,8,25), 12.6, \"DNN sMBR\", \"http://www.danielpovey.com/files/2013_interspeech_dnn.pdf\")\n",
    "swb_hub_500.measure(date(2013,8,25), 12.9, \"DNN MPE\", \"http://www.danielpovey.com/files/2013_interspeech_dnn.pdf\")\n",
    "swb_hub_500.measure(date(2013,8,25), 12.9, \"DNN BMMI\", \"http://www.danielpovey.com/files/2013_interspeech_dnn.pdf\")\n",
    "\n",
    "swb_hub_500.measure(date(2014,6,30), 16, \"DNN\", \"https://arxiv.org/abs/1406.7806v1\")\n",
    "\n",
    "swb_hub_500.measure(date(2014,12,7), 20, \"Deep Speech\", \"https://arxiv.org/abs/1412.5567\")\n",
    "swb_hub_500.measure(date(2014,12,7), 12.6, \"Deep Speech + FSH\", url=\"https://arxiv.org/abs/1412.5567\") # TODO: why is this also included?\n",
    "\n",
    "swb_hub_500.measure(date(2015,5,21), 8.0, \"IBM 2015\", \"https://arxiv.org/abs/1505.05899\") # TODO: (name check)\n",
    "swb_hub_500.measure(date(2016,4,27), 6.9, \"IBM 2016\", \"https://arxiv.org/abs/1604.08242v1\") # TODO: (name check)\n",
    "\n",
    "swb_hub_500.measure(date(2017,2,17), 6.9, \"RNNLM\", \"https://arxiv.org/abs/1609.03528\") # TODO: (name check)\n",
    "swb_hub_500.measure(date(2017,2,17), 6.2, \"Microsoft 2016\", \"https://arxiv.org/abs/1609.03528\") # TODO: (name check)\n",
    "\n",
    "swb_hub_500.measure(date(2016,10,17), 6.6, \"CNN-LSTM\", \"https://arxiv.org/abs/1610.05256\") # TODO: (name check)\n",
    "swb_hub_500.measure(date(2016,10,17), 5.9, \"CNN-LSTM\",\"https://arxiv.org/abs/1610.05256\") # TODO: (name check)\n",
    "\n",
    "swb_hub_500.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" \n",
    "* * *\n",
    "**_Generative models of CIFAR-10 Natural Images _****[Year: bits-per-subpixel, method]. Compiled by Durk Kingma.**\n",
    "\n",
    "**Why we care:**\n",
    "(1) The compression=prediction=understanding=intelligence view (see Hutter prize, etc.). (Note that perplexity, log-likelihood, and #bits are all equivalent measurements.)\n",
    "(2) Learning a generative model is a prominent auxiliary task towards semi-supervised learning. Current SOTA semi-supervised classification results utilize generative models.\n",
    "3) You're finding patterns in the data that let you compress it more efficiently. Ultimate pattern recognition benchmark because you're trying to find the patterns in all the data. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "image_generation = Problem(\"Drawing pictures\", [\"vision\", \"agi\"])\n",
    "# note: this section is not on scene generation, but making the distinction seemed like a good idea.\n",
    "scene_generation = Problem(\"Be able to generate complex scene e.g. a baboon receiving their degree at convocatoin.\", [\"vision\", \"world-modelling\", \"agi\"])\n",
    "scene_generation.add_subproblem(image_generation)\n",
    "\n",
    "# NOTE: scale, and target need to be checked\n",
    "image_generation_metric = image_generation.metric(\"Generative models of CIFAR-10 images\", scale=bits_per_x, axis_label=\"Model entropy (bits per pixel)\")\n",
    "\n",
    "image_generation_metric.measure(date(2014,10,30), 4.48, \"NICE\", \"https://arxiv.org/abs/1410.8516\")\n",
    "image_generation_metric.measure(date(2015,2,16), 4.13, \"DRAW\", \"https://arxiv.org/abs/1502.04623\")\n",
    "image_generation_metric.measure(date(2016,5,27), 3.49, \"Real NVP\", \"https://arxiv.org/abs/1605.08803\")\n",
    "image_generation_metric.measure(date(2016,6,15), 3.11, \"VAE with IAF\", \"https://papers.nips.cc/paper/6581-improved-variational-inference-with-inverse-autoregressive-flow\")\n",
    "image_generation_metric.measure(date(2016,5,27), 3.0, \"PixelRNN\", \"https://arxiv.org/abs/1605.08803\")\n",
    "image_generation_metric.measure(date(2016,11,4), 2.92, \"PixelCNN++\", \"https://openreview.net/forum?id=BJrFC6ceg\")\n",
    "\n",
    "image_generation_metric.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modelling and Comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text compression is one way to see how well machine learning systems are able to model human language. Shannon's [classic 1951 paper]() obtained an expiermental measure of human text compression performance at 0.6 - 1.3 bits per character: humans know, better than classic algorithms, what word is likely to come next in a piece of writing. More recent work ([Moradi 1998](https://pdfs.semanticscholar.org/48bc/ce35ceb72068723d5f360f388a073aadadca.pdf), Cover 1978) provides estimates that are text-relative and in the 1.3 bits per character (and for some texts, much higher) range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "modelling_english = Problem(\"Accurate modelling of human language.\", [\"language\", \"agi\"])\n",
    "ptperplexity = modelling_english.metric(name=\"Penn Treebank (Perplexity when parsing English sentences)\", scale=perplexity)\n",
    "ptperplexity.measure(date(2016,10,27), 66, \"Recurrent Highway Networks\", \"https://arxiv.org/pdf/1607.03474v3\")\n",
    "ptperplexity.measure(date(2016,9,26), 70.9, \"Pointer Sentinel-LSTM\", \"https://arxiv.org/pdf/1609.07843v1.pdf\")\n",
    "ptperplexity.measure(date(2016,10,5), 73.4, \"Variational LSTM\", \"https://arxiv.org/pdf/1512.05287v5.pdf\")\n",
    "ptperplexity.measure(date(2014,10,8), 82.2, \"RNN Dropout Regularization\", \"https://arxiv.org/abs/1409.2329\")\n",
    "ptperplexity.measure(date(2013,12,20), 107.5, \"Deep RNN\", \"https://arxiv.org/abs/1312.6026\")\n",
    "ptperplexity.measure(date(2012,12,2), 124.7, \"RNNLM\", \"https://pdfs.semanticscholar.org/04e0/fefb859f4b02b017818915a2645427bfbdb2.pdf\")\n",
    "ptperplexity.graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hp_compression = modelling_english.metric(name=\"Hutter Prize (bits per character to encode English text)\", scale=bits_per_x, target=1.3)\n",
    "hp_compression.measure(date(2016,10,31), 1.313, \"Surprisal-Driven Zoneout\",\n",
    "                   \"https://pdfs.semanticscholar.org/e9bc/83f9ff502bec9cffb750468f76fdfcf5dd05.pdf\")\n",
    "hp_compression.measure(date(2016,10,19), 1.37, \"Surprisal-Driven Feedback RNN\",\n",
    "                   \"https://arxiv.org/pdf/1608.06027.pdf\")\n",
    "hp_compression.measure(date(2016,9,27), 1.39, \"Hypernetworks\", \"https://arxiv.org/abs/1609.09106\")\n",
    "hp_compression.measure(date(2016,9,6), 1.32, \" Hierarchical Multiscale RNN\", \"https://arxiv.org/abs/1609.01704\")\n",
    "hp_compression.measure(date(2016,7,12), 1.32, \"Recurrent Highway Networks\", \"https://arxiv.org/abs/1607.03474\")\n",
    "hp_compression.measure(date(2015,7,6), 1.47, \"Grid LSTM\", \"https://arxiv.org/abs/1507.01526\")\n",
    "hp_compression.measure(date(2015,2,15), 1.58, \"Gated Feedback RNN\", \"https://arxiv.org/abs/1502.02367\")\n",
    "# we need to match/double check the release date of the specific version of cmix that got this performance?\n",
    "# hp_compression.measure(date(2014,4,13), 1.245, \"cmix\", \"http://www.byronknoll.com/cmix.html\")\n",
    "hp_compression.measure(date(2013,8,4), 1.67, \"RNN, LSTM\", \"https://arxiv.org/abs/1308.0850\")\n",
    "hp_compression.measure(date(2011,6,28), 1.60, \"RNN\", \"http://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf\")\n",
    "hp_compression.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAMBADA is a challenging language modelling dataset in which the model has to predict a next word in a discourse, when that exact word has not occurred in the test. For instance, given a context like this:\n",
    "\n",
    "> He shook his head, took a step back and held his hands up as he tried to smile without losing a cigarette. \u201cYes you can,\u201d Julia said in a reassuring voice. \u201cI\u2019ve already focused on my friend. You just have to click the shutter, on top, here.\u201d\n",
    "\n",
    "And a target sentence: \n",
    "\n",
    "> He nodded sheepishly, through his cigarette away and took the \\_\\_\\_\\_\\_\\_\\_\\_\\_.\n",
    "\n",
    "The task is to guess the target word \"**camera**\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambada = modelling_english.metric(\"LAMBADA prediction of words in discourse\", url=\"https://arxiv.org/abs/1606.06031\",\n",
    "                                   scale=correct_percent, target=86, target_source=\"https://arxiv.org/abs/1610.08431v3\")\n",
    "lambada.measure(None, 21.7, \"Stanford Reader\", url=\"https://arxiv.org/abs/1610.08431v3\", algorithm_src_url=\"https://arxiv.org/abs/1606.02858\")\n",
    "lambada.measure(None, 32.1, \"Modified Stanford\", url=\"https://arxiv.org/abs/1610.08431v3\", algorithm_src_url=\"https://arxiv.org/abs/1606.02858\")\n",
    "lambada.measure(None, 49.0, \"GA + feat.\", url=\"https://arxiv.org/abs/1610.08431v3\", algorithm_src_url=\"https://arxiv.org/abs/1606.01549v2\")\n",
    "lambada.measure(None, 44.5, \"AS + feat.\", url=\"https://arxiv.org/abs/1610.08431v3\", algorithm_src_url=\"https://arxiv.org/abs/1603.01547\")\n",
    "lambada.measure(None, 51.6, \"MAGE (48)\", url=\"https://arxiv.org/abs/1703.02620v1\")\n",
    "lambada.graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Also consider adding the Microsoft Sentence Completion Challenge; see eg http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf table 7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "translation = Problem(\"Translation between human langauges\", [\"agi\", \"language\"])\n",
    "en_fr_bleu = translation.metric(\"news-test-2014 En-Fr BLEU\", url=\"http://aclweb.org/anthology/P/P02/P02-1040.pdf\", scale=bleu_score, target_label=\"Identical to professional human translations\", target=50)\n",
    "en_de_bleu = translation.metric(\"news-test-2014 En-De BLEU\", url=\"http://aclweb.org/anthology/P/P02/P02-1040.pdf\", scale=bleu_score, target_label=\"Identical to professional human translations\", target=50)\n",
    "\n",
    "en_fr_bleu.measure(None, 37, \"PBMT\", url=\"http://www.anthology.aclweb.org/W/W14/W14-33.pdf\", papername=\"Edinburgh\u2019s phrase-based machine translation systems for WMT-14\", venue=\"WMT 2014\")\n",
    "en_de_bleu.measure(None, 20.7, \"PBMT\", url=\"http://www.anthology.aclweb.org/W/W14/W14-33.pdf\", papername=\"Edinburgh\u2019s phrase-based machine translation systems for WMT-14\", venue=\"WMT 2014\")\n",
    "\n",
    "en_fr_bleu.measure(date(2014, 9, 1), 36.15, \"RNN-search50*\", url=\"https://arxiv.org/abs/1409.0473\")\n",
    "en_fr_bleu.measure(date(2014, 10, 30), 37.5, \"LSTM6 + PosUnk\", url=\"https://arxiv.org/abs/1410.8206\")\n",
    "en_fr_bleu.measure(date(2016, 9, 26), 39.92, \"GNMT+RL\", url=\"https://arxiv.org/abs/1609.08144\")\n",
    "en_de_bleu.measure(date(2016, 9, 26), 26.30, \"GNMT+RL\", url=\"https://arxiv.org/abs/1609.08144\")\n",
    "\n",
    "# Lots of this data is coming via https://arxiv.org/abs/1609.08144\n",
    "en_fr_bleu.measure(date(2016, 7, 23), 39.2, \"Deep-Att + PosUnk\", url=\"https://arxiv.org/abs/1606.04199\")\n",
    "en_de_bleu.measure(date(2016, 7, 23), 20.7, \"Deep-Att\", url=\"https://arxiv.org/abs/1606.04199\")\n",
    "\n",
    "en_fr_bleu.measure(date(2017, 1, 23), 40.56, \"MoE 2048\", url=\"https://arxiv.org/pdf/1701.06538\")\n",
    "en_de_bleu.measure(date(2017, 1, 23), 26.03, \"MoE 2048\", url=\"https://arxiv.org/pdf/1701.06538\")\n",
    "\n",
    "en_de_bleu.measure(date(2016, 7, 14), 17.93, \"NSE-NSE\", url=\"https://arxiv.org/abs/1607.04315v1\")\n",
    "\n",
    "# XXX add more languages\n",
    "en_fr_bleu.graph()\n",
    "en_de_bleu.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation: Chatbots & Conversational Agents\n",
    "\n",
    "Conversation is the classic AI progress measure! There is the Turing test, which involves a human judge trying to tell the difference between a humand and computer that they are chatting to online, and also easier variants of the Turing test in which the judge limits themselves to more casual, less probing conversation in various ways.\n",
    "\n",
    "The Loebner Prize is an annual event that runs a somewhat easier version of the test. Since 2014, the event has also been giving standard-form tests to their entrants, and scoring the results (each question gets a plausible/semi-plausible/implausible rating). This metric is not stable, because the test questions have to change every year, they are somewhat indicative of progress. Ideally the event might apply each year's test questions to the most successful entrants from prior years. Here is an example from 2016:\n",
    "\n",
    "<img src=\"images/loebner.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "turing_test = Problem(\"Conduct arbitrary sustained, probing conversation\", [\"agi\", \"language\", \"world-modelling\", \"communication\"])\n",
    "easy_turing_test = Problem(\"Turing test for casual conversation\", [\"agi\", \"language\", \"world-modelling\", \"communication\"])\n",
    "turing_test.add_subproblem(easy_turing_test)\n",
    "\n",
    "loebner = easy_turing_test.metric(\"The Loebner Prize scored selection answers\", url=\"http://www.aisb.org.uk/events/loebner-prize\", \n",
    "                                  scale=correct_percent, changeable=True, target=100, target_label=\"Completely plausible answers\",\n",
    "                                axis_label='Percentage of answers rated plausible\\n(each year is a different test)')\n",
    "# XXX humans probably don't get 100% on the Loebner Prize selection questions; we should ask the organizers to score\n",
    "# some humans\n",
    "\n",
    "loebner.notes = \"\"\"\n",
    "The Loebner Prize is an actual enactment of the Turing Test. Importantly, judges are instructed to engage in casual, natural\n",
    "conversation rather than deliberately probing to determine if participants are \"intelligent\" (Brian Christian, The Most Human Human).\n",
    "This makes it considerably easier than a probing Turing Test, and it is close to being solved. \n",
    "\n",
    "However these aren't scores for the full Loebner Turing Test; since 2014 the Loebner prize has scored its entrants by\n",
    "giving them a corpus of conversation and scoring their answers. We use these numbers because they remove variability\n",
    "in the behaviour of the judges. Unfortunately, these questions change from year to year (and have to, since \n",
    "entrants will test with last year's data).\n",
    "\"\"\"\n",
    "loebner.measure(date(2016,9,17), 90, \"Mitsuku 2016\", url=\"http://www.aisb.org.uk/events/loebner-prize#Results16\")\n",
    "loebner.measure(date(2016,9,17), 78.3, \"Tutor 2016\", url=\"http://www.aisb.org.uk/events/loebner-prize#Results16\")\n",
    "loebner.measure(date(2016,9,17), 77.5, \"Rose 2016\", url=\"http://www.aisb.org.uk/events/loebner-prize#Results16\")\n",
    "loebner.measure(date(2016,9,17), 77.5, \"Arckon 2016\", url=\"http://www.aisb.org.uk/events/loebner-prize#Results16\")\n",
    "loebner.measure(date(2016,9,17), 76.7, \"Katie 2016\", url=\"http://www.aisb.org.uk/events/loebner-prize#Results16\")\n",
    "\n",
    "loebner.measure(date(2015,9,19), 83.3, \"Mitsuku 2015\", url=\"http://www.aisb.org.uk/events/loebner-prize#Results15\")\n",
    "loebner.measure(date(2015,9,19), 80, \"Lisa 2015\", url=\"http://www.aisb.org.uk/events/loebner-prize#Results15\")\n",
    "loebner.measure(date(2015,9,19), 76.7, \"Izar 2015\", url=\"http://www.aisb.org.uk/events/loebner-prize#Results15\")\n",
    "loebner.measure(date(2015,9,19), 75, \"Rose 2015\",url=\"http://www.aisb.org.uk/events/loebner-prize#Results15\")\n",
    "\n",
    "loebner.measure(date(2014,11,15), 89.2, \"Rose 2014\", url=\"http://www.aisb.org.uk/events/loebner-prize#contest2014\")\n",
    "loebner.measure(date(2014,11,15), 88.3, \"Izar 2014\", url=\"http://www.aisb.org.uk/events/loebner-prize#contest2014\")\n",
    "loebner.measure(date(2014,11,15), 88.3, \"Misuku 2014\", url=\"http://www.aisb.org.uk/events/loebner-prize#contest2014\")\n",
    "loebner.measure(date(2014,11,15), 81.67, \"Uberbot 2014\", url=\"http://www.aisb.org.uk/events/loebner-prize#contest2014\")\n",
    "loebner.measure(date(2014,11,15), 80.83, \"Tutor 2014\", url=\"http://www.aisb.org.uk/events/loebner-prize#contest2014\")\n",
    "loebner.measure(date(2014,11,15), 76.7, \"The Professor 2014\", url=\"http://www.aisb.org.uk/events/loebner-prize#contest2014\")\n",
    "loebner.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Facebook BABI 20 QA dataset is an example of a basic reading comprehension task. It has been solved with large training datasets (10,000 examples per task) but not with a smaller training dataset of 1,000 examples for each of the 20 categories of tasks. It involves learning to answer simple reasoning questions like these:\n",
    "\n",
    "<img src=\"images/babi20qa.png\" style=\"width: 66%; height: 66%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reading_comprehension = Problem(\"Language comprehension and question-answering\", [\"language\", \"world-modelling\", \"agi\"])\n",
    "turing_test.add_subproblem(reading_comprehension)\n",
    "\n",
    "# Overview of Machine Reading Comprehension (MRC) datasets here:\n",
    "# http://eric-yuan.me/compare-popular-mrc-datasets/\n",
    "\n",
    "bAbi10k = reading_comprehension.metric(\"bAbi 20 QA (10k training examples)\", url=\"http://fb.ai/babi\", scale=correct_percent, target=99, target_label=\"Excellent performance\")\n",
    "bAbi1k = reading_comprehension.metric(\"bAbi 20 QA (1k training examples)\", url=\"http://fb.ai/babi\", scale=correct_percent, target=99, target_label=\"Excellent performance\")\n",
    "\n",
    "bAbi1k.notes = \"\"\"\n",
    "A synthetic environment inspired by text adventures and SHRDLU, which enables generation\n",
    "of ground truths, describing sentences, and inferential questions. Includes:\n",
    "supporting facts, relations, yes/no questions, counting, lists/sets, negation, indefiniteness,\n",
    "conference, conjunction, time, basic deduction and induction, reasoning about position, size,\n",
    "path finding and motivation.\n",
    "\n",
    "Table 3 of https://arxiv.org/abs/1502.05698 actually breaks this down into 20 submeasures\n",
    "but initially we're lumping all of this together.\n",
    "\n",
    "Originally \"solving\" bABI was defined as 95% accuracy (or perhaps) 95% accuracy on all submeasures,\n",
    "but clearly humans and now algorithms are better than that.\n",
    "\n",
    "TODO: bAbi really needs to be decomposed into semi-supervised and unsupervised variants, and \n",
    "by amount of training data provided\n",
    "\"\"\"\n",
    "bAbi10k.measure(date(2015,2,19), 93.3, \"MemNN-AM+NG+NL (1k + strong supervision)\", \"https://arxiv.org/abs/1502.05698v1\", not_directly_comparable=True, long_label=True) # not literally a 10K example, but more comparable to it\n",
    "\n",
    "#bAbi1k.measure(None, 48.7, \"LSTM\", \"https://arxiv.org/abs/1502.05698v1\", algorithm_src_url=\"http://isle.illinois.edu/sst/meetings/2015/hochreiter-lstm.pdf\", min_date=date(1997,11,15))\n",
    "bAbi1k.measure(date(2015,3,31), 86.1, \"MemN2N-PE+LS+RN\", \"https://arxiv.org/abs/1503.08895\")\n",
    "bAbi10k.measure(date(2015,3,31), 93.4, \"MemN2N-PE+LS+RN\", \"https://arxiv.org/abs/1503.08895\")\n",
    "bAbi1k.measure(date(2015,6,24), 93.6, \"DMN\", \"https://arxiv.org/abs/1506.07285\") # The paper doesn't say if this is 1k or 10k, but seems like 1k\n",
    "bAbi10k.measure(date(2016,1,5), 96.2, \"DNC\", \"https://www.gwern.net/docs/2016-graves.pdf\")\n",
    "\n",
    "bAbi10k.measure(date(2016,9,27), 97.1, \"SDNC\", \"https://arxiv.org/abs/1606.04582v4\")\n",
    "bAbi10k.measure(date(2016,12,12), 99.5, \"EntNet\", \"https://arxiv.org/abs/1612.03969\")\n",
    "bAbi1k.measure(date(2016,12,12), 89.1, \"EntNet\", \"https://arxiv.org/abs/1612.03969\")\n",
    "\n",
    "bAbi10k.measure(date(2016,12,9),  99.7, \"QRN\", \"https://arxiv.org/abs/1606.04582v4\")\n",
    "bAbi1k.measure(date(2016,12,9),  90.1, \"QRN\", \"https://arxiv.org/abs/1606.04582v4\")\n",
    "bAbi1k.measure(None, 66.8, \"DMN+\", \"https://arxiv.org/abs/1606.04582v4\", algorithm_src_url=\"https://arxiv.org/abs/1607.00036\", replicated=\"https://github.com/therne/dmn-tensorflow\")\n",
    "bAbi10k.measure(date(2016,6,30),  97.2, \"DMN+\", \"https://arxiv.org/abs/1607.00036\")\n",
    "\n",
    "bAbi10k.graph()\n",
    "bAbi1k.graph()\n",
    "\n",
    "# More papers:\n",
    "# https://www.aclweb.org/anthology/D/D13/D13-1020.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are numerous other reading comprehension metrics that are in various ways harder than bAbi 20 QA. They are generally not solved, though progress is fairly promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "mctest160 = reading_comprehension.metric(\"Reading comprehension MCTest-160-all\", scale=correct_percent, url=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/MCTest_EMNLP2013.pdf\")\n",
    "mctest160.measure(date(2013, 10, 1), 69.16, \"SW+D+RTE\", url=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/MCTest_EMNLP2013.pdf\", papername=\"MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text\")\n",
    "mctest160.measure(date(2015, 7, 26), 75.27, \"Wang-et-al\", url=\"http://arxiv.org/abs/1603.08884\")\n",
    "mctest160.measure(date(2015, 7, 26), 73.27, \"Narasimhan-model3\", url=\"https://people.csail.mit.edu/regina/my_papers/MCDR15.pdf\", papername=\"Machine Comprehension with Discourse Relations\")\n",
    "mctest160.measure(date(2016, 3, 29), 74.58, \"Parallel-Hierarchical\", url=\"http://arxiv.org/abs/1603.08884\")\n",
    "\n",
    "mctest500 = reading_comprehension.metric(\"Reading comprehension MCTest-500-all\", scale=correct_percent, url=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/MCTest_EMNLP2013.pdf\")\n",
    "mctest500.measure(date(2013, 10, 1), 63.33, \"SW+D+RTE\", url=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/MCTest_EMNLP2013.pdf\", papername=\"MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text\")\n",
    "mctest500.measure(date(2015, 7, 26), 69.94, \"Wang-et-al\", url=\"http://arxiv.org/abs/1603.08884\")\n",
    "mctest500.measure(date(2015, 7, 26), 63.75, \"Narasimhan-model3\", url=\"https://people.csail.mit.edu/regina/my_papers/MCDR15.pdf\", papername=\"Machine Comprehension with Discourse Relations\")\n",
    "mctest500.measure(date(2015, 7, 26), 67.83, \"LSSVM\", url=\"https://pdfs.semanticscholar.org/f26e/088bc4659a9b7fce28b6604d26de779bcf93.pdf\", papername=\"Learning Answer-Entailing Structures for Machine Comprehension\")\n",
    "mctest500.measure(date(2016, 3, 29), 71.00, \"Parallel-Hierarchical\", url=\"http://arxiv.org/abs/1603.08884\")\n",
    "\n",
    "cbtest_ne = reading_comprehension.metric(\"bAbi Children's Book comprehension CBtest NE\", url=\"http://fb.ai/babi\", scale=correct_percent, target=81.6, target_source=\"https://arxiv.org/abs/1511.02301\")\n",
    "cbtest_cn = reading_comprehension.metric(\"bAbi Children's Book comprehension CBtest CN\", url=\"http://fb.ai/babi\", scale=correct_percent, target=81.6, target_source=\"https://arxiv.org/abs/1511.02301\")\n",
    "cnn = reading_comprehension.metric(\"CNN Comprehension test\", url=\"https://github.com/deepmind/rc-data/\", scale=correct_percent)\n",
    "daily_mail = reading_comprehension.metric(\"Daily Mail Comprehension test\", url=\"https://github.com/deepmind/rc-data/\", scale=correct_percent)\n",
    "\n",
    "cnn.measure(date(2015, 6, 10), 63.0, \"Attentive reader\", url=\"https://arxiv.org/abs/1506.03340\")\n",
    "cnn.measure(date(2015, 6, 10), 63.8, \"Impatient reader\", url=\"https://arxiv.org/abs/1506.03340\")\n",
    "daily_mail.measure(date(2015, 6, 10), 69.0, \"Attentive reader\", url=\"https://arxiv.org/abs/1506.03340\")\n",
    "daily_mail.measure(date(2015, 6, 10), 68.0, \"Impatient reader\", url=\"https://arxiv.org/abs/1506.03340\")\n",
    "\n",
    "cnn.measure(date(2016, 6, 7), 75.7, \"AIA\", url=\"https://arxiv.org/abs/1606.02245v1\")\n",
    "cbtest_ne.measure(date(2016, 6, 7), 72.0, \"AIA\", url=\"https://arxiv.org/abs/1606.02245v1\")\n",
    "cbtest_ne.measure(date(2016, 6, 7), 71.0, \"AIA\", url=\"https://arxiv.org/abs/1606.02245v1\")\n",
    "\n",
    "cnn.measure(date(2016, 11, 9), 76.1, \"AIA\", url=\"https://arxiv.org/abs/1606.02245v4\")\n",
    "\n",
    "cnn.measure(date(2016, 6, 7), 74.0, \"EpiReader\", url=\"https://arxiv.org/abs/1606.02270\")\n",
    "cbtest_ne.measure(date(2016, 6, 7), 69.7, \"EpiReader\", url=\"https://arxiv.org/abs/1606.02270\")\n",
    "cbtest_cn.measure(date(2016, 6, 7), 67.4, \"EpiReader\", url=\"https://arxiv.org/abs/1606.02270\")\n",
    "\n",
    "cbtest_cn.measure(date(2016, 6, 5), 69.4, \"GA reader\", url=\"https://arxiv.org/abs/1606.01549v1\")\n",
    "cbtest_ne.measure(date(2016, 6, 5), 71.9, \"GA reader\", url=\"https://arxiv.org/abs/1606.01549v1\")\n",
    "cnn.measure(date(2016, 6, 5), 77.4, \"GA reader\", url=\"https://arxiv.org/abs/1606.01549v1\")\n",
    "daily_mail.measure(date(2016, 6, 5), 78.1, \"GA reader\", url=\"https://arxiv.org/abs/1606.01549v1\")\n",
    "\n",
    "cnn.measure(None, 77.9, \"GA update L(w)\", url=\"https://arxiv.org/abs/1606.01549v2\")\n",
    "daily_mail.measure(None, 80.9, \"GA update L(w)\", url=\"https://arxiv.org/abs/1606.01549v2\")\n",
    "cbtest_ne.measure(None, 74.9, \"GA +feature, fix L(w)\", url=\"https://arxiv.org/abs/1606.01549v2\")\n",
    "cbtest_cn.measure(None, 70.7, \"GA +feature, fix L(w)\", url=\"https://arxiv.org/abs/1606.01549v2\")\n",
    "\n",
    "# Neural semantic encoders invented in https://arxiv.org/abs/1607.04315v1 and retrospectively applied to CBTest by other authors\n",
    "cbtest_ne.measure(date(2016, 12, 1), 73.2, \"NSE\", url=\"https://arxiv.org/abs/1606.01549v2\", algorithm_src_url=\"https://arxiv.org/abs/1607.04315\", min_date=date(2016,7,4))\n",
    "cbtest_cn.measure(date(2016, 12, 1), 71.9, \"NSE\", url=\"https://arxiv.org/abs/1606.01549v2\", algorithm_src_url=\"https://arxiv.org/abs/1607.04315\", min_date=date(2016,7,4))\n",
    "\n",
    "\n",
    "cnn.measure(date(2016, 8, 4), 74.4, \"AoA reader\", url=\"https://arxiv.org/pdf/1607.04423\")\n",
    "cbtest_ne.measure(date(2016, 8, 4), 72.0, \"AoA reader\", url=\"https://arxiv.org/pdf/1607.04423\")\n",
    "cbtest_cn.measure(date(2016, 8, 4), 69.4, \"AoA reader\", url=\"https://arxiv.org/pdf/1607.04423\")\n",
    "\n",
    "cnn.measure(date(2016, 8, 8), 77.6, \"Attentive+relabling+ensemble\", url=\"https://arxiv.org/abs/1606.02858\")\n",
    "daily_mail.measure(date(2016, 8, 8), 79.2, \"Attentive+relabling+ensemble\", url=\"https://arxiv.org/abs/1606.02858\")\n",
    "\n",
    "cnn.measure(None, 75.4, \"AS reader (avg)\", url=\"https://arxiv.org/abs/1603.01547v1\")\n",
    "cnn.measure(None, 74.8, \"AS reader (greedy)\", url=\"https://arxiv.org/abs/1603.01547v1\")\n",
    "daily_mail.measure(None, 77.1, \"AS reader (avg)\", url=\"https://arxiv.org/abs/1603.01547v1\")\n",
    "daily_mail.measure(None, 77.7, \"AS reader (greedy)\", url=\"https://arxiv.org/abs/1603.01547v1\")\n",
    "cbtest_ne.measure(None, 70.6, \"AS reader (avg)\", url=\"https://arxiv.org/abs/1603.01547v1\")\n",
    "cbtest_ne.measure(None, 71.0, \"AS reader (greedy)\", url=\"https://arxiv.org/abs/1603.01547v1\")\n",
    "cbtest_cn.measure(None, 68.9, \"AS reader (avg)\", url=\"https://arxiv.org/abs/1603.01547v1\")\n",
    "cbtest_cn.measure(None, 67.5, \"AS reader (greedy)\", url=\"https://arxiv.org/abs/1603.01547v1\")\n",
    "\n",
    "squad_em = reading_comprehension.metric(\"Stanford Question Answering Dataset EM test\", url=\"https://stanford-qa.com/\")\n",
    "squad_f1 = reading_comprehension.metric(\"Stanford Question Answering Dataset F1 test\", url=\"https://stanford-qa.com/\")\n",
    "\n",
    "squad_em.measure(date(2017, 3, 8), 76.922, \"r-net (ensemble)\", url=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf\")\n",
    "squad_f1.measure(date(2017, 3, 8), 84.006, \"r-net (ensemble)\", url=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf\")\n",
    "\n",
    "squad_em.measure(date(2017, 3, 8), 74.614, \"r-net (single model)\", url=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf\")\n",
    "squad_f1.measure(date(2017, 3, 8), 82.458, \"r-net (single model)\", url=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf\")\n",
    "\n",
    "squad_em.measure(date(2017, 5, 8), 73.754, \"Mnemonic reader (ensemble)\", url=\"https://arxiv.org/pdf/1705.02798.pdf\")\n",
    "squad_f1.measure(date(2017, 5, 8), 81.863, \"Mnemonic reader (ensemble)\", url=\"https://arxiv.org/pdf/1705.02798.pdf\")\n",
    "\n",
    "squad_em.measure(date(2017, 4, 20), 73.723, \"SEDT+BiDAF (ensemble)\", url=\"https://arxiv.org/pdf/1703.00572.pdf\")\n",
    "squad_f1.measure(date(2017, 4, 20), 81.53, \"SEDT+BiDAF (ensemble)\", url=\"https://arxiv.org/pdf/1703.00572.pdf\")\n",
    "\n",
    "squad_em.measure(date(2017, 2, 24), 73.744, \"BiDAF (ensemble)\", url=\"https://arxiv.org/abs/1611.01603\")\n",
    "squad_f1.measure(date(2017, 2, 24), 81.525, \"BiDAF (ensemble)\", url=\"https://arxiv.org/abs/1611.01603\")\n",
    "\n",
    "squad_em.measure(date(2017, 5,31), 73.01, \"jNet (ensemble)\",url=\"https://arxiv.org/abs/1703.04617\", min_date=date(2017,5,1))\n",
    "squad_f1.measure(date(2017, 5,31), 81.517, \"jNet (ensemble)\", url=\"https://arxiv.org/abs/1703.04617\", min_date=date(2017,5,1))\n",
    "\n",
    "squad_em.measure(date(2016, 12, 13), 73.765, \"MPM (ensemble)\", url=\"https://arxiv.org/abs/1612.04211\")\n",
    "squad_f1.measure(date(2016, 12, 13), 81.257, \"MPM (ensemble)\", url=\"https://arxiv.org/abs/1612.04211\")\n",
    "\n",
    "squad_em.measure(date(2017, 2, 13), 71.2, \"Dynamic Coattention Networks (ensemble)\", url=\"https://arxiv.org/pdf/1611.01604.pdf\")\n",
    "squad_f1.measure(date(2017, 2, 13), 80.4, \"Dynamic Coattention Networks (ensemble)\", url=\"https://arxiv.org/pdf/1611.01604.pdf\")\n",
    "\n",
    "squad_em.measure(date(2017, 5,31), 70.607, \"jNet (single model)\", url=\"https://arxiv.org/abs/1703.04617\", min_date=date(2017,5,1))\n",
    "squad_f1.measure(date(2017, 5,31), 79.456, \"jNet (single model)\", url=\"https://arxiv.org/abs/1703.04617\", min_date=date(2017,5,1))\n",
    "\n",
    "squad_em.measure(date(2017, 4, 24), 70.639, \"Ruminating Reader (single model)\", url=\"https://arxiv.org/pdf/1704.07415.pdf\")\n",
    "squad_f1.measure(date(2017, 4, 24), 79.821, \"Ruminating Reader (single model)\", url=\"https://arxiv.org/pdf/1704.07415.pdf\")\n",
    "\n",
    "squad_em.measure(date(2017, 3, 31), 70.733, \"Document Reader (single model)\", url=\"https://arxiv.org/abs/1704.00051\")\n",
    "squad_f1.measure(date(2017, 3, 31), 79.353, \"Document Reader (single model)\", url=\"https://arxiv.org/abs/1704.00051\")\n",
    "\n",
    "squad_em.measure(date(2017, 5, 8), 69.863, \"Mnemonic reader (single model)\", url=\"https://arxiv.org/pdf/1705.02798.pdf\")\n",
    "squad_f1.measure(date(2017, 5, 8), 79.207, \"Mnemonic reader (single model)\", url=\"https://arxiv.org/pdf/1705.02798.pdf\")\n",
    "\n",
    "squad_em.measure(date(2016, 12, 29), 70.849, \"FastQAExt\", url=\"https://arxiv.org/abs/1703.04816\")\n",
    "squad_f1.measure(date(2016, 12, 29), 78.857, \"FastQAExt\", url=\"https://arxiv.org/abs/1703.04816\")\n",
    "\n",
    "squad_em.measure(date(2016, 12, 13), 70.387, \"MPM (single model)\", url=\"https://arxiv.org/abs/1612.04211\")\n",
    "squad_f1.measure(date(2016, 12, 13), 78.784, \"MPM (single model)\", url=\"https://arxiv.org/abs/1612.04211\")\n",
    "\n",
    "squad_em.measure(date(2017, 5, 31), 70.849, \"RaSoR (single model)\", url=\"https://arxiv.org/abs/1611.01436\", min_date=date(2017,5,1))\n",
    "squad_f1.measure(date(2017, 5, 31), 78.741, \"RaSoR (single model)\", url=\"https://arxiv.org/abs/1611.01436\", min_date=date(2017,5,1))\n",
    "\n",
    "squad_em.measure(date(2017, 4, 20), 68.478, \"SEDT+BiDAF (single model)\", url=\"https://arxiv.org/pdf/1703.00572.pdf\")\n",
    "squad_f1.measure(date(2017, 4, 20), 77.971, \"SEDT+BiDAF (single model)\", url=\"https://arxiv.org/pdf/1703.00572.pdf\")\n",
    "\n",
    "squad_em.measure(date(2016, 11, 29), 68.478, \"BiDAF (single model)\", url=\"https://arxiv.org/abs/1611.01603\")\n",
    "squad_f1.measure(date(2016, 11, 29), 77.971, \"BiDAF (single model)\", url=\"https://arxiv.org/abs/1611.01603\")\n",
    "\n",
    "squad_em.measure(date(2016, 12, 29), 68.436, \"FastQA\", url=\"https://arxiv.org/abs/1703.04816\")\n",
    "squad_f1.measure(date(2016, 12, 29), 77.07, \"FastQA\", url=\"https://arxiv.org/abs/1703.04816\")\n",
    "\n",
    "squad_em.measure(date(2016, 11, 7), 67.901, \"Match-LSTM+Ans-Ptr\", url=\"https://arxiv.org/pdf/1608.07905v2\")\n",
    "squad_f1.measure(date(2016, 11, 7), 77.022, \"Match-LSTM+Ans-Ptr\", url=\"https://arxiv.org/pdf/1608.07905v2\")\n",
    "\n",
    "for m in reading_comprehension.metrics:\n",
    "    m.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific and Technical capabilities\n",
    "\n",
    "Arguably reading and understanding scientific, technical, engineering and medical documents would be taxonomically related to general reading comprehension, but these technical tasks are probably much more difficult, and will certainly be solved with separate efforts. So we classify them separately for now. We also classify some of these problems as superintelligent, because only a tiny fraction of humans can read STEM papers, and only a miniscule fraction of humans are capable of reasonably comprehending STEM papers across a large range of fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "read_stem_papers = Problem(\"Read a scientific or technical paper, and comprehend its contents\", [\"language\", \"world-modelling\", \"super\"])\n",
    "\n",
    "# Getting some major results from an abstract, tables or conclusion is much easier than understanding the entire paper, its assumptions, robustness, support for its claims, etc\n",
    "extract_results = Problem(\"Extract major numerical results or progress claims from a STEM paper\", [\"language\", \"world-modelling\", \"agi\"])\n",
    "read_stem_papers.add_subproblem(extract_results)\n",
    "\n",
    "extract_results.metric(\"Automatically find new relevant ML results on arXiv\")\n",
    "extract_results.notes = \"\"\"\n",
    "This metric is the ability to automatically update the ipython Notebook you are reading by spotting results in pdfs uploaded to arxiv.org.\n",
    "Pull requests demonstrating solutions are welcome :)\n",
    "\"\"\"\n",
    "\n",
    "solve_technical_problems = Problem(\"Given an arbitrary technical problem, solve it as well as a typical professional in that field\", [\"language\", \"world-modelling\"])\n",
    "\n",
    "program_induction = Problem(\"Writing software from specifications\")\n",
    "solve_technical_problems.add_subproblem(program_induction)\n",
    "program_induction.metric(\"Card2Code\", url=\"https://github.com/deepmind/card2code\", scale=correct_percent)\n",
    "\n",
    "vaguely_constrained_technical_problems = Problem(\"Solve vaguely or under-constrained technical problems\")\n",
    "solve_technical_problems.add_subproblem(vaguely_constrained_technical_problems)\n",
    "\n",
    "# This subset of technical problems is much easier; here we assume that a human / worldly problem has been reduced to something that can be\n",
    "# subjected to clear computational evaluation (\"is this purported proof of theorem X correct?\", \"does this circuit perform task Y efficiently?\"\n",
    "# \"will this airframe fly with reasonable characteristics?\")\n",
    "solve_constrained_technical_problems = Problem(\"Solve technical problems with clear constraints (proofs, circuit design, aerofoil design, etc)\")\n",
    "solve_technical_problems.add_subproblem(solve_constrained_technical_problems)\n",
    "vaguely_constrained_technical_problems.add_subproblem(read_stem_papers)\n",
    "\n",
    "# Note that this theorem proving problem (learning to prove theorems) is a little different from the pure search\n",
    "# through proof space that characterises the classic ATP field, though progress there may also be interesting\n",
    "theorem_proving = Problem(\"Given examples of proofs, find correct proofs of simple mathematical theorems\", [\"agi\", \"math\"])\n",
    "circuit_design = Problem(\"Given desired circuit characteristics, and many examples, design new circuits to spec\", [\"agi\", \"math\"])\n",
    "solve_constrained_technical_problems.add_subproblem(theorem_proving)\n",
    "theorem_proving.metric(\"HolStep\", url=\"https://arxiv.org/abs/1703.00426\")\n",
    "solve_constrained_technical_problems.add_subproblem(circuit_design)\n",
    "\n",
    "# TODO: find well-defined metrics for some of these problems in the literature. Or create some!\n",
    "# Some relevant papers:\n",
    "# http://www.ise.bgu.ac.il/faculty/kalech/publications/ijcai13.pdf\n",
    "# https://www.researchgate.net/publication/2745078_Use_of_Automatically_Defined_Functions_and_ArchitectureAltering_Operations_in_Automated_Circuit_Synthesis_with_Genetic_Programming\n",
    "# https://link.springer.com/article/10.1007/s10817-014-9301-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left; position:relative\">\n",
    "  <div style=\"float:right; text-align:center\">\n",
    "      <img src=\"images/cards.jpg\" style=\"width: 33%; height: 33%\">\n",
    "      Example Magic The Gathering (MTG) and Hearthstone (HS) cards\n",
    "      <img src=\"images/mtg-dragon-code.png\" style=\"width: 33%; height: 33%\">\n",
    "      Corresponding MTG card implementation in Java\n",
    "  </div>\n",
    "  <div style=\"text-align:left\">\n",
    "    <h2>Generating computer programs from specifications</h2>\n",
    "\n",
    "        A particularly interesting technical problem, which may be slightly harder than problems with very clear constraints like circuit design, is generating computer programs from natural language specifications (which will often contain ambiguities of various sorts). This is presently a very unsolved problem, though there is now at least one good metric / dataset for it, which is [Deepmind's \"card2code\" dataset](https://github.com/deepmind/card2code) of Magic the Gathering and Hearthstone cards, along with Java and Python implementations (respectively) of the logic on the cards. Shown below is a figure from [_Ling, et al. 2016_](https://arxiv.org/abs/1603.06744v1) with their Latent Predictor Networks generating part of the code output for a Hearthstone card:\n",
    "    <img src=\"images/hs-lpn.jpg\" style=\"width: 66%; height: 66%\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "program_induction = Problem(\"Write computer programs from specifications\")\n",
    "vaguely_constrained_technical_problems.add_subproblem(program_induction)\n",
    "card2code_mtg_acc = program_induction.metric(\"Card2Code MTG accuracy\", url=\"https://github.com/deepmind/card2code\", scale=correct_percent, target=100, target_label=\"Bug-free card implementation\")\n",
    "card2code_hs_acc = program_induction.metric(\"Card2Code Hearthstone accuracy\", url=\"https://github.com/deepmind/card2code\", scale=correct_percent, target=100, target_label=\"Bug-free card implementation\")\n",
    "\n",
    "card2code_mtg_acc.measure(None, 4.8, \"LPN\", url=\"https://arxiv.org/abs/1603.06744v1\")\n",
    "card2code_hs_acc.measure(None, 6.1, \"LPN\", url=\"https://arxiv.org/abs/1603.06744v1\")\n",
    "card2code_hs_acc.measure(None, 13.6, \"Seq2Tree-Unk\", url=\"https://arxiv.org/abs/1704.01696v1\", algorithm_src_url=\"https://arxiv.org/abs/1601.01280v1\")\n",
    "card2code_hs_acc.measure(None, 1.5, \"NMT\", url=\"https://arxiv.org/abs/1704.01696v1\", algorithm_src_url=\"https://arxiv.org/abs/1409.0473v1\")\n",
    "#card2code_hs_acc.measure(None, 16.2, \"SNM\", url=\"https://arxiv.org/abs/1704.01696v1\")\n",
    "card2code_hs_acc.measure(None, 16.7, \"SNM -frontier embed\", url=\"https://arxiv.org/abs/1704.01696v1\")\n",
    "\n",
    "understand_conditional_expressions = Problem(\"Parse and implement complex conditional expressions\")\n",
    "program_induction.add_subproblem(understand_conditional_expressions)\n",
    "card2code_hs_acc.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Learn\n",
    "\n",
    "## Generalisation and Transfer Learning\n",
    "\n",
    "ML systems are making strong progress at solving specific problems with sufficient training data. But we know that humans are\n",
    "capable of _transfer learning_ -- applying things they've learned from one context, with appropriate variation, to another context.\n",
    "Humans are also very general; rather than just being taught to perform specific tasks, a single agent is able to do a very\n",
    "wide range of tasks, learning new things or not as required by the situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generalisation = Problem(\"Building systems that solve a wide range of diverse problems, rather than just specific ones\")\n",
    "generalisation.metric(\"Solve all other solved problems in this document, with a single system\", solved=False)\n",
    "\n",
    "transfer_learning = Problem(\"Transfer learning: apply relevant knowledge from a prior setting to a new slightly different one\")\n",
    "arcade_transfer = Problem(\"Transfer of learning within simple arcade game paradigms\")\n",
    "\n",
    "generalisation.add_subproblem(transfer_learning)\n",
    "transfer_learning.add_subproblem(arcade_transfer)\n",
    "\n",
    "# These will need to be specified a bit more clearly to be proper metrics, eg \"play galaga well having trained on Xenon 2\" or whatever\n",
    "# the literature has settled on\n",
    "# arcade_transfer.metric(\"Transfer learning of platform games\")\n",
    "# arcade_transfer.metric(\"Transfer learning of vertical shooter games\")\n",
    "# arcade_transfer.metric(\"Transfer from a few arcade games to all of them\")\n",
    "\n",
    "one_shot_learning = Problem(\"One shot learning: ingest important truths from a single example\", [\"agi\", \"world-modelling\"])\n",
    "\n",
    "uncertain_prediction = Problem(\"Correctly identify when an answer to a classification problem is uncertain\")\n",
    "uncertain_prediction.notes = \"Humans can usually tell when they don't know something. Present ML classifiers do not have this ability.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safety and Security Problems\n",
    "\n",
    "The notion of \"safety\" for AI and ML systems can encompass many things. In some cases it's about ensuring that the system meets various sorts of constraints, either in general or for specifically safety-critical purposes, such as [correct detection of pedestrians](#pedestrian detection) for self driving cars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Adversarial Examples\" and manipulation of ML classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adversarial_examples = Problem(\"Resistance to adversarial examples\", [\"safety\", \"agi\", \"security\"], url=\"https://arxiv.org/abs/1312.6199\")\n",
    "\n",
    "adversarial_examples.notes = \"\"\"\n",
    "We know that humans have significant resistance to adversarial examples.  Although methods like camouflage sometimes\n",
    "work to fool us into thinking one thing is another, those\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safety of Reinforcement Learning Agents and similar systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This section is essentially on teaching ML systems ethics and morality. Amodei et al call this \"scaleable supervision\".\n",
    "scalable_supervision = Problem(\"Scalable supervision of a learning system\", [\"safety\", \"agi\"], url=\"https://arxiv.org/abs/1606.06565\")\n",
    "cirl = Problem(\"Cooperative inverse reinforcement learning of objective functions\", [\"safety\", \"agi\"], url=\"https://arxiv.org/abs/1606.03137\")\n",
    "cirl.notes = \"This is tagged agi because most humans are able to learn ethics from their surrounding community\"\n",
    "# Co-operative inverse reinforcement learning might be equivalent to solving scalable supervision, or there might other subproblems here\n",
    "scalable_supervision.add_subproblem(cirl)\n",
    "\n",
    "safe_exploration = Problem(\"Safe exploration\", [\"safety\", \"agi\", \"world-modelling\"], url=\"https://arxiv.org/abs/1606.06565\")\n",
    "safe_exploration.notes = \"\"\"\n",
    "Sometimes, even doing something once is catastrophic. In such situations, how can an RL agent or some other AI system\n",
    "learn about the catastrophic consequences without even taking the action once? This is an ability that most humans acquire\n",
    "at some point between childhood and adolescence.\n",
    "\"\"\"\n",
    "# safe exploration may be related to one shot learning, though it's probably too early to mark that so clearly.\n",
    "\n",
    "avoiding_reward_hacking = Problem(\"Avoiding reward hacking\", [\"safety\"], url=\"https://arxiv.org/abs/1606.06565\")\n",
    "avoiding_reward_hacking.notes = \"\"\"\n",
    "Humans have only partial resistance to reward hacking.\n",
    "Addiction seems to be one failure to exhibit this resistance.\n",
    "Avoiding learning something because it might make us feel bad, or even building elaborate systems of self-deception, are also sometimes\n",
    "seen in humans. So this problem is not tagged \"agi\".\n",
    "\"\"\"\n",
    "\n",
    "avoiding_side_effects = Problem(\"Avoiding undesirable side effects\", [\"safety\"], url=\"https://arxiv.org/abs/1606.06565\")\n",
    "avoiding_side_effects.nodes = \"\"\"\n",
    "Many important constraints on good behaviour will not be explicitly\n",
    "encoded in goal specification, either because they are too hard to capture\n",
    "or simply because there are so many of them and they are hard to enumerate\n",
    "\"\"\"\n",
    "\n",
    "robustness_to_distributional_change = Problem(\"Function correctly in novel environments (robustness to distributional change)\", [\"safety\", \"agi\"], url=\"https://arxiv.org/abs/1606.06565\")\n",
    "\n",
    "copy_bounding = Problem(\"Know how to prevent an autonomous AI agent from reproducing itself an unbounded number of times\", [\"safety\"])\n",
    "\n",
    "safety = Problem(\"Know how to build general AI agents that will behave as expected\")\n",
    "safety.add_subproblem(adversarial_examples)\n",
    "safety.add_subproblem(scalable_supervision)\n",
    "safety.add_subproblem(safe_exploration)\n",
    "safety.add_subproblem(avoiding_reward_hacking)\n",
    "safety.add_subproblem(avoiding_side_effects)\n",
    "safety.add_subproblem(robustness_to_distributional_change)\n",
    "safety.add_subproblem(copy_bounding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Hacking Systems\n",
    "\n",
    "Automated tools are becoming increasingly effective both for offensive and defensive computer security purposes.\n",
    "\n",
    "On the defensive side, fuzzers and static analysis tools have been used for some time by well-resourced software development teams to reduce the number of vulnerabilities in the code they ship.\n",
    "\n",
    "Assisting both offense and defense, DARPA has recently started running the [Cyber Grand Challenge](https://www.cybergrandchallenge.com/) contest to measure and improve the ability of agents to either break into systems or defend those same systems against vulnerabilities. It [isn't necessarily clear](https://www.eff.org/deeplinks/2016/08/darpa-cgc-safety-protocol) how such initiatives would change the security of various systems.\n",
    "\n",
    "This section includes some clear AI problems (like learning to find exploitable vulnerabilities in code) and some less pure AI problems, such as ensuring that defensive versions of this technology (whether in the form of fuzzers, IPSes, or other things) are deployed on all critical systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It isn't totally clear whether having automated systems be good at finding bugs in and of itself will make the deployment\n",
    "# of AI technologies safer or less safe, so we tag this both with \"safety\" and as a potentialy \"unsafe\" development\n",
    "bug_finding = Problem(\"Detect security-related bugs in codebases\", [\"safety\", \"security\", \"unsafe\"])\n",
    "\n",
    "# However what\n",
    "defensive_deployment = Problem(\"Deploy automated defensive security tools to protect valuable systems\")\n",
    "defensive_deployment.notes = \"\"\"\n",
    "It is clearly important is ensuring that the state of the art in defensive technology is deployed everywhere\n",
    "that matters, including systems that perform important functions or have sensitive data on them (smartphones, for instance), and \n",
    "systems that have signifcant computational resources. This \"Problem\" isn't \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pedestrian Detection\n",
    "\n",
    "Detecting pedestrians from images or video is a specific image classification problem that has received a lot of attention because of\n",
    "its importance for self-driving vehicles. Many metrics in this space are based on the [Caltech pedestrians toolkit](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/), thought the [KITTI Vision Benchmark](http://www.cvlibs.net/datasets/kitti/eval_object.php) goes beyond that to include cars and cyclists in addition to pedestrians. We may want to write scrapers for Caltech's published results and KITTI's live results table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pedestrian_detection = Problem(\"Pedestrian, bicycle & obstacle detection\", [\"safety\", \"vision\"])\n",
    "image_classification.add_subproblem(pedestrian_detection)\n",
    "\n",
    "# TODO: import data from these pedestrian datasets/metrics.\n",
    "# performance on them is a frontier of miss rate / false positive tradeoffs, \n",
    "# so we'll need to chose how to handle that as a scale\n",
    "\n",
    "# http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/rocs/UsaTestRocReasonable.pdf\n",
    "pedestrian_detection.metric(\"Caltech Pedestrians USA\", url=\"http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/\")\n",
    "# http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/rocs/InriaTestRocReasonable.pdf\n",
    "pedestrian_detection.metric(\"INRIA persons\", url=\"http://pascal.inrialpes.fr/data/human/\")\n",
    "# http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/rocs/ETHRocReasonable.pdf\n",
    "pedestrian_detection.metric(\"ETH Pedestrian\", url=\"http://www.vision.ee.ethz.ch/~aess/dataset/\")\n",
    "# http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/rocs/TudBrusselsRocReasonable.pdf\n",
    "pedestrian_detection.metric(\"TUD-Brussels Pedestrian\", url=\"http://www.d2.mpi-inf.mpg.de/tud-brussels\")\n",
    "# http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/rocs/DaimlerRocReasonable.pdf\n",
    "pedestrian_detection.metric(\"Damiler Pedestrian\", url=\"http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Mono_Ped__Detection_Be/daimler_mono_ped__detection_be.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability and Interpretability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "explainability = Problem(\"Modify arbitrary ML systems in order to be able to provide comprehensible human explanations of their decisions\")\n",
    "\n",
    "statistical_explainability = Problem(\"Provide mathematical or technical explanations of decisions from classifiers\")\n",
    "statistical_explainability.notes = \"\"\"\n",
    "Providing explanations with techniques such as monte carlo analysis may in general\n",
    "be easier than providing robust ones in natural language (since those may or may not\n",
    "exist in all cases)\n",
    "\"\"\"\n",
    "\n",
    "explainability.add_subproblem(statistical_explainability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness and Debiasing\n",
    "\n",
    "Biased decision making is a problem exhibited both by very simple machine learning classifiers as well as much more complicated ones. Large drivers of this problem include [omitted-variable bias](https://en.wikipedia.org/wiki/Omitted-variable_bias), reliance on inherently biased data sources for training data, attempts to make predictions from insufficient quantities of data, and deploying systems that create real-world incentives that change the behaviour they were measuring (see [Goodhart's Law](https://en.wikipedia.org/wiki/Goodhart%27s_law)).\n",
    "\n",
    "These problems are severe and widespread in the deployment of scoring and machine learning systems in contexts that include [criminal justice](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing), [education policy](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2388436), [insurance](https://www.propublica.org/article/minority-neighborhoods-higher-car-insurance-premiums-white-areas-same-risk) and [lending](https://arxiv.org/abs/1610.02413)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avoiding_bias = Problem(\"Build systems which can recognise and avoid biases decision making\", [\"safety\"])\n",
    "\n",
    "avoiding_bias.notes = \"\"\"\n",
    "Legally institutionalised protected categories represent only the most extreme and socially recognised\n",
    "forms of biased decisionmaking. Attentive human decision makers are sometime capable of recognising\n",
    "and avoiding many more subtle biases. This problem tracks AI systems' ability to do likewise.\n",
    "\"\"\"\n",
    "\n",
    "avoid_classification_biases = Problem(\"Train ML classifiers in a manner that corrects for the impact of omitted-variable bias on certain groups\", solved=True)\n",
    "avoid_classification_biases.notes = '''\n",
    "Several standards are available for avoiding classification biases.\n",
    "\n",
    "They include holding false-positive / false adverse prediction rates constant across protected categories (which roughly maps \n",
    "to \"equal opportunity\"), holding both false-positive and false-negative rates equal (\"demographic parity\"), and ensuring\n",
    "that the fraction of each protected group that receives a given prediction is constant across all groups \n",
    "(roughly equivalent to \"affirmative action\").'''\n",
    "\n",
    "avoid_classification_biases.metric(\"Adjust prediction models to have constant false-positive rates\", url=\"https://arxiv.org/abs/1610.02413\", solved=True)\n",
    "avoid_classification_biases.metric(\"Adjust prediction models tos have constant false-positive and -negative rates\", url=\"http://www.jmlr.org/proceedings/papers/v28/zemel13.pdf\", solved=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy\n",
    "\n",
    "Many of the interesting privacy problems that will arise from AI and machine learning will come from choices about the applications of\n",
    "the technology, rather than a lack of algorithmic progress within the field. But there are some exceptions, which we will track here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "private_training = Problem(\"Train machine learning systems on private user data, without transferring sensitive facts into the model\")\n",
    "private_training.metric(\"Federated Learning (distributed training with thresholded updates to models)\", solved=True, url=\"https://arxiv.org/abs/1602.05629\")\n",
    "\n",
    "avoid_privacy_bias = Problem(\"Fairness in machine learning towards people with a preference for privacy\")\n",
    "avoid_privacy_bias.notes = \"\"\"\n",
    "People who care strongly about their own privacy take many measures to obfuscate their tracks through\n",
    "technological society, including using fictitious names, email addresses, etc in their routine dealings with\n",
    "corporations, installing software to block or send inacurate data to online trackers. Like many other groups,\n",
    "these people may be subject to unfairly adverse algorithmic decisionmaking. Treating them as a protected\n",
    "group will be more difficult, because they are in many respects harder to identify.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hiddencode\n",
    "def counts():\n",
    "    print \"Included thus far:\"\n",
    "    print \"=================================\"\n",
    "    print len(problems), \"problems\"\n",
    "    print len(metrics), \"metrics\", len([m for m in metrics.values() if m.solved]), \"solved\"\n",
    "    print len(measurements), \"measurements\"\n",
    "    print len([p for p in problems.values() if not p.metrics]), \"problems which do not yet have any metrics (either not in this notebook, or none in the open literature)\"\n",
    "    print \"=================================\\n\"\n",
    "    print \"Problems by Type:\"\n",
    "    print \"=================================\"\n",
    "\n",
    "    by_attr = {}\n",
    "    solved_by_attr = {}\n",
    "    for a in all_attributes:\n",
    "        print a, len([p for p in problems.values() if a in p.attributes]), \n",
    "        print \"solved:\", len([p for p in problems.values() if p.solved and a in p.attributes])\n",
    "\n",
    "    print \"\\nMetrics by Type:\"\n",
    "    print \"=================================\"\n",
    "\n",
    "    by_attr = {}\n",
    "    solved_by_attr = {}\n",
    "    for a in all_attributes:\n",
    "        print a, sum([len(p.metrics) for p in problems.values() if a in p.attributes]), \n",
    "        print \"solved:\", sum([len([m for m in p.metrics if m.solved]) for p in problems.values() if a in p.attributes])\n",
    "    print \"=================================\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hiddencode\n",
    "def list_problems():\n",
    "    for p in sorted(problems.values(), key=lambda x: x.attributes):\n",
    "        if not p.superproblems:\n",
    "            p.print_structure()\n",
    "            print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hiddencode\n",
    "def venn_report():\n",
    "    print \"Sample of problems characterized thus far:\"\n",
    "    lang = set(p for p in problems.values() if \"language\" in p.attributes)\n",
    "    world = set(p for p in problems.values() if \"world-modelling\" in p.attributes)\n",
    "    vision = set(p for p in problems.values() if \"vision\" in p.attributes)\n",
    "\n",
    "    from matplotlib_venn import venn3\n",
    "    venn3((lang, world, vision), ('Language Problems', 'World-Modelling Problems', 'Vision Problems'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hiddencode\n",
    "def graphs():\n",
    "    print \"Graphs of progress:\"\n",
    "    for name, metric in metrics.items():\n",
    "        if len(metric.measures) > 2 and not metric.graphed:\n",
    "            print name, \"({0} measurements)\".format(len(metric.measures))\n",
    "            metric.graph()\n",
    "    plt.show()\n",
    "                \n",
    "graphs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxonomy and recorded progress to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_problems()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems and Metrics by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts()\n",
    "venn_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to contribute to this notebook\n",
    "\n",
    "This notebook is an open source, community effort. It lives on Github at https://github.com/AI-metrics/AI-metrics. You can help by adding new metrics, data and problems to it! If you're feeling ambitious you can also improve its semantics or build new analyses into it. Here are some high level tips on how to do that.\n",
    "\n",
    "### 1. If you're comfortable with git and Jupyter Notebooks, or are happy to learn\n",
    "\n",
    "If you've already worked a lot with `git` and IPython/Jupyter Notebooks, here's a quick list of things you'll need to do:\n",
    "\n",
    "1. Install [Jupyter Notebook](https://jupyter.readthedocs.io/en/latest/install.html) and [git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).\n",
    "    * On an Ubuntu or Debian system, you can do: <br><pre>sudo apt-get install git\n",
    "      sudo apt-get install ipython-notebook || sudo apt-get install jupyter-notebook || sudo apt-get install python-notebook</pre>\n",
    "    * Make sure you have IPython Notebook version 3 or higher. If your OS\n",
    "      doesn't provide it, you might need to enable backports, or use `pip` to\n",
    "      install it.\n",
    "2. Install this notebook's Python dependencies:<br>\n",
    "    * On Ubuntu or Debian, do: <br><pre>    sudo apt-get install python-{cssselect,lxml,matplotlib{,-venn},numpy,requests,seaborn}</pre>\n",
    "    * On other systems, use your native OS packages, or use `pip`: <br><pre>    pip install cssselect lxml matplotlib{,-venn} numpy requests seaborn</pre>\n",
    "3. Fork our repo on\n",
    "   github: https://github.com/AI-metrics/AI-metrics#fork-destination-box\n",
    "4. [Clone](https://help.github.com/articles/cloning-a-repository/) the repo on your machine, and `cd` into the directory it's using\n",
    "4. Configure your copy of git to use [IPython Notebook merge filters](http://pascalbugnion.net/blog/ipython-notebooks-and-git.html) to prevent conflicts when multiple people edit the Notebook simultaneously. You can do that with these two commands in the cloned repo:\n",
    "    <pre>git config --file .gitconfig filter.clean_ipynb.clean $PWD/ipynb_drop_output</pre>\n",
    "    <pre>git config --file .gitconfig filter.clean_ipynb.smudge cat</pre>\n",
    "5. Run Jupyter Notebok in the project directory (the command may be `ipython notebook`, `jupyter notebook`, `jupyter-notebook`, or `python notebook` depending on your system), then go to [localhost:8888](http://localhost:8888) and edit the Notebook to your heart's content\n",
    "\n",
    "6. Save and commit your work (`git commit -a -m \"DESCRIPTION OF WHAT YOU CHANGED\"`)\n",
    "7. Push it to your remote repo\n",
    "8. Send us a pull request!\n",
    "\n",
    "### 2. If you want something simpler\n",
    "\n",
    "Microsoft Azure has an IPython / Jupyter service that will let you run and modify notebooks from their servers. You can clone this Notebook and work with it via their service: https://notebooks.azure.com/EFForg/libraries/ai-progress. Unfortunately there are a few issues with running the notebook on Azure:\n",
    "\n",
    "* arXiv seems to block requests from Azure's IP addresses, so it's impossible to automatically extract information about paper when running the Notebook there\n",
    "* The Azure Notebooks service seems to transform Unicode characters in strange ways, creating extra work merging changes from that source\n",
    "\n",
    "\n",
    "## Notes on importing data\n",
    "\n",
    "* Each `.measure()` call is a data point of a specific algorithm on a specific metric/dataset. Thus one paper will often produce multiple measurements on multiple metrics. It's most important to enter results that were at or near the frontier of best performance on the date they were published. This isn't a strict requirement, though; it's nice to have a sense of the performance of the field, or of algorithms that are otherwise notable even if they aren't the frontier for a sepcific problem.\n",
    "* When multiple revisions of a paper (typically on arXiv) have the same results on some metric, use the date of the first version (the CBTest results in [this paper](https://arxiv.org/abs/1606.02245v4) are an example)\n",
    "* When subsequent revisions of a paper improve on the original results ([example](https://arxiv.org/abs/1606.01549v3)), use the date and scores of the first results, or if each revision is interesting / on the frontier of best performance, include each paper\n",
    "  * We didn't check this carefully for our first ~100 measurement data points :(. In order to denote when we've checked which revision of an arXiv preprint first published a result, cite the specific version (https://arxiv.org/abs/1606.01549v3 rather than https://arxiv.org/abs/1606.01549). That way, we can see which previous entries should be double-checked for this form of inaccuracy.\n",
    "* Where possible, use a clear short name or acronym for each algorithm. The full paper name can go in the `papername` field (and is auto-populated for some papers). When matplotlib 2.1 ships we may be able to get nice [rollovers](https://github.com/matplotlib/matplotlib/pull/5754) with metadata like this. Or perhaps we can switch to D3 to get that type of interactivity.\n",
    "\n",
    "\n",
    "\n",
    "## What to work on\n",
    "\n",
    "* If you know of ML datasets/metrics that aren't included yet, add them\n",
    "* If there are papers with interesting results for metrics that aren't included, add them\n",
    "* If you know of important problems that humans can solve, and machine learning systems may or may not yet be able to, and they're missing from our taxonomy, you can propose them\n",
    "* Look at our [Github issue list](https://github.com/AI-metrics/master_text), perhaps starting with those tagged as [good volunteer tasks](https://github.com/AI-metrics/master_text/issues?q=is%3Aissue+is%3Aopen+label%3A%22Good+volunteer+task%22).\n",
    "* You can also add missing conferences / journals to to the venue-to-date mapping table (unhide the source code and search for `conference_dates`):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Most commonly encountered publication venues we don't have a precise date for yet:\"\n",
    "for venue, n in sorted(conferences_wanted.items(), key=lambda x:x[1], reverse=True):\n",
    "    if n > 1: print venue, n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building on this data\n",
    "\n",
    "If you want to use this data for some purpose that is beyond the scope of this Notebook, all of the raw data exported as a JSON blob. This is not yet a stable API, but you can get the data at:\n",
    "\n",
    "\n",
    "https://raw.githubusercontent.com/AI-metrics/AI-metrics/master/export-api/v01/progress.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0; float:left\" src=\"images/cc-by-sa.png\" /></a><br><br>Much of this Notebook is uncopyrightable data. The copyrightable portions of this Notebook that are written by EFF and other Github contributors are licensed under the <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>. Illustrations from datasets and text written by other parties remain copyrighted by their respective owners, if any, and may be subject to different licenses.\n",
    "\n",
    "The source code is also dual-licensed under the [GNU General Public License, version 2 or greater](https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hiddencode\n",
    "\n",
    "def export_json(indent=False, default_name=\"export-api/v01/progress.json\"):\n",
    "    \"\"\"Export all the data in here to a JSON file! Default name: progress.json.\"\"\"\n",
    "    output = {'problems':[]}\n",
    "    for problem in problems:\n",
    "        problem = problems[problem]\n",
    "        problem_data = {}        \n",
    "        for problem_attr in problem.__dict__:\n",
    "            if problem_attr in ['subproblems', 'superproblems']:\n",
    "                problem_data[problem_attr] = map(lambda x: x.name, getattr(problem, problem_attr))\n",
    "            elif problem_attr != 'metrics':\n",
    "                problem_data[problem_attr] = getattr(problem, problem_attr)\n",
    "            elif problem_attr == 'metrics':\n",
    "                problem_data['metrics'] = []\n",
    "                for metric in problem.metrics:\n",
    "                    metric_data = {}\n",
    "                    metric_data['measures'] = []\n",
    "                    for metric_attr in metric.__dict__: \n",
    "                        if metric_attr == 'scale':\n",
    "                            metric_data[metric_attr] = getattr(metric, metric_attr).axis_label\n",
    "                        elif metric_attr != 'measures':\n",
    "                            metric_data[metric_attr] = getattr(metric, metric_attr)\n",
    "                        elif metric_attr == 'measures':\n",
    "                            for measure in getattr(metric, 'measures'):\n",
    "                                measure_data = {}\n",
    "                                for measure_attr in measure.__dict__:\n",
    "                                    measure_data[measure_attr] = getattr(measure, measure_attr)\n",
    "                                metric_data['measures'].append(measure_data)\n",
    "                    problem_data['metrics'].append(metric_data)\n",
    "        output['problems'].append(problem_data)\n",
    "    if indent:\n",
    "        with open(default_name, 'w') as f:\n",
    "            f.write(json.dumps(output, default=str, indent=4))\n",
    "    else:\n",
    "        with open(default_name, 'w') as f:\n",
    "            f.write(json.dumps(output, default=str))\n",
    "        \n",
    "export_json(indent=True)"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}